{
  "hash": "5c2d7d336108525c224443ba46921802",
  "result": {
    "markdown": "---\ntitle: \"AutoCorrelation, Stationarity and Random-Walk - Part 1\"\nauthor: \"Francois de Ryckel\"\ndate: '2022-09-29'\ncategories: [time-series]\ndate-modified: '2022-09-09'\n---\n\n\nThis post is to set up the basic concepts of time-series analysis.  \n\n# Autocovariance & autocorrelation \n\nAuto-correlation as the name indicates is the correlation of the time-series with itself, more specifically with a lag version of itself. \n\nLet's consider $\\{X_t\\}$ a time series.  \n\n*   Then the mean function of $\\{X_t\\}$ (the first moment) is defined as $\\mu_t = \\textbf{E}(X_t)$.  In other words, $\\mu_t$ is the expected value of the time series at point t.  \n*   The Variance of the time series is defined as $\\sigma_t ^2 = Var(X_t) = \\textbf{E}[(X_t - \\mu_t)^2]$. \n*   In general, $\\mu_t$ and $\\sigma_t ^2$ are different at different point in time. \n\nNow, we define the **auto-covariance** function of the time series as $$\\gamma(s, t) = Cov(X_s, X_t) = \\textbf{E}[(X_s - \\mu_s)(X_t - \\mu_t)]$$. \nIn the same vein, we define the **auto-correlation** function of the time series as \n$$\\rho(s,t) = Corr(X_s, X_t) = \\frac {\\gamma (s, t)}{\\sigma_s \\sigma_t} = \\frac{Cov(X_s, X_t)}{\\sqrt{Var(X_s) Var(X_t)}}$$\n\nAutocovariance and autocorrelation measure the linear correlation between between two points $X_s$ and $X_t$ on the same time-series. \n\nFew properties of autocavariance and autocorrelation of time-series \n\n* $\\gamma(t, t) = \\sigma_t^2$ \n* $\\gamma(s, t) = \\gamma(t, s)$ \n* $|\\gamma(s, t)| \\le \\sigma_s \\sigma_t$ \n* $\\rho(t, t) \\equiv 1$\n\n## Autocorrelation plots - Correlogram \n\nAs exercise, we can plot the auto-correlation of a non-stationary (aka with significant autocorrelation) time-series.  We are using the [Monthly Milk production](https://www.kaggle.com/datasets/bhaveshsonagra/monthly-milk-production) (no idea where the data come from)\n\n### Using R \n\nIn R the standard function to plot a correlogram is the *acf()* function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\n\nmilk <- read_csv('../../raw_data/milk.csv')\nacf(milk$milk_prod_per_cow_kg)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nGraph clearly shows some seasonality (at the 12 lags ==> yearly correlation) which indicates that our data are non-stationary (next section). \n\nIf we are more attached to the auto-correlation values, we can store the results in a dataframe. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nyo <- acf(milk$milk_prod_per_cow_kg, plot = F)\nyo\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAutocorrelations of series 'milk$milk_prod_per_cow_kg', by lag\n\n    0     1     2     3     4     5     6     7     8     9    10    11    12 \n1.000 0.892 0.778 0.620 0.487 0.428 0.376 0.415 0.454 0.562 0.687 0.769 0.845 \n   13    14    15    16    17    18    19    20    21    22 \n0.745 0.638 0.490 0.364 0.306 0.255 0.287 0.321 0.417 0.529 \n```\n:::\n:::\n\n\n\nWe could use the *ggplot* package to create a function to draw acf and get more customization.  We will re-use this function later as well. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# slightly fancier version (with more customization)\nggacf <- function(series) {\n  significance_level <- qnorm((1 + 0.95)/2)/sqrt(sum(!is.na(series)))  \n  a <- acf(series, plot=F)\n  a.2 <- with(a, data.frame(lag, acf))\n  g <- ggplot(a.2[-1,], aes(x=lag,y=acf)) + \n    geom_segment(mapping = aes(xend = lag, yend = 0), linewidth = 0.8) + \n    xlab('Lag') + ylab('ACF') + \n    geom_hline(yintercept=c(significance_level,-significance_level), linetype= 'dashed', color = 'dodgerblue4');\n\n  # fix scale for integer lags\n  if (all(a.2$lag%%1 == 0)) {\n    g<- g + scale_x_discrete(limits = factor(seq(1, max(a.2$lag))));\n  }\n  return(g);\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tibble)\n\nggacf(milk$milk_prod_per_cow_kg)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Using Python \n\nIn python, we need to use the *statsmodel* package. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom pandas import read_csv\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf\n\ndf = read_csv('../../raw_data/milk.csv', index_col=0)\nplot_acf(df)\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n# Stationarity \n\nA time-series $\\{X_t\\}$ is (weakly) stationary if: \n\n* $E[X_t] = \\mu$ is a constant \n* $E[X_t^2] < \\infty$ \n* $Cov(X_t, X_{t+k}) = \\gamma(k)$ is independent of t for each integer k. $\\gamma(k)$ is called the lag $k$ autocovariance of function of $\\{X_t\\}$\n\n# White Noise \n\nWhite noise is a special type of time-series and a special case of stationarity. The concept emerge when studying **Random Walk**.  $\\{X_t\\}$ is a random-walk if it satisfies the equation: \n\n$$X_t = X_{t-1} + W_t$$ (@eq-rw1)\nwhere $\\{W_t\\}$ is a white-noise.  In other words, $W_t \\sim iid N(0, \\sigma_w^2)$\n\n\n\n$\\{W_t\\}$ is a white-noise if \n\n* $E[W_t] = \\mu$ is a constant for all t\n* $Var[W_t] = \\sigma_w^2$ is a constant \n* $Cov(W_s, W_t) = 0$ for any s and t with $s<t$.  In other words, any 2 subset of W are uncorrelated. \n\nIf $\\{X_t\\}$ is iid (independent and identically distributed) and $\\rho(k) = 1$ (when k=0) and $\\rho(k) = 0$ (otherwise, aka for any other values of k), then $\\{X_t\\}$ is a white noise. \n\nTherefore, as $n \\rightarrow \\infty$, we can say that $\\frac{1}{n} (X_1 + \\dots + X_n) = E[X_t] = \\mu$\n\nWe can generate white-noise in R using the arima.sim() function.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\n# Generate a white noise in R\nwn <- stats::arima.sim(model = list(order = c(0, 0, 0)), n = 250)\ndf <- tibble(x = 1:250, y = as.vector(wn))\n\nggplot(df, aes(x, y)) + \n  geom_line(color = 'dodgerblue3') + \n  xlab('Time') + ylab('White Noise') + \n  labs(title = 'Generated White Noise')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n:::\n\n\nAnd let's check the autocorrelation plot to visually confirm that. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# using the standard R function. \nacf(wn, lag.max = 20)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWe could also use a ggplot function to plot the auto-correlation of our time-series. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggacf(wn)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n# Python code ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}