{
  "hash": "0ac4796c7e3bddca1e26756d74876dfc",
  "result": {
    "markdown": "---\ntitle: \"02 - Statistical Moments\"\nauthor: \"Francois de Ryckel\"\ndate: \"2022-11-02\"\ncategories: [statistics, Mean, Skewness, Kurtosis, log-transformation, Box-Cox]\ndescription: 'Introducing the first 4 moments of statistical analysis: mean, standard deviation, skewness and kurtosis.  Showing how to use R and Python on these concepts. We then provide 2 methods to transform data in order to bring it closer to a normal distribution.'\neditor: source\nimage: kurtosis.png\ndate-modified: \"2023-11-21\"\n---\n\n\nThis post is about summarizing the various statistical moments when doing quantitative finance. The focus is on the asset returns. From a [previous post](../../quant-part1/normality-returns/index.qmd), we already know that financial asset returns do not follow a normal distribution (too peaked at the mean and fat tails). \n\nWe'll show these parameters using both R and python.  \n\nWe'll use the SPY as a low-ish vol asset and AMD as an equity with higher vol.  We only use the last 5 years of data (from 2018 and beyond)\n\nLet's first load our libraries and the 2 data frame worth of prices. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)    # read_csv()\nlibrary(dplyr)    # mutate(), filter()\nlibrary(lubridate)\nlibrary(ggplot2)\n\ndf_spy <- read_csv('../../../raw_data/SPY.csv') |> \n  select(date, adjClose) |> \n  arrange(date) |> \n  mutate(return = log(adjClose / lag(adjClose))) |> \n  filter(date > '2018-01-01')\n\ndf_amd <- read_csv('../../../raw_data/AMD.csv') |> \n  select(date, adjClose) |> \n  arrange(date) |> \n  mutate(return = log(adjClose / lag(adjClose))) |> \n  filter(date > '2018-01-01')\n```\n:::\n\n\n\n# Mean \n\nThe mean is our first moment. We'll cons\n\nMathematically speaking, we define the mean as $$\\sum^{n}_{i=1} \\frac{r_i}{n}$$\n\n* $r_i$ is the return of the i-th observation\n    * using log return \n* $n$ is the number of observation\n\n# Standard Deviation \n\nStandard deviation is the second moment.  \n\n$$\\sigma = \\sqrt{\\frac{\\sum(x_i - \\mu)^2}{n}}$$\nIn the case of a sample: $s = \\sqrt{\\frac{(x_i - \\bar{x})^2}{n-1}}$\n\nThe variance is the square of the standard deviation.  There is this neat little alternative expression of variance (I like the way it sounds). \n\n> Variance is the difference between the mean of the square and the square of the mean.  \n\nHere is how it goes: \n\n$$\\sigma^2 = \\frac{\\sum(x_i-\\mu)^2}{n} = \\frac{\\sum(x_i)^2}{n} - 2 \\frac{\\sum(x_i \\mu)}{n} + \\frac{\\sum(\\mu)^2}{n}$$\n\nConsidering \n\n* $\\frac{\\sum(x_i \\mu)}{n} = \\mu \\frac{\\sum(x_i)}{n} = \\mu \\mu=\\mu^2$ \n* and considering $\\frac{\\sum(\\mu)^2}{n} = n \\frac{\\mu^2}{n} = \\mu^2$, we can re-write our variance like this\n\n$$\\sigma^2 = \\frac{\\sum(x_i)^2}{n} - 2 \\mu^2 + \\mu^2 = \\frac{\\sum(x_i)^2}{n} - \\mu^2$$\n\n# Coefficient of variation \n\nThe coefficient of variation is not a statistical moment, but considering it is the ratio of the first 2 moments (ratio of sd to the mean), we include it here as well. \nIt allows to compare together 2 different distributions (that have different mean and sd).  \n\n$$CV = \\frac{\\sigma}{\\mu}$$\n\nOr in the case of a sample $CV = \\frac{s}{\\bar{x}}$\n\n# Skewness \n\n::: {.callout-tip} \n\n# Skewness \n\nSkewness is measure of asymmetry of a distribution (or actually lack of).  How symmetric around the mean is the distribution? \nA standard normal distribution is perfectly symmetrical and has zero skew. Other examples of zero-skewed distributions are the T Distribution, the uniform distribution and the Laplace distribution. However, other distributions don’t have zero skew. \n\nIn a zero skew distribution, the mean = the median and the distribution is symmetric. \n\n:::\n\n![Illustration of skewness](skewness.png)\n\nIn a sense, skewness is quantifying for us how far is the median from the mean.  \n\nMathematically, we define skewness as \n\n$$\\frac{\\frac{1}{n} \\sum(x_i - \\bar{x})^3}{\\sqrt{ \\left( \\frac{1}{n} \\sum(x_i - \\bar{x})^2 \\right)^3}}$$ {#eq-skewness-pop}\n\nIn the case of a sample, we'll multiply @eq-skewness-pop by a factor of $\\frac{\\sqrt{n(n-1)}}{n-2}$\n\n* skenwess = 0 ==> normally distributed\n* $-0.5 \\leq skewness \\leq 0.5$  ==> moderately skewed\n* $skewness \\leq -1$ or $skewness \\geq 1$ ==> highly skewed\n\nIn finance, understanding skewness can help investors understand probable future outcomes. Negative skewness (left-tail) in investment returns would indicate a greater probability of large losses, while positive skewness (right-tail) would indicate a higher chance of large gains.\n\n# Kurtosis \n\n::: {.callout-tip} \n\n# Kurtosis\n\nKurtosis is a measure that describes the shape of a distribution's tails in relation to its overall shape. A distribution can be infinitely peaked with low kurtosis, and a distribution can be perfectly flat-topped with infinite kurtosis. Thus, kurtosis measures \"tailedness,\" not \"peakedness.\" \n\n:::\n\nBecause, we raised the difference of a data point to its mean to the 4th power, it is really the data points far away from the mean that do participate to the kurtosis. \n\n> \"increasing kurtosis is associated with the “movement of probability mass from the shoulders of a distribution into its center and tails.\"  (Someone important who got published and knew his stuff!) \n\nMathematically, we define kurtosis as \n\n$$\\frac{\\frac{1}{n} \\sum(x_i - \\bar{x})^4}{ \\left( \\frac{1}{n} \\sum(x_i - \\bar{x})^2 \\right)^2}$$ {#eq-kurtosis-pop}\n\nSome statistical packages are providing **excess kurtosis** by subtracting 3 to the kurtosis value. So for a data set that is perfectly normally distributed, we expect the excess kurtosis to be 0. \n\nThere are 3 categories of kurtosis: leptokurtic (positive excess kurtosis), mesokurtic (aka normal distribution), platykurtic. \n![Type of Kurtosis](kurtosis.png)\n\n* A kurtosis greater than 3 => leptokurtic\n* A kurtosis around 3 => mesokurtic \n* A kurtosis less than 3 => platykurtic\n\nA Student's T distribution with degree of freedom 4 has infinite kurtosis (huge peak and tails + narrow shoulders)\n\nMost equities display a leptokurtic behavior (skinny at the mean - most returns are clustered around the mean) and narrow shoulders and fat tails.  \n\nKurtosis helps understand the extremities in the data by measuring the “tailedness.” High kurtosis (‘Leptokurtic’ data) indicates a distribution with tail data exceeding the tails of the normal distribution. This means there are more chances of outliers, indicating a “heavy-tailed” distribution and potentially higher risk in financial contexts.\nEspecially in finance, kurtosis is critical to assess market volatility. Higher kurtosis indicates more frequent significant market movements than expected for a normal distribution.\n\n\n# Using Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\namd = pd.read_csv('../../../raw_data/AMD.csv')\n\nx = amd['adjClose']\n\nreturns_21d = (x / x.shift(21) - 1).dropna()\nreturns_21d_log = np.log(x / x.shift(21)).dropna()\n\nmean_21dret = np.mean(returns_21d)\nmean_21d_log_ret = np.mean(returns_21d_log)\nstd_21dret = np.std(returns_21d)\nstd_21d_log_ret = np.std(returns_21d_log)\n\nprint(\"The mean rolling 21 days return is: %s\" % round(mean_21dret, 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe mean rolling 21 days return is: 0.01117\n```\n:::\n\n```{.python .cell-code}\nprint(\"The mean rolling 21 days log return is: %s\" % round(mean_21d_log_ret, 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe mean rolling 21 days log return is: -0.00594\n```\n:::\n\n```{.python .cell-code}\nprint(\"The standard deviation of the rolling 21 days return is: %s\" %round(std_21dret, 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe standard deviation of the rolling 21 days return is: 0.19248\n```\n:::\n\n```{.python .cell-code}\nprint(\"The standard deviation of the rolling 21 days return is: %s\" %round(std_21d_log_ret, 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe standard deviation of the rolling 21 days return is: 0.1832\n```\n:::\n:::\n\n\nAn histogram of the returns will help visualize the center and spread of the data. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.hist(returns_21d, bins = 'rice', label = 'Rolling 21-days return')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=614}\n:::\n:::\n\n\nWe are seeing a larger right tail with indeed the mean looking around 0. \nAn other to visualize this and putting emphasis on the outliers would be to plot the returns on a box-and-whiskers plot. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nplt.boxplot(returns_21d, labels = ['Rolling 21-days return'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'whiskers': [<matplotlib.lines.Line2D object at 0x157ebda90>, <matplotlib.lines.Line2D object at 0x157ebdd30>], 'caps': [<matplotlib.lines.Line2D object at 0x157ebdfd0>, <matplotlib.lines.Line2D object at 0x157ecf2b0>], 'boxes': [<matplotlib.lines.Line2D object at 0x157ebd7f0>], 'medians': [<matplotlib.lines.Line2D object at 0x157ecf550>], 'fliers': [<matplotlib.lines.Line2D object at 0x157ecf7f0>], 'means': []}\n```\n:::\n\n```{.python .cell-code}\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.stats import skew, skewtest, kurtosis\n\nprint('The skew of 21-day returns is: %f' % round(skew(returns_21d), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe skew of 21-day returns is: 1.122000\n```\n:::\n\n```{.python .cell-code}\nprint('The kurtosis of 21-day returns is %f' % round(kurtosis(returns_21d), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe kurtosis of 21-day returns is 3.690000\n```\n:::\n\n```{.python .cell-code}\n# now checking on log returns\nprint('The skew of 21-day log returns is: %f' % round(skew(returns_21d_log), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe skew of 21-day log returns is: 0.214000\n```\n:::\n\n```{.python .cell-code}\nprint('The kurtosis of 21-day log returns is %f' % round(kurtosis(returns_21d_log), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe kurtosis of 21-day log returns is 0.973000\n```\n:::\n:::\n\n\nThe skew value is quite large and positive which confirms the right tail of the histogram \n\n\n::: {.cell}\n\n```{.python .cell-code}\nskewtest(returns_21d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSkewtestResult(statistic=28.030054369118826, pvalue=6.993000434657173e-173)\n```\n:::\n:::\n\n\nVery small p-value.  We reject the null-hypothesis.  The distribution is not symetrical. \n\n# Transformation to reduce skewness and kurtosis  \n\nWhy do we care about skewness and kurtosis?  Many of the statistical tests and machine learning algorithms are making assumptions on the type of distribution (often assuming the data are normally distributed).  Skewness and kurtosis are part of the tool box to check the normality of the data. \n\nThere are 2 types of transformation one can do to transform data to a more normal distribution.\n\nLet's consider [weekly oil price](https://fred.stlouisfed.org/series/WCOILWTICO) (as per the FED Bank).  I am using here a commodity as they are known to be positively skewed (as opposed to equities which tends to be negatively skewed)\n\n## Log-transformations \n\nLog transformations tends to reduce the spread of the data.  They are appropriate for **right-skewed data** that is the tail is on the right side of the data. Majority of the data are on the left side with few extreme data on the right side.  \nIn the case of left-skewed data, a log transformation will accentuate that skewness even more. \n\nAlso you cannot log negative values (duh!)\n\nThe effect of a log transformation on dummy data looks like \n![Log transformation on dummy data](log-dummy-data.png)\n\nThe points that were all close to each other are now more spread out.  For low values, the log curve is pretty steep in increase the spread.  And for the more extreme values (on the right side) log bring them closer to each others (curve at higher values is less steep, lower gradient). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(dplyr)\n\nw_oil <- read_csv('../../../raw_data/WCOILWTICO.csv') |> \n  mutate(w_log_return = log(WCOILWTICO / lag(WCOILWTICO)), \n         w_return = WCOILWTICO / lag(WCOILWTICO))\n\nlibrary(moments)\nskewness(w_oil$w_return, na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 27.9621\n```\n:::\n\n```{.r .cell-code}\nkurtosis(w_oil$w_return, na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1081.682\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nggplot(w_oil, aes(x = w_return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-5.png){width=672}\n:::\n:::\n\n\nApplying now the same process (histogram, skewness and kurtosis) on our log transformed returns and checking the results. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nskewness(w_oil$w_log_return, na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -3.073457\n```\n:::\n\n```{.r .cell-code}\nkurtosis(w_oil$w_log_return, na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 336.4981\n```\n:::\n\n```{.r .cell-code}\nggplot(w_oil, aes(x = w_log_return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nIndeed we have a slightly more normal distribution of data (although, we still have both skewness and kurtosis)\n\n## Box-Cox transformation \n\nThe Box-Cox transformation is a statistical technique used to transform non-normal data into a more normal distribution. It is named after George Box and John Tukey, who introduced the method in 1964.\n\nThe Box-Cox transformation is based on the idea that many non-normal distributions can be approximated by a power transformation of a normal distribution. The power transformation is given by:\n\n$$\\begin{equation}\nY_i^{(\\lambda)} = \n\\left\\{ \\begin{aligned}\n\\frac{Y_i^{\\lambda} -1}{\\lambda} \\; \\space (\\lambda \\neq 0) \\\\\nlog(Y_i) \\; \\space (\\lambda = 0)\n\\end{aligned}\\right.\n\\end{equation}$$\n\nIf $\\lambda$ equal ... \n\n* 2: square transformation\n* 1: no transformation needed; produces results identical to original data\n* 0.50: square root transformation\n* 0.33: cube root transformation\n* 0.00: natural log transformation\n* -0.50: reciprocal square root transformation\n* -1.00: reciprocal (inverse) transformation \n\nOnce data have been transformed, we will need to apply the inverse transformed back at some point.  The inverse transformation is using the log function (inverse of exponent!)\n\n$$\\begin{equation}\nY_i = \n\\left\\{ \\begin{aligned}\nexp \\left(\\frac{ log(1 + \\lambda Y_i^{(\\lambda)})}{\\lambda} \\right) \\; \\space (\\lambda \\neq 0) \\\\\nexp(Y_i^{\\lambda}) \\; \\space (\\lambda = 0)\n\\end{aligned}\\right.\n\\end{equation}$$\n\nHow is lambda chosen?  Using maximum likelihood. \n\nLet's apply the transformation to our oil data using the *tidymodel* approach with the *recipes* library. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(recipes)\n\ndat <- w_oil |> na.omit() |> select(w_return)\n\nrecipe_bc <- recipe(~ w_return, data = dat) |> \n  step_BoxCox(w_return) |> \n  # the estimating of lambda step\n  prep(training = dat, retain = TRUE)\n\n# to check the recipe steps\ntidy(recipe_bc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n  number operation type   trained skip  id          \n   <int> <chr>     <chr>  <lgl>   <lgl> <chr>       \n1      1 step      BoxCox TRUE    FALSE BoxCox_SJuL5\n```\n:::\n\n```{.r .cell-code}\n# to check the estimation of lambda\ntidy(recipe_bc, number = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  terms    value id          \n  <chr>    <dbl> <chr>       \n1 w_return 0.115 BoxCox_SJuL5\n```\n:::\n\n```{.r .cell-code}\n# transforming the data using the estimated lambda\nw_oil_bc <- juice(recipe_bc)\n\nw_oil$w_return_bc <- c(NA, w_oil_bc$w_return)\n\nskewness(w_oil$w_return_bc, na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8642865\n```\n:::\n\n```{.r .cell-code}\nkurtosis(w_oil$w_return_bc, na.rm = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 324.378\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nggplot(w_oil, aes(x = w_return_bc)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}