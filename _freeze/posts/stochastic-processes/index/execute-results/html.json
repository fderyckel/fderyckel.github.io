{
  "hash": "6ccaf0c6bb282f75ae88b0fb9d4c4f28",
  "result": {
    "markdown": "---\ntitle: \"Stochastic processes - Discrete Time Markov Chain\"\nauthor: \"Francois de Ryckel\"\ndate: \"2022-11-12\"\ncategories: [Markov Chain]\neditor: source\n---\n\n\nThis post is an introduction to Markov Chain with a presentation of Discrete Time Markov Chains.  \n\n## Definition \n\nA stochastic process is $\\{ X(t), t \\in T \\}$ is a collection of random variables indexed by a parameter t that belongs to a set T.  \n\n* t is generally the time \n* $X(t)$ is the state of the process at time t\n* The *state space* $S$ of a stochastic process is all possible state $X(t)$ for any $t \\in T$\n* if T is a countable set, we call this a **discrete-time process** \n\nA **discrete-time Markov Chain** is a discrete-time stochastic process which state space *S* is finite such that: \n$$\\mathbb{P}(X_{n+1} = j | X_0 = i_0, X_1 = i_1, X_2 = i_2, \\dots, x_n = i) = \\mathbb{P}(X_{n+1} = j | X_n = i) = P_{ij}$$ \n\nthat is, the conditional probability of the process being in state j at time n + 1 given all the previous states depends only on the last-known position (state i at time n).  \n\n## Some other teminology \n\n* A state is called **absorbing** if the chain cannot leave it once it enters it. An absorbing Markov chain has at least one absorbing state. \n* A state is termed **reflecting** if once the chain leaves it, it cannot return to it. \n* The **period d of a state i** is the number such that, starting in i, the chain can return to i only in the number of steps that are multiples of d. A\nstate with period d = 1 is called **aperiodic.** Periodicity is a class property. \n  + For a reflecting state, the period is infinite, since the chain never comes back to this state. \n  + Absorbing states necessarily have loops and thus are aperiodic states.\n* a state is called recurrent if with probability 1 the chain ever reenters that state. Otherwise, the state is called transient. \n* A Markov Chain that has a unique stationary distribution (or steady-state distribution) is called an **ergodic** chain. \n\n\n## Chapman-Kolmogorov equations \n\nWe denote the probability to go from state $i$ to state $j$ in n-steps by $\\bf{P}_{ij}^{(n)}$.  It is also denoted as the **n-steps transition probability matrix**.  That is for any time $m >= 0, \\bf{P}_{ij}^n = \\mathbb{P}(X_{m+n} = j | X_m = i)$ . \n$\\bf{P}^{(n)} = \\bf{P}^n$ based on the Chapman-Kolmogorov equation. \n\nThe Chapman-Kolmogorov equation states that for all positive integers $m$ and $n$ , $\\bf{P}^{(m+n)} = \\bf{P}^m \\cdot \\bf{P}^n$ where P is a one-step probability transition matrix (a square matrix)\n\n## Example 1\n\nTo model a Markov Chain, let's first set up a *one-step probability transition matrix* (called here **osptm**).  \n\nWe start with an easy 3 possible state process. That is the state space $S = \\{1, 2, 3\\}$.  The osptm will provide the probability to go from one state to another. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nosptm = matrix(c(0.7,0.1,0.2, 0,0.6,0.4, 0.5,0.2,0.3), nrow = 3, byrow = TRUE)\nosptm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]  0.7  0.1  0.2\n[2,]  0.0  0.6  0.4\n[3,]  0.5  0.2  0.3\n```\n:::\n:::\n\n\nWe can always have a look at how the osptm looks like. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# note we have to transpose the osptm matrix first. \nosptm_transposed = t(osptm)\nosptm_transposed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]  0.7  0.0  0.5\n[2,]  0.1  0.6  0.2\n[3,]  0.2  0.4  0.3\n```\n:::\n\n```{.r .cell-code}\ndiagram::plotmat(osptm_transposed, pos = c(1, 2), arr.length = 0.3, \n                 box.col = \"lightblue\", box.prop = 0.5, box.size = 0.12, box.type=\"circle\", \n                 self.cex = 0.6, self.shifty=-0.01, self.shiftx = 0.15)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe *markovchain* package can provide us with all the state characteristics of a one-step probabilty transition matrix. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(markovchain)\nosptm_mc <- new(\"markovchain\", transitionMatrix = osptm)\n\nrecurrentClasses(osptm_mc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] \"1\" \"2\" \"3\"\n```\n:::\n\n```{.r .cell-code}\ntransientClasses(osptm_mc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlist()\n```\n:::\n\n```{.r .cell-code}\nabsorbingStates(osptm_mc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncharacter(0)\n```\n:::\n\n```{.r .cell-code}\nperiod(osptm_mc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\nround(steadyStates(osptm_mc), 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          1      2      3\n[1,] 0.4651 0.2558 0.2791\n```\n:::\n:::\n\n\n\nThe next step is to calculate, for instance, what is the probability to go from state 1 to state 3 in 4 steps.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(expm)\n\n# the expm library brings in the \" %^%\" operator for power. \nosptm %^% 4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       [,1]   [,2]   [,3]\n[1,] 0.5021 0.2303 0.2676\n[2,] 0.3860 0.3104 0.3036\n[3,] 0.4760 0.2483 0.2757\n```\n:::\n:::\n\n\nLooking at the result, we can see that the probability to go from State 1 to State 3 in 4 steps is 0.2676  \n\nWe can also calculate the unconditional distribution after 4 steps \n\n\n::: {.cell}\n\n```{.r .cell-code}\ninitial_pro <- c(1/3, 1/3, 1/3)\ninitial_pro %*% (osptm %^% 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       [,1]  [,2]   [,3]\n[1,] 0.4547 0.263 0.2823\n```\n:::\n:::\n\n\n\n## Example 2 \n\nUsing a slightly more interesting one-step probability transition matrix having 6 different states. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#specifying transition probability matrix\nosptm<- matrix(c(0.3,0.7,0,0,0,0,1,0,0,0,0,0,0.5,0,0,0,0,0.5, 0,0,0.6,0,0,0.4,0,0,0,0,0.1,0.9,0,0,0,0,0.7,0.3), nrow=6, byrow=TRUE)\nosptm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  0.3  0.7  0.0    0  0.0  0.0\n[2,]  1.0  0.0  0.0    0  0.0  0.0\n[3,]  0.5  0.0  0.0    0  0.0  0.5\n[4,]  0.0  0.0  0.6    0  0.0  0.4\n[5,]  0.0  0.0  0.0    0  0.1  0.9\n[6,]  0.0  0.0  0.0    0  0.7  0.3\n```\n:::\n\n```{.r .cell-code}\nosptm_transposed = t(osptm)\ndiagram::plotmat(osptm_transposed, arr.length = 0.3, arr.width = 0.1, \n                 box.col = \"lightblue\", box.prop = 0.5, box.size = 0.09, box.type=\"circle\", \n                 cex.txt = 0.8, self.cex = 0.6, self.shifty=-0.01, self.shiftx = 0.13)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nosptm_mc <- new(\"markovchain\", transitionMatrix = osptm)\n\nrecurrentClasses(osptm_mc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] \"1\" \"2\"\n\n[[2]]\n[1] \"5\" \"6\"\n```\n:::\n\n```{.r .cell-code}\ntransientClasses(osptm_mc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] \"3\"\n\n[[2]]\n[1] \"4\"\n```\n:::\n\n```{.r .cell-code}\nabsorbingStates(osptm_mc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncharacter(0)\n```\n:::\n\n```{.r .cell-code}\nperiod(osptm_mc)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in period(osptm_mc): The matrix is not irreducible\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\nround(steadyStates(osptm_mc), 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          1      2 3 4      5      6\n[1,] 0.0000 0.0000 0 0 0.4375 0.5625\n[2,] 0.5882 0.4118 0 0 0.0000 0.0000\n```\n:::\n:::\n\n\nWe can see that there are 2 possible steady states.  Hence the Markov Chain is non-ergodic. \n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}