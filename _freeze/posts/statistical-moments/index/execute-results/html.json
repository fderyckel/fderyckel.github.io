{
  "hash": "b8c4453df2f455aa18ae7da21d763a00",
  "result": {
    "markdown": "---\ntitle: \"Statistical Moments\"\nauthor: \"Francois de Ryckel\"\ndate: \"2022-11-02\"\ncategories: [statistics, code, analysis]\neditor: source\ndate-modified: \"2023-04-22\"\n---\n\n\nThis post is about summarizing the various statistical moments when doing quantitative finance. The focus is on the asset returns. From a [previous post](../quant-part1/normality-returns/index.qmd), we already know that financial asset returns do not follow a normal distribution (too peaked at the mean and fat tails). \n\nWe'll show these parameters using both R and python.  \n\nWe'll use the SPY as a low-ish vol asset and AMD as an equity with higher vol.  We only use the last 5 years of data (from 2018 and beyond)\n\nLet's first load our libraries and the 2 data frame worth of prices. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)    # read_csv()\nlibrary(dplyr)    # mutate(), filter()\nlibrary(lubridate)\nlibrary(ggplot2)\n\ndf_spy <- read_csv('../../raw_data/SPY.csv') |> \n  select(date, adjClose) |> \n  arrange(date) |> \n  mutate(return = log(adjClose / lag(adjClose))) |> \n  filter(date > '2018-01-01')\n\ndf_amd <- read_csv('../../raw_data/AMD.csv') |> \n  select(date, adjClose) |> \n  arrange(date) |> \n  mutate(return = log(adjClose / lag(adjClose))) |> \n  filter(date > '2018-01-01')\n```\n:::\n\n\n\n# Mean \n\nThe mean is our first moment. We'll cons\n\nMathematically speaking, we define the mean as $$\\sum^{n}_{i=1} \\frac{r_i}{n}$$\n\n* $r_i$ is the return of the i-th observation\n    * using log return \n* $n$ is the number of observation\n\n# Standard Deviation \n\nStandard deviation is the second moment.  \n\n$$\\sigma = \\sqrt{\\frac{\\sum(x_i - \\mu)^2}{n}}$$\nIn the case of a sample: $s = \\sqrt{\\frac{(x_i - \\bar{x})^2}{n-1}}$\n\nThe variance is the square of the standard deviation.  There is this neat little alternative expression of variance (I like the way it sounds). \n\n> Variance is the difference between the mean of the square and the square of the mean.  \n\nHere is how it goes: \n\n$$\\sigma^2 = \\frac{\\sum(x_i-\\mu)^2}{n} = \\frac{\\sum(x_i)^2}{n} - 2 \\frac{\\sum(x_i \\mu)}{n} + \\frac{\\sum(\\mu)^2}{n}$$\n\nConsidering \n\n* $\\frac{\\sum(x_i \\mu)}{n} = \\mu \\frac{\\sum(x_i)}{n} = \\mu \\mu=\\mu^2$ \n* and considering $\\frac{\\sum(\\mu)^2}{n} = n \\frac{\\mu^2}{n} = \\mu^2$, we can re-write our variance like this\n\n$$\\sigma^2 = \\frac{\\sum(x_i)^2}{n} - 2 \\mu^2 + \\mu^2 = \\frac{\\sum(x_i)^2}{n} - \\mu^2$$\n\n# Coefficient of variation \n\nThe coefficient of variation is not a statistical moment, but considering it is the ratio of the first 2 moments (ratio of sd to the mean), we include it here as well. \nIt allows to compare together 2 different distributions (that have different mean and sd).  \n\n$$CV = \\frac{\\sigma}{\\mu}$$\n\nOr in the case of a sample $CV = \\frac{s}{\\bar{x}}$\n\n# Skewness \n\n::: {.callout-tip} \n\n# Skewness \n\nSkewness is measure of asymmetry of a distribution (or actually lack of).  How symmetric around the mean is the distribution? \nA standard normal distribution is perfectly symmetrical and has zero skew. Other examples of zero-skewed distributions are the T Distribution, the uniform distribution and the Laplace distribution. However, other distributions don’t have zero skew. \n\nIn a zero skew distribution, the mean = the median and the distribution is symmetric. \n\n:::\n\nIn a sense, skewness is quantifying for us how far is the median from the mean.  \n\nMathematically, we define skewness as \n\n$$\\frac{\\frac{1}{n} \\sum(x_i - \\bar{x})^3}{\\sqrt{ \\left( \\frac{1}{n} \\sum(x_i - \\bar{x})^2 \\right)^3}}$$ {#eq-skewness-pop}\n\nIn the case of a sample, we'll multiply @eq-skewness-pop by a factor of $\\frac{\\sqrt{n(n-1)}}{n-2}$\n\n* skenwess = 0 ==> normallly distributed\n* $ -0.5 \\leq skewness \\leq 0.5 $  ==> moderately skew\n* $$\n\n\n# Kurtosis \n\n::: {.callout-tip} \n\n# Kurtosis\n\nKurtosis is a measure that describes the shape of a distribution's tails in relation to its overall shape. A distribution can be infinitely peaked with low kurtosis, and a distribution can be perfectly flat-topped with infinite kurtosis. Thus, kurtosis measures \"tailedness,\" not \"peakedness.\" \n\n:::\n\nBecause, we raised the difference of a data point to its mean to the 4th power, it is really the data points far away from the mean that do participate to the kurtosis. \n\n> \"increasing kurtosis is associated with the “movement of probability mass from the shoulders of a distribution into its center and tails.\"  (Someone important who got published and knew his stuff!) \n\nMathematically, we define kurtosis as \n\n$$\\frac{\\frac{1}{n} \\sum(x_i - \\bar{x})^4}{ \\left( \\frac{1}{n} \\sum(x_i - \\bar{x})^2 \\right)^2}$$ {#eq-skewness-pop}\n\nSome statistical packages are providing **excess kurtosis** by subtracting 3 to the kurtosis value. So for a data set that is perfectly normally distributed, we expect the excess kurtosis to be 0. \n\nThere are 3 categories of kurtosis: leptokurtic (positive excess kurtosis), mesokurtic (aka normal distribution), platykurtic. \n![Type of Kurtosis](kurtosis.png)\n\n* A kurtosis greater than 3 => leptokurtic\n* A kurtosis around 3 => mesokurtic \n* A kurtosis less than 3 => platykurtic\n\nA Student's T distribution with degree of freedom 4 has infinite kurtosis (huge peak and tails + narrow shoulders)\n\nMost equities display a leptokurtic behavior (skinny at the mean - most returns are clustered around the mean) and narrow shoulders and fat tails.  \n\n# Using Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\namd = pd.read_csv('../../raw_data/AMD.csv')\n\nx = amd['adjClose']\n\nreturns_21d = np.log(x / x.shift(21)).dropna()\n\nmean_21dret = np.mean(returns_21d)\nstd_21dret = np.std(returns_21d)\n\nprint(\"The mean rolling 21 days return is: %s\" % round(mean_21dret, 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe mean rolling 21 days return is: -0.00594\n```\n:::\n\n```{.python .cell-code}\nprint(\"The standard deviation of the rolling 21 days return is: %s\" %round(std_21dret, 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe standard deviation of the rolling 21 days return is: 0.1832\n```\n:::\n:::\n\n\nThe standard deviation is quite bigger than the mean.  An histogram of the returns will confirm that. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.hist(returns_21d, bins = 'rice', label = 'Rolling 21-days return')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(array([  2.,   0.,   3.,   4.,   5.,  26.,  31.,  56.,  66., 117., 192.,\n       269., 357., 420., 607., 637., 644., 567., 464., 380., 206., 157.,\n       109.,  98.,  73.,  31.,  28.,  14.,  10.,   8.,   2.,   1.,   0.,\n         3.,   2.,   1.]), array([-0.7537718 , -0.70724263, -0.66071346, -0.6141843 , -0.56765513,\n       -0.52112596, -0.47459679, -0.42806762, -0.38153845, -0.33500928,\n       -0.28848011, -0.24195095, -0.19542178, -0.14889261, -0.10236344,\n       -0.05583427, -0.0093051 ,  0.03722407,  0.08375324,  0.1302824 ,\n        0.17681157,  0.22334074,  0.26986991,  0.31639908,  0.36292825,\n        0.40945742,  0.45598659,  0.50251575,  0.54904492,  0.59557409,\n        0.64210326,  0.68863243,  0.7351616 ,  0.78169077,  0.82821994,\n        0.8747491 ,  0.92127827]), <BarContainer object of 36 artists>)\n```\n:::\n\n```{.python .cell-code}\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe are seeing a larger left tail with indeed the mean looking around 0. \nAn other to visualize this and putting emphasis on the outliers would be to plot the returns on a box-and-whiskers plot. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nplt.boxplot(returns_21d, labels = ['Rolling 21-days return'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'whiskers': [<matplotlib.lines.Line2D object at 0x7ff03ab44370>, <matplotlib.lines.Line2D object at 0x7ff03ab44730>], 'caps': [<matplotlib.lines.Line2D object at 0x7ff03ab449a0>, <matplotlib.lines.Line2D object at 0x7ff03ab44a60>], 'boxes': [<matplotlib.lines.Line2D object at 0x7ff03ab441c0>], 'medians': [<matplotlib.lines.Line2D object at 0x7ff03ab44eb0>], 'fliers': [<matplotlib.lines.Line2D object at 0x7ff03ab45330>], 'means': []}\n```\n:::\n\n```{.python .cell-code}\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.stats import skew, skewtest\n\nskew(returns_21d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.21432056577181507\n```\n:::\n:::\n\n\nThe skew value is quite large and negative which confirms the fat left tail we saw on the histogram \n\n\n::: {.cell}\n\n```{.python .cell-code}\nskewtest(returns_21d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSkewtestResult(statistic=6.477804949429641, pvalue=9.306654969735504e-11)\n```\n:::\n:::\n\n\nVery small p-value.  We reject the null-hypothesis.  The distribution is not symetrical. \n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}