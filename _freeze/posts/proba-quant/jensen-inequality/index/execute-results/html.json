{
  "hash": "ac28d266f360ba425746dffdc2efa6fb",
  "result": {
    "markdown": "---\ntitle: \"Jensen's Inequality\"\nauthor: \"Francois de Ryckel\"\ndate: \"2023-10-21\"\ncategories: [Probability, Jensen's inequality, arithmetic mean, geometric mean]\nimage: 'jensen-parabola.png'\ndate-modified: \"2023-10-22\"\n---\n\nJensen's inequality from [Johan Jensen](https://en.wikipedia.org/wiki/Johan_Jensen_(mathematician)), Danish Mathematician, has popped up several times through my quantitative finance journey. As I never explicitly dealt with it, I thought I'll make a post about it.  \n\nThe question Jensen's inequality address is how does the mean of a function relate the function of the mean.  First, we'll check how in the case of linear functions, there are no difference there.  Then we'll go on more complex functions where their concavity (aka second derivative) matters. \n\n# Linear transformations of Random Variables \n\nWhat happen when we transform in a linear way a random variable? \n\nWe claim that as long as *f* is linear: **mean(f(x)) == f(mean(x))**  \n\nLet's use an easy & simple example.  A fair dice is thrown.  Let's imagine a payoff which is a linear function: say, the payoff is $2 * \\text{(face of the die)} + 3$.  \n\nIn this case $X$ is a discrete random variable that can take the values ${1, 2, 3, 4, 5, 6}$.  \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\n# function to define the payoff\ndef payoff(x): \n  return 2*x+3\n\n# let's define the outcome of our discrete random variable. \noutcomes = [1, 2, 3, 4, 5, 6]\n\n# let's calculate the payoff for each outcome\npayoffs = [payoff(outcome) for outcome in outcomes]\n```\n:::\n\n\nWith linear transformations and in this example, we have **the mean of the payoff that is equal to the payoff of the mean**. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nprint(f\"The mean of the outcomes [1, 2, 3, 4, 5, 6] is {np.mean(outcomes)}\")\nprint(f\"The payoff of the mean is {payoff(np.mean(outcomes))}\")\nprint(f\"The payoffs are {payoffs} and the mean of the payoffs is {np.mean(payoffs)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe mean of the outcomes [1, 2, 3, 4, 5, 6] is 3.5\nThe payoff of the mean is 10.0\nThe payoffs are [5, 7, 9, 11, 13, 15] and the mean of the payoffs is 10.0\n```\n:::\n:::\n\n\nNow this equality doesn't hold anymore when it comes to non-linear function.  \n\n# Convex transformations of Random Variables\n\nThe **Jensen's inequality** is in regards to **convex** functions (aka its second derivative is positive on an interval) and states that *the mean of the transformation is always greater or equal to the transformation of the mean*.  \n\nUsing the language of probability, we can write that for any convex functions $$\\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X])$$ {#eq-jensineq}\n\nAgain let's take an easy example with the convex function $f(x) = X^2$.  We will use the same inputs (the roll of a die) as the above example. \n\nThe intuition is that the mean of squared numbers is greater than the square of the mean numbers.  This is because bigger number squared will add extreme value (on the high side) to the mean.  \n\nUsing a small Python script.  \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\noutcomes = [1, 2, 3, 4, 5, 6]\n\n# defining our transform functions. \ndef payoff(x): \n  return x**2\n\npayoffs = [payoff(outcome) for outcome in outcomes]\npayoffs\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n[1, 4, 9, 16, 25, 36]\n```\n:::\n:::\n\n\nLet's now calculate both $\\mathbb{E}[f(X)]$ and  $f(\\mathbb{E}[X])$ \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nprint(f\"The mean of the transformed outcomes is {round(np.mean(payoffs), 3)}\")\nprint(f\"The transformed of the mean is {payoff(np.mean(outcomes))}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe mean of the transformed outcomes is 15.167\nThe transformed of the mean is 12.25\n```\n:::\n:::\n\n\nWhen is it an equality then?  When all the observations in the sets have the exact same values, then $\\mathbb{E}[f(X)] = f(\\mathbb{E}[X])$.  \n\nVisually, we could see this on the parabola. \n![The mean of the squares greater than the square of the mean](jensen-parabola.png)\n\n## Simulation \n\nTo make our experiment a bit more realistic, we could simulate 50 roll of dice and check that the Jensen's inequality still hold. Our convex function is stil $f(x)=X^2$. \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\noutcomes = np.random.randint(1, 7, 50)\n\ndef payoff(x): \n  return x**2\n\npayoffs = [payoff(outcome) for outcome in outcomes]\n```\n:::\n\n\nAnd let's again compare the mean of the transformed vs the transformed of the mean. \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nprint(f\"The mean of the transformed outcomes is {np.mean(payoffs)}\")\nprint(f\"The transformed of the mean is {payoff(np.mean(outcomes))}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe mean of the transformed outcomes is 14.2\nThe transformed of the mean is 11.0224\n```\n:::\n:::\n\n\n# Jensen's inequality in finance \n\n## Arithmetic mean vs Geometric mean\n\nOne way, the Jensen's inequality is used in finance is when it comes to returns.  We can indeed compute the average returns as a arithmetic average or as a geometric average. \n\nLet's say, we have a data set with n observations. Then we define, the arithmetic mean as \n$$AM = \\frac{1}{n} \\sum_{i=1}^n i$$ \nand the geometric mean as \n$$GM = \\left( \\prod_{i=1}^n i \\right)^{\\frac{1}{n}}$$ \n\nUsing logarithms, and starting with the arithmetic mean, we have: \n$$log(AM) = log \\left( \\frac{1}{n} \\sum_{i=1}^n i \\right)$$ {#eq-logAM} \n\nContinuing with the geometric mean, we have: \n$$log (GM) = log \\left( \\prod_{i=1}^n i \\right)^{\\frac{1}{n}} = \\frac{1}{n} log \\left( \\prod_{i=1}^n i \\right) = \\frac{1}{n} \\sum_{i=1}^n log(i)$$ {#eq-logGM}\n\nWe could transform @eq-logGM saying that the geometric mean is the exponential of the arithmetic mean.\n$$GM = exp \\left( \\frac{1}{n} \\sum_{i=1}^n i \\right)$$ {#eq-logGM2}\n\nBack to Jensens's inequality.  \nThe log function is a concave function.  We just re-write @eq-jensineq changing the inequality sign: *the mean of the transformation is less or equal to the transformation of the mean*.  On probabilistic terms, $$\\mathbb{E}[f(X)] \\leq f(\\mathbb{E}[X])$$\n\n\n$$log(GM) = \\text{mean of the transformation (log)}$$\n$$log(AM) = \\text{transformation (log) of the mean}$$ \n\nIf we go back to our dice throwing example, we should see that the $log(GM) <= log(AM)$.  Let's model that\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\noutcomes = [1, 2, 3, 4, 5, 6]\nmean_outcomes = np.mean(outcomes)\n\ndef payoff(x): \n  return np.log(x)\n\npayoffs = [payoff(outcome) for outcome in outcomes]\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nprint(f\"Log(GM) - The mean of the transformed (log) is {np.mean(payoffs)}\")\nprint(f\"Log(AM) - The transformed (log) of the mean is {np.log(np.mean(outcomes))}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLog(GM) - The mean of the transformed (log) is 1.0965418686683501\nLog(AM) - The transformed (log) of the mean is 1.252762968495368\n```\n:::\n:::\n\n\nThis match our initial statement.  mean of the transformed being smaller than the transformed of the mean as the transformation is concave! \n\nAnd we can undo that log using an exponent (which will preserve the inequality sign). Hence GM < AM. In finance, especially portfolio management, GM means are preferred as it is taking into account the compounding effect of the returns. \n\n## Options convexity \n\nWe are talking about the convexity of options to draw the attention on the non-linear relationship (in this case convex relationship ... duh!) between the option's price and the price of its underlying asset.  \nThis comes from the second order derivative of the option's price in regards to the price of the underlying.  In the Black-Schole Merton equation, this can be seen as: \n$$\\frac{\\partial{V}}{\\partial{t}} + \\color{blue}{\\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2{V}}{\\partial{S}^2}} + r S \\frac{\\partial{V}}{\\partial{S}} - rV = 0$$\n\nRecall the value of a vanilla European call option at expiration is $C(S) = max(S-K, 0)$ where: \n\n* $K$ is the strike price\n* $S$ is the price of the underlying \n* $C(S)$ is the value of the option price at expiry\n* $S-K$ is a linear relationship \n\nBut the rate of change of the option price in regards to its underlying is not linear. \n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nS0 = 100     # Initial stock price\nK = 100      # Strike price\nr = 0.05     # Risk-free rate\nT = 1.0      # Time to maturity\nsigma = 0.2  # Volatility\n\nS = np.linspace(80, 120, 100)\n\n# Black-Scholes call option price formula\nd1 = (np.log(S / K) + (r + (sigma**2) / 2) * T) / (sigma * np.sqrt(T))\nd2 = d1 - sigma * np.sqrt(T)\n\nC = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n\ndelta = np.gradient(C, S)\ngamma = np.gradient(delta, S)\n\nplt.figure(figsize = (12, 6))\n\nplt.subplot(131)\nplt.plot(S, C, label = 'Option Price')\nplt.xlabel('Asset Price')\nplt.ylabel('Option Value')\n\nplt.subplot(132)\nplt.plot(S, delta, label = 'Delta')\nplt.xlabel('Asset Price')\nplt.ylabel('Delta')\n\nplt.subplot(133)\nplt.plot(S, gamma, label = 'Gamma')\nplt.xlabel('Asset Price')\nplt.ylabel('Gamma')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=1141 height=566}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}