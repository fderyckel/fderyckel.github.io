{
  "hash": "45788cb3d43d3430d9895c420315179a",
  "result": {
    "markdown": "---\ntitle: \"KNN\"\nauthor: \"Fran√ßois de Ryckel\"\ndate: \"2023-11-14\"\ncategories: [ML, KNN]\ndescription: 'Using KNN in both python and R'\neditor: source\ndate-modified: \"2023-11-14\"\n---\n\nSome very basic ML using KNN and python and the tidymodel framework. \n\n# Introduction \n\nKNN stands for *K Nearest Neighbor*.  \n\nKNN is not really a machine learning techniques in the sense that it trains a model. Nonetheless, it is a supervised ML algorithm that can be used for both classification and regression.  The intuition behind the model is that observations that are closed to each other (close in terms of distance in a hyperplane) have similar labels (classification) or values (regression).  \n\nAs mentioned, there is no training phase when using KNN.  Instead, there is only prediction.  \nWe take an observation and check the **K** observations next to it.  We check the label of the K observations next to our data to be labeled and using a majority voting system we assign the label. For regression, it calculates the average or weighted average of the target values of the K neighbors to predict the value for the input data point.\n\n![KNN illustration](knn01.png) \n\nLooking at the above image, we can see that, using k=3, the 3 observations closest to the star (our data to be classified) are all brown circle.  Hence we should classify the star as a brown circle instead of an orange rectangle. \n\n::: {.callout-caution appearance=\"simple\"} \n\n## Scaling\n\nBecause KNN use distance, it is important to scale the data as a pre-processing steps.  Otherwise, features with big scale (let's say price) will skew the distance against features with lower scale (let's say percentage).  \n\n:::\n\n## Pros-Cons of KNN \n\n### Pros \n\n* KNN is non-parametric. It's not making any assumptions on the the type of distribution of the data. \n\n### Cons\n\n* non-efficient in terms of memory \n* non-efficient on speed of execution with new data \n\n# Example in Python \n\n## Starting example \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nx = [4, 5, 10, 4, 3, 11, 14 , 8, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\nclasses = [0, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(x, y, c = classes)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<matplotlib.collections.PathCollection at 0x16e1f9cd0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=566 height=411}\n:::\n:::\n\n\nNow let's create a KNN object and a new point \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(list(zip(x, y)), classes)\n\nnew_x = 3\nnew_y = 20\nnew_point = [(new_x, new_y)]\nprediction = knn.predict(new_point)\n\nplt.scatter(x + [new_x], y + [new_y], c = classes + [prediction[0]])\nplt.text(x = new_x-1, y = new_y-1, s = f\"new point, class:{prediction[0]}\")\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nText(2, 19, 'new point, class:0')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=566 height=411}\n:::\n:::\n\n\n## Example with synthetic data \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_blobs\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\n# create our synthetic data\nX, y = make_blobs(n_samples = 1000, n_features = 2, \n                  centers = 4, cluster_std = 1.9, \n                  random_state = 4)\n\n\n# visualizing the dataset \nplt.scatter(X[:,0], X[:,1], c = y, s = 20)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n<matplotlib.collections.PathCollection at 0x16e32ea90>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=577 height=411}\n:::\n:::\n\n\nSplitting our dataset into training & testing + running KNN on the data \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# spliting our data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\nknn5 = KNeighborsClassifier(n_neighbors = 5)\nknn19 = KNeighborsClassifier(n_neighbors = 19)\n\n# fit our 'model' with either '5' or '19' Nearest Neighbors\nknn5.fit(X_train, y_train)\nknn19.fit(X_train, y_train)\n\n# apply prediction on our test set\ny_pred_5 = knn5.predict(X_test)\ny_pred_19 = knn19.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\nprint('Accuracy with K = 5 is', round(accuracy_score(y_test, y_pred_5)*100, 2), '%')\nprint('Accuracy with k = 19 is', round(accuracy_score(y_test, y_pred_19)*100, 2), '%')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy with K = 5 is 87.6 %\nAccuracy with k = 19 is 89.6 %\n```\n:::\n:::\n\n\nLet's visualize both 'models' and the impact of the choice of K. \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n#using subplots to compare\nplt.figure(figsize = (9, 5))\n\n# first subplot\nplt.subplot(1, 2, 1)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred_5, s=21)\nplt.title('Predictions with K=5')\n\n# second subplot\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred_19, s=21)\nplt.title('Prediction with K=19')\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nText(0.5, 1.0, 'Prediction with K=19')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=726 height=431}\n:::\n:::\n\n\nBecause the data are already pretty well separted, the only changes we see easily are the ones in the junction between the blue and purple dots. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}