{
  "hash": "0687e0d1296bbe28a07c82f0183eeec7",
  "result": {
    "markdown": "---\ntitle: \"Naive-Bayes - Part 1\"\nauthor: \"Francois de Ryckel\"\ndate: \"2023-05-16\"\ncategories: [ML, Naive-Bayes, sentiment analysis]\ndescription: 'Making Naive-Bayes work in R'\neditor: source\ndate-modified: \"2023-05-16\"\n---\n\n\nSome very basic ML using Naive-Bayes and the tidymodel framework. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(dplyr)  # mutate(), row_number()\n\ndf <- read_csv('../../../raw_data/financial_news.csv', col_names = c('sentiment', 'text')) |> \n  mutate(sentiment = factor(sentiment))\n```\n:::\n\n\n\nUsing the tidyverse, we'll \n\n* split the df into a training and testing set.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rsample)    # initial_split(), training(), testing()\nlibrary(recipes)\nlibrary(parsnip)    # naive_bayes(), set_engine()\nlibrary(workflows)  # workflow()\n\nlibrary(discrim)\nlibrary(textrecipes)\nlibrary(yardstick)\n\nlist_splits <- initial_split(df, prop = 0.8, strata = 'sentiment')\ndf_train <- training(list_splits)\ndf_test <- testing(list_splits)\n\nlist_recipe <- recipe(sentiment ~., data = df_train) |> \n  step_tokenize(text) |> \n  step_stopwords(text) |> \n  step_tokenfilter(text, max_tokens = 100) |> \n  step_tfidf(text)\n  \n\nmod_nb <- naive_Bayes() |> set_engine('naivebayes') |> set_mode('classification')\nmod_svm <- svm_poly() |> set_engine('kernlab') |> set_mode('classification')\nlist_cv <- vfold_cv(df_train, v = 5, strata = 'sentiment')\n\nwf_nb <- workflow() |> add_recipe(list_recipe) |> add_model(mod_nb)\nwf_nb\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_tokenize()\n• step_stopwords()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n```\n:::\n\n```{.r .cell-code}\nwf_svm <- workflow() |> add_recipe(list_recipe) |> add_model(mod_svm)\n\nfit_mod_nb <- fit(wf_nb, df_train)\npred_mod_nb <- predict(fit_mod_nb, df_test)\npred_mod_nb_prob <- predict(fit_mod_nb, df_test, type = 'prob')\n\nfit_mod_svm <- fit(wf_svm, df_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Setting default kernel parameters  \n```\n:::\n\n```{.r .cell-code}\npred_mod_svm <- predict(fit_mod_svm, df_test)\npred_mod_svm_prob <- predict(fit_mod_svm, df_test, type = 'prob')\n\nbind_cols(df_test, pred_mod_nb) |> conf_mat(sentiment, .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction negative neutral positive\n  negative       65      55       81\n  neutral        53     509      168\n  positive        3      12       24\n```\n:::\n\n```{.r .cell-code}\nbind_cols(df_test, pred_mod_svm) |> conf_mat(sentiment, .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction negative neutral positive\n  negative       31       6       17\n  neutral        71     526      181\n  positive       19      44       75\n```\n:::\n\n```{.r .cell-code}\n#roc_nb <- bind_cols(df_test, pred_mod_nb_prob) |> roc_curve()\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}