{
  "hash": "9f12ca6c6cc015f735412aef026113d3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"PCA\"\nauthor: \"François de Ryckel\"\ndate: \"2024-10-06\"\ncategories: [ML, PCA, Covariance]\ndescription: 'Understanding Principal Component Analysis'\neditor: source\nhtml-math-method: mathml\ndate-modified: \"2024-10-06\"\n---\n\n\n\nPrincipal Component Analysis is a widely used method to reduce the dimensionality of a dataset as well as to de-correlate it.  It can also be used to weight the importance of variables.  The PCA transforms variables into another set of variables called *Principal Components*.  \n\nIt takes the data and tries to find a direction (let's say vector l) such that variance of points projected on vector l is maximum.  \n\nThis is unsupervised learning.  So we don't need the label of that data set.  \n\nLet's take an example without label.\n\n# Example 1 \n\nIn our very basic fictious example, we have 3 variables. \n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.panel-tabset}\n\n## R  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\ndf <- tibble(english = c(90, 90, 60, 60, 30), \n             math = c(60, 90, 60, 60, 30), \n             art = c(90, 30, 60, 90, 30))\n\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n  english  math   art\n    <dbl> <dbl> <dbl>\n1      90    60    90\n2      90    90    30\n3      60    60    60\n4      60    60    90\n5      30    30    30\n```\n\n\n:::\n:::\n\n\n## Python \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\ndf_py = pd.DataFrame({'english': [90, 90, 60, 60, 30], \n                      'math': [60, 90, 60, 60, 30], \n                      'art': [90, 30, 60, 90, 30]})\n\ndf_py\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   english  math  art\n0       90    60   90\n1       90    90   30\n2       60    60   60\n3       60    60   90\n4       30    30   30\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n## Step 1: find the mean of each variable \n\n::: {.panel-tabset}\n\n## R  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf |> summarise(across(everything(), mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  english  math   art\n    <dbl> <dbl> <dbl>\n1      66    60    60\n```\n\n\n:::\n\n```{.r .cell-code}\n# or another way\ncolMeans(as.matrix(df))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nenglish    math     art \n     66      60      60 \n```\n\n\n:::\n:::\n\n\n\n## Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_py.mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nenglish    66.0\nmath       60.0\nart        60.0\ndtype: float64\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## Step 2: Compute the Covariance matrix of the whole dataset  \n\nAs a reminder, we find the covariance between 2 variables $X, Y$ as \n$$cov(X, Y) = \\frac{1}{n-1} \\cdot \\sum_{i=1}^{n} \\left( (x_i - \\bar{x}) (y_i - \\bar{y}) \\right)$$\n\nSo let's show the covariance of English and Math. \n\n* Mean of english $= 66$\n* Mean of math $= 60$\n$$\\frac{(90 - 66) \\cdot (60-60) + (90 - 66) \\cdot (90-60) + (60 - 66) \\cdot (60-60) + (60 - 66) \\cdot (60-60) + (30 - 66) \\cdot (30-60)}{4}$$\n$$\\frac{24 \\cdot 0 + 24 \\cdot 30 + -6 \\cdot 0 + -6 \\cdot 0 + -36 \\cdot -30}{4}$$\n$$\\frac{0 + 720 + 0 + 0 + 1080}{4} = \\frac{1800}{4} = 450$$\n\n\n\n::: {.panel-tabset}\n\n## R  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncov(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        english math art\nenglish     630  450 225\nmath        450  450   0\nart         225    0 900\n```\n\n\n:::\n\n```{.r .cell-code}\n# or using matrix\ncov(as.matrix(df))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        english math art\nenglish     630  450 225\nmath        450  450   0\nart         225    0 900\n```\n\n\n:::\n:::\n\n\n\n## Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_py.cov()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         english   math    art\nenglish    630.0  450.0  225.0\nmath       450.0  450.0    0.0\nart        225.0    0.0  900.0\n```\n\n\n:::\n:::\n\n\n\n:::\n\nUsing matrices, another way to compute the covariance matrix is the following: \n\n$$\\frac{1}{n-1} \\left( \\textbf{X} - \\bar{X} \\right)^T \\cdot \\left( \\textbf{X} - \\bar{X} \\right)$$\n\nRemember, the positive covariance between math and english indicates that both subject covary in the same direction.  And the null covariance between math and art indicates that there is no predictable relationship between the art and math subject.  \n\n## Step 3: Compute the eigenvectors and eigenvalues of the covariance matrix  \n\nRecall that the eigenvectors satifies the following relationship: \n\n$$\\textbf{A}\\cdot v = {\\lambda} \\cdot v$$\n$$\\left( \\textbf{A} - \\lambda \\right) v = 0$$ \n$$det\\left( \\textbf{A} - \\lambda \\textbf{I} \\right) = 0$$\n\n::: {.panel-tabset}\n\n## R  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\neigen(as.matrix(cov(df)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neigen() decomposition\n$values\n[1] 1137.58744  786.38798   56.02458\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.6558023 -0.3859988  0.6487899\n[2,] -0.4291978 -0.5163664 -0.7410499\n[3,] -0.6210577  0.7644414 -0.1729644\n```\n\n\n:::\n:::\n\n\n\n## Python\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\n\ndf_py_cov_mat = df_py.cov()\n\neigenvalues, eigenvectors = np.linalg.eig(df_py_cov_mat)\nprint(eigenvalues)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[  56.02457535 1137.5874413   786.38798335]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(eigenvectors)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[ 0.6487899  -0.65580225 -0.3859988 ]\n [-0.74104991 -0.4291978  -0.51636642]\n [-0.17296443 -0.62105769  0.7644414 ]]\n```\n\n\n:::\n:::\n\n\n\n:::\n\nIt is serendipity that the first eigenvectors is the highest (aka explained most of the variance).  The second one is the second highest and third one is last.  \n\nThis is a 3D space with each eigen vector being orthogonal to the other.  In an N-dimensional space, each eigenvectors are orthogonal. \n\nTo find the percentage of variance expalained by the eigenvalue $k$ (where $k$ is one of the dimension), we compute: \n\n$$\\frac{\\lambda_k}{\\sum_{i=1}^{n}}$$\n\n## Step 4: Compute the new data frame based on the principal components. \n\nTo transform the eigenvectors to the new subspace we used: \n$$\\textbf{W}^t \\cdot X$$\n\n* $X$ is our initial data matrix.  Our df in the above steps\n* $\\textbf{W}^t$ is the transpose of the eigenvector matrix. \n\n::: {.panel-tabset}\n\n## R  \n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Python\n\n\n\n::: {.cell}\n\n:::\n\n\n\n:::\n\n\n# Another Example. \n\n\n::: {.panel-tabset}\n\n## R  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_mat <- matrix(NA, nrow = 10, ncol = 2)\nx_mat[, 1] <- c(14.3, 12.2, 13.7, 12.0, 13.4, 14.3, 13.0, 14.8, 11.1, 14.3)\nx_mat[, 2] <- c(7.5, 5.5, 6.7, 5.1, 5.2, 6.3, 7.6, 7.3, 5.3, 7.2)\n```\n:::\n\n\n\n## Python \n\n:::\n\n\nFind the covariance matrix.  \nRemember we d\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}