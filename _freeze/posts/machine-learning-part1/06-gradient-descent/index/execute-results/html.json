{
  "hash": "dfacc8d70695a445fbe0afb28a609460",
  "result": {
    "markdown": "---\ntitle: \"Gradient Descent\"\nauthor: \"Francois de Ryckel\"\ndate: \"2024-04-26\"\ncategories: [Regression, Lasso, Ridge]\neditor: source\ndate-modified: '2024-04-30'\n---\n\nGradient Descent is an optimization technique used in many Machine Learning algorithms to find the minimum of a function.  It does require a convex and differentiable function to ensure we have a minimum. \nAt its core it's like searching for the lowest valley in a hilly landscape. The idea is to take **small steps** in the **steepest downhill direction** until you reach the lowest point. \n\nGradient Descent is used to find the parameters of the cost function.  Think the parameters in the linear regression for instance. \n\n# Basic Gradient Descent \n\nOne of the main disadvantage of gradient descent is getting stuck to a local minimum or a saddle point and not finding the global minimum. \n\nTo perform a gradient descent, we need \n\n* a function\n* its derivative \n* a starting point (where do we start going down)\n* the size of steps (aka **learning rate**)\n* the number of iterations \n* optionally, we can set a threshold for when we stop the iterative process of going down the hill.  \n\n$$f(x) = 0.91 X^2 + 11x - 7$$\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# the original function \ndef cost_function(x): \n  return(3.91*x**2 + 5*x - 59)\n\n# the derivative of our function \ndef gradient(x): \n  return(3.91*x+5)\n\ncost_function(4.5)\n\n# quick visual check \nx = np.linspace(-10, 10, 1000)\ny = cost_function(x)\n\nplt.clf()\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('cost_function(x)')\nplt.title('Plot of the cost_function(x) vs x')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=593 height=449}\n:::\n:::\n\n\nWe can now put everything together and define our gradient descent function.  \n\n$$x_{n+1} = x_n - {Gradient} \\space \\cdot \\space {Learning \\space Rate}$$\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef gradient_descent(f, deriv, start, learning_rate, n_iter): \n  x = start\n  for i in range(n_iter): \n    grad = gradient(x)\n    \n    # we now update x\n    x = x - learning_rate * grad \n    \n  print(f\"Minimum value of x: {x:.2f}\")\n  print(f\"Minimum value for our Cost function: {cost_function(x):.3f}\")\n\ngradient_descent(cost_function, gradient, 15, 0.01, 10000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMinimum value of x: -1.28\nMinimum value for our Cost function: -59.000\n```\n:::\n:::\n\n\nWe could change slightly our code to store the iterations for visualization or analysis. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef gradient_descent(f, deriv, start, learning_rate, n_iter): \n  x = start\n  # initialize a list to store values \n  cost_values = []\n  \n  for i in range(n_iter): \n    cost = cost_function(x)\n    grad = gradient(x)\n    # update of x\n    x = x - learning_rate * grad\n    # append the value of cost to the list\n    cost_values.append(cost)\n    # print the progress\n    if i % 10 == 0  and i < 200: \n      print(f\"Iteration {i}: x = {x:.4f}, cost = {cost:.4f}\")\n  \ngradient_descent(cost_function, gradient, \n                 start = np.random.randn(), learning_rate = 0.01, \n                 n_iter = 1000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIteration 0: x = -0.1305, cost = -59.3916\nIteration 10: x = -0.5082, cost = -60.4952\nIteration 20: x = -0.7616, cost = -60.5584\nIteration 30: x = -0.9317, cost = -60.2958\nIteration 40: x = -1.0459, cost = -59.9822\nIteration 50: x = -1.1225, cost = -59.7098\nIteration 60: x = -1.1739, cost = -59.4992\nIteration 70: x = -1.2084, cost = -59.3453\nIteration 80: x = -1.2315, cost = -59.2364\nIteration 90: x = -1.2471, cost = -59.1607\nIteration 100: x = -1.2575, cost = -59.1088\nIteration 110: x = -1.2645, cost = -59.0734\nIteration 120: x = -1.2692, cost = -59.0495\nIteration 130: x = -1.2723, cost = -59.0333\nIteration 140: x = -1.2745, cost = -59.0224\nIteration 150: x = -1.2759, cost = -59.0150\nIteration 160: x = -1.2768, cost = -59.0101\nIteration 170: x = -1.2775, cost = -59.0068\nIteration 180: x = -1.2779, cost = -59.0046\nIteration 190: x = -1.2782, cost = -59.0031\n```\n:::\n:::\n\n\nWe could visualize how the process happen.  We'll return the *cost_values* list for that to our function. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# the original function \ndef cost_function(x): \n  return(0.91*x**2 + 5*x - 59)\n\n# the derivative of our function \ndef gradient(x): \n  return(0.91*x+5)\n\ndef gradient_descent(f, deriv, start, learning_rate, n_iter): \n  x = start\n  cost_values = []\n  x_list = [start]\n  \n  for i in range(n_iter): \n    cost = cost_function(x)\n    grad = gradient(x)\n    x = x - learning_rate * grad\n    cost_values.append(cost)\n    x_list.append(x)\n    print(\"Iteration {}: x = {}, cost = {}\".format(i, x, cost))\n      \n  return(x_list)\n\nx_list = gradient_descent(cost_function, gradient, \n                          start = 7, learning_rate = 0.3, \n                          n_iter = 5)\n\n\nx = np.linspace(-10, 10, 50)\ny = cost_function(x)\n\nplt.clf()\nplt.plot(x, y)\nfor i in range(len(x_list) - 1):\n    x1 = x_list[i]\n    y1 = cost_function(x1)\n    x2 = x_list[i + 1]\n    y2 = cost_function(x2)\n    plt.plot([x1, x2], [y1, y2], 'ro--')\n    plt.text(x1 + 0.5, y1 - 2, round(y1, 2))\n# Label the final cost value\nx_final = x_list[-1]\ny_final = cost_function(x_final)\nplt.text(x_final, y_final - 5, round(y_final, 2))\n\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Gradient Descent for f(x)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIteration 0: x = 3.589, cost = 20.590000000000003\nIteration 1: x = 1.109203, cost = -29.33336189\nIteration 2: x = -0.693609419, cost = -52.33438352135981\nIteration 3: x = -2.004254047613, cost = -62.03025153122578\nIteration 4: x = -2.9570926926146512, cost = -65.36576903655549\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=596 height=449}\n:::\n:::\n\n\n# Stochastic Gradient Descent \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}