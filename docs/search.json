[
  {
    "objectID": "my_env/lib/python3.10/site-packages/pyzmq-24.0.1.dist-info/AUTHORS.html",
    "href": "my_env/lib/python3.10/site-packages/pyzmq-24.0.1.dist-info/AUTHORS.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "This project was started and continues to be led by Brian E. Granger (ellisonbg AT gmail DOT com). Min Ragan-Kelley (benjaminrk AT gmail DOT com) is the primary developer of pyzmq at this time.\nThe following people have contributed to the project:\n\nAlexander Else (alexander DOT else AT team DOT telstra DOT com)\nAlexander Pyhalov (apyhalov AT gmail DOT com)\nAlexandr Emelin (frvzmb AT gmail DOT com)\nAmr Ali (amr AT ledgerx DOT com)\nAndre Caron (andre DOT l DOT caron AT gmail DOT com)\nAndrea Crotti (andrea DOT crotti DOT 0 AT gmail DOT com)\nAndrew Gwozdziewycz (git AT apgwoz DOT com)\nBaptiste Lepilleur (baptiste DOT lepilleur AT gmail DOT com)\nBrandyn A. White (bwhite AT dappervision DOT com)\nBrian E. Granger (ellisonbg AT gmail DOT com)\nBrian Hoffman (hoffman_brian AT bah DOT com)\nCarlos A. Rocha (carlos DOT rocha AT gmail DOT com)\nChris Laws (clawsicus AT gmail DOT com)\nChristian Wyglendowski (christian AT bu DOT mp)\nChristoph Gohlke (cgohlke AT uci DOT edu)\nCurtis (curtis AT tinbrain DOT net)\nCyril Holweck (cyril DOT holweck AT free DOT fr)\nDan Colish (dcolish AT gmail DOT com)\nDaniel Lundin (dln AT eintr DOT org)\nDaniel Truemper (truemped AT googlemail DOT com)\nDouglas Creager (douglas DOT creager AT redjack DOT com)\nEduardo Stalinho (eduardooc DOT 86 AT gmail DOT com)\nEren Güven (erenguven0 AT gmail DOT com)\nErick Tryzelaar (erick DOT tryzelaar AT gmail DOT com)\nErik Tollerud (erik DOT tollerud AT gmail DOT com)\nFELD Boris (lothiraldan AT gmail DOT com)\nFantix King (fantix DOT king AT gmail DOT com)\nFelipe Cruz (felipecruz AT loogica DOT net)\nFernando Perez (Fernando DOT Perez AT berkeley DOT edu)\nFrank Wiles (frank AT revsys DOT com)\nFélix-Antoine Fortin (felix DOT antoine DOT fortin AT gmail DOT com)\nGavrie Philipson (gavriep AT il DOT ibm DOT com)\nGodefroid Chapelle (gotcha AT bubblenet DOT be)\nGreg Banks (gbanks AT mybasis DOT com)\nGreg Ward (greg AT gerg DOT ca)\nGuido Goldstein (github AT a-nugget DOT de)\nIan Lee (IanLee1521 AT gmail DOT com)\nIonuț Arțăriși (ionut AT artarisi DOT eu)\nIvo Danihelka (ivo AT danihelka DOT net)\nIyed (iyed DOT bennour AT gmail DOT com)\nJim Garrison (jim AT garrison DOT cc)\nJohn Gallagher (johnkgallagher AT gmail DOT com)\nJulian Taylor (jtaylor DOT debian AT googlemail DOT com)\nJustin Bronder (jsbronder AT gmail DOT com)\nJustin Riley (justin DOT t DOT riley AT gmail DOT com)\nMarc Abramowitz (marc AT marc-abramowitz DOT com)\nMatthew Aburn (mattja6 AT gmail DOT com)\nMichel Pelletier (pelletier DOT michel AT gmail DOT com)\nMichel Zou (xantares09 AT hotmail DOT com)\nMin Ragan-Kelley (benjaminrk AT gmail DOT com)\nNell Hardcastle (nell AT dev-nell DOT com)\nNicholas Pilkington (nicholas DOT pilkington AT gmail DOT com)\nNicholas Piël (nicholas AT nichol DOT as)\nNick Pellegrino (npellegrino AT mozilla DOT com)\nNicolas Delaby (nicolas DOT delaby AT ezeep DOT com)\nOndrej Certik (ondrej AT certik DOT cz)\nPaul Colomiets (paul AT colomiets DOT name)\nPawel Jasinski (pawel DOT jasinski AT gmail DOT com)\nPhus Lu (phus DOT lu AT gmail DOT com)\nRobert Buchholz (rbu AT goodpoint DOT de)\nRobert Jordens (jordens AT gmail DOT com)\nRyan Cox (ryan DOT a DOT cox AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nScott Maxwell (scott AT codecobblers DOT com)\nScott Sadler (github AT mashi DOT org)\nSimon Knight (simon DOT knight AT gmail DOT com)\nStefan Friesel (sf AT cloudcontrol DOT de)\nStefan van der Walt (stefan AT sun DOT ac DOT za)\nStephen Diehl (stephen DOT m DOT diehl AT gmail DOT com)\nSylvain Corlay (scorlay AT bloomberg DOT net)\nThomas Kluyver (takowl AT gmail DOT com)\nThomas Spura (tomspur AT fedoraproject DOT org)\nTigger Bear (Tigger AT Tiggers-Mac-mini DOT local)\nTorsten Landschoff (torsten DOT landschoff AT dynamore DOT de)\nVadim Markovtsev (v DOT markovtsev AT samsung DOT com)\nYannick Hold (yannickhold AT gmail DOT com)\nZbigniew Jędrzejewski-Szmek (zbyszek AT in DOT waw DOT pl)\nhugo shi (hugoshi AT bleb2 DOT (none))\njdgleeson (jdgleeson AT mac DOT com)\nkyledj (kyle AT bucebuce DOT com)\nspez (steve AT hipmunk DOT com)\nstu (stuart DOT axon AT jpcreative DOT co DOT uk)\nxantares (xantares AT fujitsu-l64 DOT (none))\n\nas reported by:\ngit log --all --format='- %aN (%aE)' | sort -u | sed 's/@/ AT /1' | sed -e 's/\\.\\([^ ]\\)/ DOT \\1/g'\nwith some adjustments.\n\n\n\nBrandon Craig-Rhodes (brandon AT rhodesmill DOT org)\nEugene Chernyshov (chernyshov DOT eugene AT gmail DOT com)\nCraig Austin (craig DOT austin AT gmail DOT com)\n\n\n\n\n\nTravis Cline (travis DOT cline AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nZachary Voase (z AT zacharyvoase DOT com)"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "href": "my_env/lib/python3.10/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "from imp import reload"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-for-nbagg-backend.",
    "href": "my_env/lib/python3.10/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-for-nbagg-backend.",
    "title": "QUANT dira-t-on?",
    "section": "UAT for NbAgg backend.",
    "text": "UAT for NbAgg backend.\nThe first line simply reloads matplotlib, uses the nbagg backend and then reloads the backend, just to ensure we have the latest modification to the backend code. Note: The underlying JavaScript will not be updated by this process, so a refresh of the browser after clearing the output and saving is necessary to clear everything fully.\n\nimport matplotlib\nreload(matplotlib)\n\nmatplotlib.use('nbagg')\n\nimport matplotlib.backends.backend_nbagg\nreload(matplotlib.backends.backend_nbagg)\n\n\nUAT 1 - Simple figure creation using pyplot\nShould produce a figure window which is interactive with the pan and zoom buttons. (Do not press the close button, but any others may be used).\n\nimport matplotlib.backends.backend_webagg_core\nreload(matplotlib.backends.backend_webagg_core)\n\nimport matplotlib.pyplot as plt\nplt.interactive(False)\n\nfig1 = plt.figure()\nplt.plot(range(10))\n\nplt.show()\n\n\n\nUAT 2 - Creation of another figure, without the need to do plt.figure.\nAs above, a new figure should be created.\n\nplt.plot([3, 2, 1])\nplt.show()\n\n\n\nUAT 3 - Connection info\nThe printout should show that there are two figures which have active CommSockets, and no figures pending show.\n\nprint(matplotlib.backends.backend_nbagg.connection_info())\n\n\n\nUAT 4 - Closing figures\nClosing a specific figure instance should turn the figure into a plain image - the UI should have been removed. In this case, scroll back to the first figure and assert this is the case.\n\nplt.close(fig1)\nplt.close('all')\n\n\n\nUAT 5 - No show without plt.show in non-interactive mode\nSimply doing a plt.plot should not show a new figure, nor indeed update an existing one (easily verified in UAT 6). The output should simply be a list of Line2D instances.\n\nplt.plot(range(10))\n\n\n\nUAT 6 - Connection information\nWe just created a new figure, but didn’t show it. Connection info should no longer have “Figure 1” (as we closed it in UAT 4) and should have figure 2 and 3, with Figure 3 without any connections. There should be 1 figure pending.\n\nprint(matplotlib.backends.backend_nbagg.connection_info())\n\n\n\nUAT 7 - Show of previously created figure\nWe should be able to show a figure we’ve previously created. The following should produce two figure windows.\n\nplt.show()\nplt.figure()\nplt.plot(range(5))\nplt.show()\n\n\n\nUAT 8 - Interactive mode\nIn interactive mode, creating a line should result in a figure being shown.\n\nplt.interactive(True)\nplt.figure()\nplt.plot([3, 2, 1])\n\nSubsequent lines should be added to the existing figure, rather than creating a new one.\n\nplt.plot(range(3))\n\nCalling connection_info in interactive mode should not show any pending figures.\n\nprint(matplotlib.backends.backend_nbagg.connection_info())\n\nDisable interactive mode again.\n\nplt.interactive(False)\n\n\n\nUAT 9 - Multiple shows\nUnlike most of the other matplotlib backends, we may want to see a figure multiple times (with or without synchronisation between the views, though the former is not yet implemented). Assert that plt.gcf().canvas.manager.reshow() results in another figure window which is synchronised upon pan & zoom.\n\nplt.gcf().canvas.manager.reshow()\n\n\n\nUAT 10 - Saving notebook\nSaving the notebook (with CTRL+S or File->Save) should result in the saved notebook having static versions of the figures embedded within. The image should be the last update from user interaction and interactive plotting. (check by converting with ipython nbconvert <notebook>)\n\n\nUAT 11 - Creation of a new figure on second show\nCreate a figure, show it, then create a new axes and show it. The result should be a new figure.\nBUG: Sometimes this doesn’t work - not sure why (@pelson).\n\nfig = plt.figure()\nplt.axes()\nplt.show()\n\nplt.plot([1, 2, 3])\nplt.show()\n\n\n\nUAT 12 - OO interface\nShould produce a new figure and plot it.\n\nfrom matplotlib.backends.backend_nbagg import new_figure_manager,show\n\nmanager = new_figure_manager(1000)\nfig = manager.canvas.figure\nax = fig.add_subplot(1,1,1)\nax.plot([1,2,3])\nfig.show()"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "href": "my_env/lib/python3.10/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "title": "QUANT dira-t-on?",
    "section": "UAT 13 - Animation",
    "text": "UAT 13 - Animation\nThe following should generate an animated line:\n\nimport matplotlib.animation as animation\nimport numpy as np\n\nfig, ax = plt.subplots()\n\nx = np.arange(0, 2*np.pi, 0.01)        # x-array\nline, = ax.plot(x, np.sin(x))\n\ndef animate(i):\n    line.set_ydata(np.sin(x+i/10.0))  # update the data\n    return line,\n\n#Init only required for blitting to give a clean slate.\ndef init():\n    line.set_ydata(np.ma.array(x, mask=True))\n    return line,\n\nani = animation.FuncAnimation(fig, animate, np.arange(1, 200), init_func=init,\n                              interval=100., blit=True)\nplt.show()\n\n\nUAT 14 - Keyboard shortcuts in IPython after close of figure\nAfter closing the previous figure (with the close button above the figure) the IPython keyboard shortcuts should still function.\n\n\nUAT 15 - Figure face colours\nThe nbagg honours all colours apart from that of the figure.patch. The two plots below should produce a figure with a red background. There should be no yellow figure.\n\nimport matplotlib\nmatplotlib.rcParams.update({'figure.facecolor': 'red',\n                            'savefig.facecolor': 'yellow'})\nplt.figure()\nplt.plot([3, 2, 1])\n\nplt.show()\n\n\n\nUAT 16 - Events\nPressing any keyboard key or mouse button (or scrolling) should cycle the line while the figure has focus. The figure should have focus by default when it is created and re-gain it by clicking on the canvas. Clicking anywhere outside of the figure should release focus, but moving the mouse out of the figure should not release focus.\n\nimport itertools\nfig, ax = plt.subplots()\nx = np.linspace(0,10,10000)\ny = np.sin(x)\nln, = ax.plot(x,y)\nevt = []\ncolors = iter(itertools.cycle(['r', 'g', 'b', 'k', 'c']))\ndef on_event(event):\n    if event.name.startswith('key'):\n        fig.suptitle('%s: %s' % (event.name, event.key))\n    elif event.name == 'scroll_event':\n        fig.suptitle('%s: %s' % (event.name, event.step))\n    else:\n        fig.suptitle('%s: %s' % (event.name, event.button))\n    evt.append(event)\n    ln.set_color(next(colors))\n    fig.canvas.draw()\n    fig.canvas.draw_idle()\n\nfig.canvas.mpl_connect('button_press_event', on_event)\nfig.canvas.mpl_connect('button_release_event', on_event)\nfig.canvas.mpl_connect('scroll_event', on_event)\nfig.canvas.mpl_connect('key_press_event', on_event)\nfig.canvas.mpl_connect('key_release_event', on_event)\n\nplt.show()\n\n\n\nUAT 17 - Timers\nSingle-shot timers follow a completely different code path in the nbagg backend than regular timers (such as those used in the animation example above.) The next set of tests ensures that both “regular” and “single-shot” timers work properly.\nThe following should show a simple clock that updates twice a second:\n\nimport time\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\n\ndef update(text):\n    text.set(text=time.ctime())\n    text.axes.figure.canvas.draw()\n    \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\ntimer.start()\nplt.show()\n\nHowever, the following should only update once and then stop:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center') \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\n\nplt.show()\n\nAnd the next two examples should never show any visible text at all:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\n\nUAT 18 - stopping figure when removed from DOM\nWhen the div that contains from the figure is removed from the DOM the figure should shut down it’s comm, and if the python-side figure has no more active comms, it should destroy the figure. Repeatedly running the cell below should always have the same figure number\n\nfig, ax = plt.subplots()\nax.plot(range(5))\nplt.show()\n\nRunning the cell below will re-show the figure. After this, re-running the cell above should result in a new figure number.\n\nfig.canvas.manager.reshow()\n\n\n\nUAT 19 - Blitting\nClicking on the figure should plot a green horizontal line moving up the axes.\n\nimport itertools\n\ncnt = itertools.count()\nbg = None\n\ndef onclick_handle(event):\n    \"\"\"Should draw elevating green line on each mouse click\"\"\"\n    global bg\n    if bg is None:\n        bg = ax.figure.canvas.copy_from_bbox(ax.bbox) \n    ax.figure.canvas.restore_region(bg)\n\n    cur_y = (next(cnt) % 10) * 0.1\n    ln.set_ydata([cur_y, cur_y])\n    ax.draw_artist(ln)\n    ax.figure.canvas.blit(ax.bbox)\n\nfig, ax = plt.subplots()\nax.plot([0, 1], [0, 1], 'r')\nln, = ax.plot([0, 1], [0, 0], 'g', animated=True)\nplt.show()\nax.figure.canvas.draw()\n\nax.figure.canvas.mpl_connect('button_press_event', onclick_handle)"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/idna-3.4.dist-info/LICENSE.html",
    "href": "my_env/lib/python3.10/site-packages/idna-3.4.dist-info/LICENSE.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "Copyright (c) 2013-2021, Kim Davies All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/QtPy-2.2.1.dist-info/AUTHORS.html",
    "href": "my_env/lib/python3.10/site-packages/QtPy-2.2.1.dist-info/AUTHORS.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "pyqode.qt: Colin Duquesnoy (@ColinDuquesnoy)\nspyderlib.qt: Pierre Raybaut (@PierreRaybaut)\nqt-helpers: Thomas Robitaille (@astrofrog)\n\n\n\n\n\nDaniel Althviz (@dalthviz)\nCarlos Cordoba (@ccordoba12)\nC.A.M. Gerlach (@CAM-Gerlach)\nSpyder Development Team (Spyder-IDE)\n\n\n\n\n\nThe QtPy Contributors"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Unicode.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Unicode.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "print('☃')\n\n☃"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/update-display-id.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/update-display-id.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "display('above')\ndisplay_with_id(1, 'here')\ndisplay('below')\n\n'above'\n\n\n8\n\n\n'below'\n\n\n\ndisplay_with_id(2, 'here')\ndisplay_with_id(3, 'there')\ndisplay_with_id(4, 'here')\n\n8\n\n\n6\n\n\n8\n\n\n\ndisplay_with_id(5, 'there')\ndisplay_with_id(6, 'there', update=True)\n\n6\n\n\n\ndisplay_with_id(7, 'here')\ndisplay_with_id(8, 'here', update=True)\ndisplay_with_id(9, 'result', execute_result=True)\n\n8\n\n\n10\n\n\n\ndisplay_with_id(10, 'result', update=True)"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/JupyterWidgets.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/JupyterWidgets.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "# it should also handle custom msg'es\nlabel.send({'msg': 'Hello'})"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Skip Exceptions.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Skip Exceptions.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "print('ok')\n\nok"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Sleep1s.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Sleep1s.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "t0 = datetime.datetime.utcnow()\ntime.sleep(1)\nt1 = datetime.datetime.utcnow()\n\n\ntime_format = '%Y-%m-%dT%H:%M:%S.%fZ'\nprint(t0.strftime(time_format), end='')\n\n\nprint(t1.strftime(time_format), end='')"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Parallel Execute B.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Parallel Execute B.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "This notebook uses a file system based “lock” to assert that two instances of the notebook kernel will run in parallel. Each instance writes to a file in a temporary directory, and then tries to read the other file from the temporary directory, so that running them in sequence will fail, but running them in parallel will succeed.\nTwo notebooks are launched, each which sets the this_notebook variable. One notebook is set to this_notebook = 'A' and the other this_notebook = 'B'.\n\nimport os\nimport os.path\nimport tempfile\nimport time\n\n\n# the variable this_notebook is injectected in a cell above by the test framework.\nthis_notebook = 'B'\nother_notebook = 'A'\ndirectory = os.environ['NBEXECUTE_TEST_PARALLEL_TMPDIR']\nwith open(os.path.join(directory, 'test_file_{}.txt'.format(this_notebook)), 'w') as f:\n    f.write('Hello from {}'.format(this_notebook))\n\n\nstart = time.time()\ntimeout = 5\nend = start + timeout\ntarget_file = os.path.join(directory, 'test_file_{}.txt'.format(other_notebook))\nwhile time.time() < end:\n    time.sleep(0.1)\n    if os.path.exists(target_file):\n        with open(target_file, 'r') as f:\n            text = f.read()\n        if text == 'Hello from {}'.format(other_notebook):\n            break\nelse:\n    assert False, \"Timed out – didn't get a message from {}\".format(other_notebook)"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/HelloWorld.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/HelloWorld.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "print(\"Hello World\")\n\nHello World"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Check History in Memory.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Check History in Memory.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "ip = get_ipython()\nassert ip.history_manager.hist_file == ':memory:'"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Other Comms.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Other Comms.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "comm = Comm('this-comm-tests-a-missing-handler', data={'id': 'foo'})\n\n\ncomm.send(data={'id': 'bar'})"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Disable Stdin.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Disable Stdin.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "try:\n    input = raw_input\nexcept:\n    pass\n\nname = input(\"name: \")"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/SVG.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/SVG.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "SVG(data='''\n<svg height=\"100\" width=\"100\">\n    <circle cx=\"50\" cy=\"50\" r=\"40\" stroke=\"black\" stroke-width=\"2\" fill=\"red\" />\n</svg>''')"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Parallel Execute A.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Parallel Execute A.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "This notebook uses a file system based “lock” to assert that two instances of the notebook kernel will run in parallel. Each instance writes to a file in a temporary directory, and then tries to read the other file from the temporary directory, so that running them in sequence will fail, but running them in parallel will succeed.\nTwo notebooks are launched, each which sets the this_notebook variable. One notebook is set to this_notebook = 'A' and the other this_notebook = 'B'.\n\nimport os\nimport os.path\nimport tempfile\nimport time\n\n\n# the variable this_notebook is injectected in a cell above by the test framework.\nthis_notebook = 'A'\nother_notebook = 'B'\ndirectory = os.environ['NBEXECUTE_TEST_PARALLEL_TMPDIR']\nwith open(os.path.join(directory, 'test_file_{}.txt'.format(this_notebook)), 'w') as f:\n    f.write('Hello from {}'.format(this_notebook))\n\n\nstart = time.time()\ntimeout = 5\nend = start + timeout\ntarget_file = os.path.join(directory, 'test_file_{}.txt'.format(other_notebook))\nwhile time.time() < end:\n    time.sleep(0.1)\n    if os.path.exists(target_file):\n        with open(target_file, 'r') as f:\n            text = f.read()\n        if text == 'Hello from {}'.format(other_notebook):\n            break\nelse:\n    assert False, \"Timed out – didn't get a message from {}\".format(other_notebook)"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/UnicodePy3.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/UnicodePy3.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "print('☃')\n\n☃"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Autokill.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Autokill.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "import os\nimport signal\npid = os.getpid()\nos.kill(pid, signal.SIGTERM)"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Output.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Output.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "print(\"hi\")\nwith output1:\n    print(\"in output\")\n\nhi\n\n\n\nwith output1:\n    raise ValueError(\"trigger msg_type=error\")\n\n\nimport ipywidgets as widgets\noutput2 = widgets.Output()\noutput2\n\n\n\n\n\nprint(\"hi2\")\nwith output2:\n    print(\"in output2\")\n    clear_output(wait=True)\n\nhi2\n\n\n\nimport ipywidgets as widgets\noutput3 = widgets.Output()\noutput3\n\n\n\n\n\nprint(\"hi3\")\nwith output3:\n    print(\"hello\")\n    clear_output(wait=True)\n    print(\"world\")\n\nhi3\n\n\n\nimport ipywidgets as widgets\noutput4 = widgets.Output()\noutput4\n\n\n\n\n\nprint(\"hi4\")\nwith output4:\n    print(\"hello world\")\n    clear_output()\n\nhi4\n\n\n\nimport ipywidgets as widgets\noutput5 = widgets.Output()\noutput5\n\n\n\n\n\nprint(\"hi5\")\nwith output5:\n    display(\"hello world\") # this is not a stream but plain text\nclear_output()\n\n\nimport ipywidgets as widgets\noutput_outer = widgets.Output()\noutput_inner = widgets.Output()\noutput_inner\n\n\n\n\n\noutput_outer\n\n\n\n\n\nwith output_inner:\n    print('in inner')\n    with output_outer:\n        print('in outer')\n    print('also in inner')"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Skip Exceptions with Cell Tags.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Skip Exceptions with Cell Tags.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "print('ok')\n\nok"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Interrupt.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Interrupt.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "print(\"done\")\n\ndone"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Factorials.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Factorials.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "for m in range(10):\n    i, j = j, i + j\n    print(j)\n\n2\n3\n5\n8\n13\n21\n34\n55\n89\n144"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Error.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Error.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "0/0\n\nZeroDivisionError: division by zero"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Empty Cell.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Empty Cell.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "\"Code 1\"\n\n'Code 1'\n\n\n\n\"Code 2\"\n\n'Code 2'"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Inline Image.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Inline Image.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "Image('python.png')"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Skip Execution with Cell Tag.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Skip Execution with Cell Tag.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "print('ok')\n\nok"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Clear Output.html",
    "href": "my_env/lib/python3.10/site-packages/nbclient/tests/files/Clear Output.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "for i in range(10):\n    clear_output()\n    print(i)\n\n9\n\n\n\nprint(\"Hello world\")\nclear_output()\n\n\nprint(\"Hello world\", end='')\nclear_output(wait=True)  # no output after this\n\nHello world\n\n\n\nprint(\"Hello\", end='')\nclear_output(wait=True)  # here we have new output after wait=True\nprint(\"world\", end='')\n\nworld\n\n\n\nhandle0 = display(\"Hello world\", display_id=\"id0\")\n\n'Hello world'\n\n\n\nhandle1 = display(\"Hello\", display_id=\"id1\")\n\n'world'\n\n\n\nhandle1.update('world')\n\n\nhandle2 = display(\"Hello world\", display_id=\"id2\")\nclear_output()  # clears all output, also with display_ids\n\n\nhandle3 = display(\"Hello world\", display_id=\"id3\")\nclear_output(wait=True)\n\n'Hello world'\n\n\n\nhandle4 = display(\"Hello\", display_id=\"id4\")\nclear_output(wait=True)\nprint('world', end='')\n\nworld\n\n\n\nhandle4.update('Hello world')  # it is cleared, so it should not show up in the above cell"
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/soupsieve-2.3.2.post1.dist-info/license_files/LICENSE.html",
    "href": "my_env/lib/python3.10/site-packages/soupsieve-2.3.2.post1.dist-info/license_files/LICENSE.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "Copyright (c) 2018 - 2022 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/ipykernel-6.16.2.dist-info/licenses/COPYING.html",
    "href": "my_env/lib/python3.10/site-packages/ipykernel-6.16.2.dist-info/licenses/COPYING.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "This project is licensed under the terms of the Modified BSD License (also known as New or Revised or 3-Clause BSD), as follows:\n\nCopyright (c) 2015, IPython Development Team\n\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the IPython Development Team nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nThe IPython Development Team is the set of all contributors to the IPython project. This includes all of the IPython subprojects.\nThe core team that coordinates development on GitHub can be found here: https://github.com/ipython/.\n\n\n\nIPython uses a shared copyright model. Each contributor maintains copyright over their contributions to IPython. But, it is important to note that these contributions are typically only changes to the repositories. Thus, the IPython source code, in its entirety is not the copyright of any single person or institution. Instead, it is the collective copyright of the entire IPython Development Team. If individual contributors want to maintain a record of what changes/contributions they have specific copyright on, they should indicate their copyright in the commit message of the change, when they commit the change to one of the IPython repositories.\nWith this in mind, the following banner should be used in any source code file to indicate the copyright and license terms:\n# Copyright (c) IPython Development Team.\n# Distributed under the terms of the Modified BSD License."
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/traitlets-5.5.0.dist-info/licenses/COPYING.html",
    "href": "my_env/lib/python3.10/site-packages/traitlets-5.5.0.dist-info/licenses/COPYING.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "Traitlets is adapted from enthought.traits, Copyright (c) Enthought, Inc., under the terms of the Modified BSD License.\nThis project is licensed under the terms of the Modified BSD License (also known as New or Revised or 3-Clause BSD), as follows:\n\nCopyright (c) 2001-, IPython Development Team\n\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the IPython Development Team nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nThe IPython Development Team is the set of all contributors to the IPython project. This includes all of the IPython subprojects.\nThe core team that coordinates development on GitHub can be found here: https://github.com/jupyter/.\n\n\n\nIPython uses a shared copyright model. Each contributor maintains copyright over their contributions to IPython. But, it is important to note that these contributions are typically only changes to the repositories. Thus, the IPython source code, in its entirety is not the copyright of any single person or institution. Instead, it is the collective copyright of the entire IPython Development Team. If individual contributors want to maintain a record of what changes/contributions they have specific copyright on, they should indicate their copyright in the commit message of the change, when they commit the change to one of the IPython repositories.\nWith this in mind, the following banner should be used in any source code file to indicate the copyright and license terms:\n# Copyright (c) IPython Development Team.\n# Distributed under the terms of the Modified BSD License."
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/jupyter_client-7.4.4.dist-info/licenses/COPYING.html",
    "href": "my_env/lib/python3.10/site-packages/jupyter_client-7.4.4.dist-info/licenses/COPYING.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "This project is licensed under the terms of the Modified BSD License (also known as New or Revised or 3-Clause BSD), as follows:\n\nCopyright (c) 2001-2015, IPython Development Team\nCopyright (c) 2015-, Jupyter Development Team\n\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the Jupyter Development Team nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nThe Jupyter Development Team is the set of all contributors to the Jupyter project. This includes all of the Jupyter subprojects.\nThe core team that coordinates development on GitHub can be found here: https://github.com/jupyter/.\n\n\n\nJupyter uses a shared copyright model. Each contributor maintains copyright over their contributions to Jupyter. But, it is important to note that these contributions are typically only changes to the repositories. Thus, the Jupyter source code, in its entirety is not the copyright of any single person or institution. Instead, it is the collective copyright of the entire Jupyter Development Team. If individual contributors want to maintain a record of what changes/contributions they have specific copyright on, they should indicate their copyright in the commit message of the change, when they commit the change to one of the Jupyter repositories.\nWith this in mind, the following banner should be used in any source code file to indicate the copyright and license terms:\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License."
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/jupyter_server-1.21.0.dist-info/COPYING.html",
    "href": "my_env/lib/python3.10/site-packages/jupyter_server-1.21.0.dist-info/COPYING.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "This project is licensed under the terms of the Modified BSD License (also known as New or Revised or 3-Clause BSD), as follows:\n\nCopyright (c) 2001-2015, IPython Development Team\nCopyright (c) 2015-, Jupyter Development Team\n\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the Jupyter Development Team nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nThe Jupyter Development Team is the set of all contributors to the Jupyter project. This includes all of the Jupyter subprojects.\nThe core team that coordinates development on GitHub can be found here: https://github.com/jupyter/.\n\n\n\nJupyter uses a shared copyright model. Each contributor maintains copyright over their contributions to Jupyter. But, it is important to note that these contributions are typically only changes to the repositories. Thus, the Jupyter source code, in its entirety is not the copyright of any single person or institution. Instead, it is the collective copyright of the entire Jupyter Development Team. If individual contributors want to maintain a record of what changes/contributions they have specific copyright on, they should indicate their copyright in the commit message of the change, when they commit the change to one of the Jupyter repositories.\nWith this in mind, the following banner should be used in any source code file to indicate the copyright and license terms:\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License."
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/nbformat-5.7.0.dist-info/licenses/COPYING.html",
    "href": "my_env/lib/python3.10/site-packages/nbformat-5.7.0.dist-info/licenses/COPYING.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "This project is licensed under the terms of the Modified BSD License (also known as New or Revised or 3-Clause BSD), as follows:\n\nCopyright (c) 2001-2015, IPython Development Team\nCopyright (c) 2015-, Jupyter Development Team\n\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the Jupyter Development Team nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nThe Jupyter Development Team is the set of all contributors to the Jupyter project. This includes all of the Jupyter subprojects.\nThe core team that coordinates development on GitHub can be found here: https://github.com/jupyter/.\n\n\n\nJupyter uses a shared copyright model. Each contributor maintains copyright over their contributions to Jupyter. But, it is important to note that these contributions are typically only changes to the repositories. Thus, the Jupyter source code, in its entirety is not the copyright of any single person or institution. Instead, it is the collective copyright of the entire Jupyter Development Team. If individual contributors want to maintain a record of what changes/contributions they have specific copyright on, they should indicate their copyright in the commit message of the change, when they commit the change to one of the Jupyter repositories.\nWith this in mind, the following banner should be used in any source code file to indicate the copyright and license terms:\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License."
  },
  {
    "objectID": "my_env/lib/python3.10/site-packages/jupyter_core-4.11.2.dist-info/licenses/COPYING.html",
    "href": "my_env/lib/python3.10/site-packages/jupyter_core-4.11.2.dist-info/licenses/COPYING.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "Jupyter is licensed under the terms of the Modified BSD License (also known as New or Revised or 3-Clause BSD), as follows:\n\nCopyright (c) 2015-, Jupyter Development Team\n\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the Jupyter Development Team nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nThe Jupyter Development Team is the set of all contributors to the Jupyter project. This includes all of the Jupyter subprojects. A full list with details is kept in the documentation directory, in the file about/credits.txt.\nThe core team that coordinates development on GitHub can be found here: https://github.com/ipython/.\n\n\n\nJupyter uses a shared copyright model. Each contributor maintains copyright over their contributions to Jupyter. It is important to note that these contributions are typically only changes to the repositories. Thus, the Jupyter source code in its entirety is not the copyright of any single person or institution. Instead, it is the collective copyright of the entire Jupyter Development Team. If individual contributors want to maintain a record of what changes/contributions they have specific copyright on, they should indicate their copyright in the commit message of the change, when they commit the change to one of the Jupyter repositories.\nWith this in mind, the following banner should be used in any source code file to indicate the copyright and license terms:\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License."
  },
  {
    "objectID": "posts/statistical-moments/index.html",
    "href": "posts/statistical-moments/index.html",
    "title": "Statistical Moments",
    "section": "",
    "text": "This post is about summarizing the various statistical moments when doing quantitative finance. The focus is on the asset returns. From a previous post, we already know that financial asset returns do not follow a normal distribution (too peaked at the mean and fat tails).\nWe’ll show these parameters using both R and python.\nWe’ll use the SPY as a low-ish vol asset and AMD as an equity with higher vol. We only use the last 5 years of data (from 2018 and beyond)\nLet’s first load our libraries and the 2 data frame worth of prices.\n\nlibrary(readr)    # read_csv()\nlibrary(dplyr)    # mutate(), filter()\nlibrary(lubridate)\nlibrary(ggplot2)\n\ndf_spy <- read_csv('../../raw_data/SPY.csv') |> \n  select(date, adjClose) |> \n  arrange(date) |> \n  mutate(return = log(adjClose / lag(adjClose))) |> \n  filter(date > '2018-01-01')\n\ndf_amd <- read_csv('../../raw_data/AMD.csv') |> \n  select(date, adjClose) |> \n  arrange(date) |> \n  mutate(return = log(adjClose / lag(adjClose))) |> \n  filter(date > '2018-01-01')\n\n\nMean\nThe mean is our first moment. We’ll cons\nMathematically speaking, we define the mean as \\[\\sum^{n}_{i=1} \\frac{r_i}{n}\\]\n\n\\(r_i\\) is the return of the i-th observation\n\nusing log return\n\n\\(n\\) is the number of observation\n\n\n\nStandard Deviation\nStandard deviation is the second moment.\n\\[\\sigma = \\sqrt{\\frac{\\sum(x_i - \\mu)^2}{n}}\\] In the case of a sample: \\(s = \\sqrt{\\frac{(x_i - \\bar{x})^2}{n-1}}\\)\nThe variance is the square of the standard deviation. There is this neat little alternative expression of variance (I like the way it sounds).\n\nVariance is the difference between the mean of the square and the square of the mean.\n\nHere is how it goes:\n\\[\\sigma^2 = \\frac{\\sum(x_i-\\mu)^2}{n} = \\frac{\\sum(x_i)^2}{n} - 2 \\frac{\\sum(x_i \\mu)}{n} + \\frac{\\sum(\\mu)^2}{n}\\]\nConsidering\n\n\\(\\frac{\\sum(x_i \\mu)}{n} = \\mu \\frac{\\sum(x_i)}{n} = \\mu \\mu=\\mu^2\\)\nand considering \\(\\frac{\\sum(\\mu)^2}{n} = n \\frac{\\mu^2}{n} = \\mu^2\\), we can re-write our variance like this\n\n\\[\\sigma^2 = \\frac{\\sum(x_i)^2}{n} - 2 \\mu^2 + \\mu^2 = \\frac{\\sum(x_i)^2}{n} - \\mu^2\\]\n\n\nCoefficient of variation\nThe coefficient of variation is not a statistical moment, but considering it is the ratio of the first 2 moments (ratio of sd to the mean), we include it here as well. It allows to compare together 2 different distributions (that have different mean and sd).\n\\[CV = \\frac{\\sigma}{\\mu}\\]\nOr in the case of a sample \\(CV = \\frac{s}{\\bar{x}}\\)\n\n\nSkewness\n\n\n\n\n\n\nSkewness\n\n\n\nSkewness is measure of asymmetry of a distribution (or actually lack of). How symmetric around the mean is the distribution? A standard normal distribution is perfectly symmetrical and has zero skew. Other examples of zero-skewed distributions are the T Distribution, the uniform distribution and the Laplace distribution. However, other distributions don’t have zero skew.\nIn a zero skew distribution, the mean = the median and the distribution is symmetric.\n\n\nIn a sense, skewness is quantifying for us how far is the median from the mean.\nMathematically, we define skewness as\n\\[\\frac{\\frac{1}{n} \\sum(x_i - \\bar{x})^3}{\\sqrt{ \\left( \\frac{1}{n} \\sum(x_i - \\bar{x})^2 \\right)^3}} \\tag{1}\\]\nIn the case of a sample, we’ll multiply Equation 2 by a factor of \\(\\frac{\\sqrt{n(n-1)}}{n-2}\\)\n\nskenwess = 0 ==> normallly distributed\n$ -0.5 skewness $ ==> moderately skew\n$$\n\n\n\nKurtosis\n\n\n\n\n\n\nKurtosis\n\n\n\nKurtosis is a measure that describes the shape of a distribution’s tails in relation to its overall shape. A distribution can be infinitely peaked with low kurtosis, and a distribution can be perfectly flat-topped with infinite kurtosis. Thus, kurtosis measures “tailedness,” not “peakedness.”\n\n\nBecause, we raised the difference of a data point to its mean to the 4th power, it is really the data points far away from the mean that do participate to the kurtosis.\n\n“increasing kurtosis is associated with the “movement of probability mass from the shoulders of a distribution into its center and tails.” (Someone important who got published and knew his stuff!)\n\nMathematically, we define kurtosis as\n\\[\\frac{\\frac{1}{n} \\sum(x_i - \\bar{x})^4}{ \\left( \\frac{1}{n} \\sum(x_i - \\bar{x})^2 \\right)^2} \\tag{2}\\]\nSome statistical packages are providing excess kurtosis by subtracting 3 to the kurtosis value. So for a data set that is perfectly normally distributed, we expect the excess kurtosis to be 0.\nThere are 3 categories of kurtosis: leptokurtic (positive excess kurtosis), mesokurtic (aka normal distribution), platykurtic. \n\nA kurtosis greater than 3 => leptokurtic\nA kurtosis around 3 => mesokurtic\nA kurtosis less than 3 => platykurtic\n\nA Student’s T distribution with degree of freedom 4 has infinite kurtosis (huge peak and tails + narrow shoulders)\nMost equities display a leptokurtic behavior (skinny at the mean - most returns are clustered around the mean) and narrow shoulders and fat tails.\n\n\nUsing Python\n\nimport numpy as np\nimport pandas as pd\n\namd = pd.read_csv('../../raw_data/AMD.csv')\n\nx = amd['adjClose']\n\nreturns_21d = np.log(x / x.shift(21)).dropna()\n\nmean_21dret = np.mean(returns_21d)\nstd_21dret = np.std(returns_21d)\n\nprint(\"The mean rolling 21 days return is: %s\" % round(mean_21dret, 5))\n\nThe mean rolling 21 days return is: -0.00594\n\nprint(\"The standard deviation of the rolling 21 days return is: %s\" %round(std_21dret, 5))\n\nThe standard deviation of the rolling 21 days return is: 0.1832\n\n\nThe standard deviation is quite bigger than the mean. An histogram of the returns will confirm that.\n\nimport matplotlib.pyplot as plt\n\nplt.hist(returns_21d, bins = 'rice', label = 'Rolling 21-days return')\n\n(array([  2.,   0.,   3.,   4.,   5.,  26.,  31.,  56.,  66., 117., 192.,\n       269., 357., 420., 607., 637., 644., 567., 464., 380., 206., 157.,\n       109.,  98.,  73.,  31.,  28.,  14.,  10.,   8.,   2.,   1.,   0.,\n         3.,   2.,   1.]), array([-0.7537718 , -0.70724263, -0.66071346, -0.6141843 , -0.56765513,\n       -0.52112596, -0.47459679, -0.42806762, -0.38153845, -0.33500928,\n       -0.28848011, -0.24195095, -0.19542178, -0.14889261, -0.10236344,\n       -0.05583427, -0.0093051 ,  0.03722407,  0.08375324,  0.1302824 ,\n        0.17681157,  0.22334074,  0.26986991,  0.31639908,  0.36292825,\n        0.40945742,  0.45598659,  0.50251575,  0.54904492,  0.59557409,\n        0.64210326,  0.68863243,  0.7351616 ,  0.78169077,  0.82821994,\n        0.8747491 ,  0.92127827]), <BarContainer object of 36 artists>)\n\nplt.show()\n\n\n\n\nWe are seeing a larger left tail with indeed the mean looking around 0. An other to visualize this and putting emphasis on the outliers would be to plot the returns on a box-and-whiskers plot.\n\nplt.boxplot(returns_21d, labels = ['Rolling 21-days return'])\n\n{'whiskers': [<matplotlib.lines.Line2D object at 0x173da8040>, <matplotlib.lines.Line2D object at 0x173da82e0>], 'caps': [<matplotlib.lines.Line2D object at 0x173da8580>, <matplotlib.lines.Line2D object at 0x173da8820>], 'boxes': [<matplotlib.lines.Line2D object at 0x173d88e80>], 'medians': [<matplotlib.lines.Line2D object at 0x173da8ac0>], 'fliers': [<matplotlib.lines.Line2D object at 0x173da8d60>], 'means': []}\n\nplt.show()\n\n\n\n\n\nfrom scipy.stats import skew, skewtest\n\nskew(returns_21d)\n\n0.21432056577181507\n\n\nThe skew value is quite large and negative which confirms the fat left tail we saw on the histogram\n\nskewtest(returns_21d)\n\nSkewtestResult(statistic=6.477804949429641, pvalue=9.306654969735504e-11)\n\n\nVery small p-value. We reject the null-hypothesis. The distribution is not symetrical."
  },
  {
    "objectID": "posts/markov-chains/index.html",
    "href": "posts/markov-chains/index.html",
    "title": "Markov Chains",
    "section": "",
    "text": "Introduction to Markov Chains\nA Markov chain is a random process with the Markov property. A random process or often called stochastic property is a mathematical object defined as a collection of random variables. A Markov chain has either discrete state space (set of possible values of the random variables) or discrete index set (often representing time) - given the fact, many variations for a Markov chain exists. Usually the term “Markov chain” is reserved for a process with a discrete set of times, that is a Discrete Time Markov chain (DTMC).\nTo develop better intuition about Markov chain, the simpler version of it is to model a basic random walk."
  },
  {
    "objectID": "posts/markov-chains/index.html#from-scratch",
    "href": "posts/markov-chains/index.html#from-scratch",
    "title": "Markov Chains",
    "section": "From scratch",
    "text": "From scratch\n\nimport numpy as np\n\nstart = 0\ny = []\nn = 1000\n\nfor i in range(n): \n  step = np.random.choice([-1, 1], p = [0.5, 0.5])\n  start += step\n  y.append(start)\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(y)\n#plt.xlabel('Number of steps')\n#plt.ylabel(r'$S_{n}$')\n\n\n\n\nA random walk with a 1000 steps and equal probabilty to go left or right"
  },
  {
    "objectID": "posts/markov-chains/index.html#using-financial-data",
    "href": "posts/markov-chains/index.html#using-financial-data",
    "title": "Markov Chains",
    "section": "Using financial data",
    "text": "Using financial data\nPython code coming from this post\nA Monte-Carlo simulation of a random-walk of a stock price does assume that the returns follow a normal distribution. A second assumption is that the past volatility of returns will continue (or be very similar) in the future. This is of course not totally the case.\n\nGetting data and feature engineeriing\nGetting data using the yfinance package.\n\nimport yfinance as yf\nimport pandas as pd\n\nyo = yf.download(\"SBUX\", start = \"2005-01-01\")\n\nyo.to_csv(\"../../raw_data/sbux.csv\")\n\nyo['Adj Close'][-1]\n\nyo.info()\n\nWe have imported SBUX stock price data and stored them on disk. We’ll retrieve it using pandas and construct our returns and volatility variables.\n\nimport pandas as pd\n\nsbux = pd.read_csv(\"../../raw_data/sbux.csv\")\n\nsbux.tail() \n\n# get the daily returns and then filter on the last 2 years of trading. \n# calculate volatiliy on these last years (not realistic of course)\ndaily_returns = sbux['Adj Close'].pct_change()\n#sbux = sbux.tail(505)\ndaily_volat = daily_returns.std()\n\nprint(daily_volat)\n\nsbux.info()\n\n0.01948403159171024\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4483 entries, 0 to 4482\nData columns (total 7 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   Date       4483 non-null   object \n 1   Open       4483 non-null   float64\n 2   High       4483 non-null   float64\n 3   Low        4483 non-null   float64\n 4   Close      4483 non-null   float64\n 5   Adj Close  4483 non-null   float64\n 6   Volume     4483 non-null   int64  \ndtypes: float64(5), int64(1), object(1)\nmemory usage: 245.3+ KB\n\n\n\n\nSingle simulation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlook_back = 252 \ncount = 0\nprice_list = []\nlast_price = sbux['Adj Close'].iloc[-1]\n\nprice = last_price * (1 + np.random.normal(0, daily_volat)) \nprice_list.append(price)\n\nfor y in range(look_back): \n  if count == 251: \n    break\n  price = price_list[count] * (1 + np.random.normal(0, daily_volat))\n  price_list.append(price) \n  count += 1\n  \nplt.plot(price_list)\nplt.show()\n\n#price_list\n\n\n\n\nAn here would be another single simulation. It will of course look vastly different although it is build from the same normal distribution with same mean \\(\\mu = 0\\) and sd \\(\\sigma = 0\\).\n\n\n\n\n\nNow we can re-use that code if we want to create 100’s of these simulations.\n\nnum_of_simulations = 100 \nmodel_ahead = 252 \n\ndf = pd.DataFrame()\nlast_price_list = []\n\nfor x in range(num_of_simulations): \n  count = 0\n  price_list = []\n  last_price = sbux.iloc[-1]['Adj Close'] \n  price = last_price * (1 + np.random.normal(0, daily_volat)) \n  price_list.append(price) \n  \n  for y in range(model_ahead): \n    if count == 251: \n      break\n    price = price_list[count] * (1 + np.random.normal(0, daily_volat)) \n    price_list.append(price) \n    count += 1\n  \n  df[x] = price_list\n  last_price_list.append(price_list[-1])\n    \n  \nfig = plt.figure()\nfig.suptitle(\"Monte Carlo Simulation for SBUX\") \nplt.plot(df)\nplt.xlabel(\"Day\")\nplt.ylabel(\"Price\")\nplt.show()\n\n\n\n\nWith just 10 simulated random-walks on SBUX given the last 17 years of volatility, we can see that price could range between $40 to around $140 dollars over the next 252 trading days (one year).\n\n\nAnalysis of our MC simulation\n\nprint(\"Expected price: \", round(np.mean(last_price_list), 2))\nprint(\"Quantile (5%): \", np.percentile(last_price_list, 5))\nprint(\"Quantile (95%): \", np.percentile(last_price_list, 95))\n\nExpected price:  85.6\nQuantile (5%):  51.877349251768564\nQuantile (95%):  128.62538135167497\n\n\n\nplt.hist(last_price_list, bins=10) \nplt.show()"
  },
  {
    "objectID": "posts/markov-chains/index.html#transition-matrix",
    "href": "posts/markov-chains/index.html#transition-matrix",
    "title": "Markov Chains",
    "section": "Transition Matrix",
    "text": "Transition Matrix\nIn a transition matrix, the rows are you starting state and columns are your end of state.\nSo with the below matrix, the probability to go from state A to state A is 0.8 and probability to go from state A to state D is 0.2. In this sense, all the rows of a transition matrix should always add up to 1.\n\nstate_A = [0.1, 0.4, 0.3, 0.2, 0]\nstate_B = [0.0, 0.5, 0.5, 0.0, 0]\nstate_C = [0.0, 0.0, 1.0, 0.0, 0]\nstate_D = [0.0, 0.0, 0.0, 0.0, 1.0]\nstate_E = [0.0, 0.0, 0.0, 0.5, 0.5]\n\ntransition_matrix = [state_A, state_B, state_C, state_D, state_E]\n\nWe could also create a function to check if a transition matrix is indeed a properly formatted transition matrix to model a markov chain.\n\ndef check_markov_chain(m): \n  for i in range(0, len(m)): \n    if sum(m[i]) != 1: \n      return False\n  print(\"This is a correctly formatted transition matrix\") \n  \ncheck_markov_chain(transition_matrix)\n\nThis is a correctly formatted transition matrix"
  },
  {
    "objectID": "posts/starting-quarto/index.html",
    "href": "posts/starting-quarto/index.html",
    "title": "Starting a quarto blog",
    "section": "",
    "text": "This is a post on trying to make a quarto blog (this blog)\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/kmeans-intro/index.html",
    "href": "posts/kmeans-intro/index.html",
    "title": "Intro to Kmeans",
    "section": "",
    "text": "The purpose of this article is to get a deeper dive into Kmeans as an unsupervised machine learning algorithms. To see the algorithm at work on some financial data, you can use the post on kmeans and regime change.\nAs mentioned, Kmeans is an unsupervised learning algorithm, aiming to group “observations” based on their distances to a fixed number of “centroids” (aka: the fixed number is K). Each centroids is defined as the mean (in each dimension) of all the points belonging to that centroid. All the points belonging to a centroid makes a cluster: all the observations belonging to the k-centroid makes the k-cluster.\nThe objective is, for a given number of centroids (i.e. k), to minimize the total within-cluster variation or intra cluster variation (distance from the observation to the centroid).\nThe standard Kmeans algorithm aims to minimize the total sum of the square distances (Euclidean distance) between observations and centroids.\nFirst, we calculate the within-centroid sum of square: \\[W(C_k) = \\sum_{x_i \\in C_k} (x_i - \\mu_k)^2 \\tag{1}\\]\nThe objective is to minimize the total within cluster variation, that is the total sum of the distance of each observations to its centroid and that for each of the k-centroids: \\[tot.withinss = \\sum_{k=1}^k W(C_k) = \\sum_{k=1}^k \\sum_{x_i \\in C_k} (x_i - \\mu_k)^2   \\tag{2}\\]"
  },
  {
    "objectID": "posts/kmeans-intro/index.html#number-of-clusters",
    "href": "posts/kmeans-intro/index.html#number-of-clusters",
    "title": "Intro to Kmeans",
    "section": "Number of clusters",
    "text": "Number of clusters\n\nThe elbow method\nThe idea is to identify where does drop in the total within-cluster sum of square start to slowdown. Of course the total within-clusters sum of square decrease as the number of centroids increase. If we have n centroids (that is \\(n = k\\) - as many centroids as observations), the total within-cluster sum of square will be 0. And if we have only one centroid, the total within-one-cluster sum of square will be the sum of square of the mean of each of the variables.\nSo when does adding a centroid does not significantly reduce the total within-cluster sum of square.\n\n\nThe silhouette method"
  },
  {
    "objectID": "posts/kmeans-intro/index.html#number-of-iterations",
    "href": "posts/kmeans-intro/index.html#number-of-iterations",
    "title": "Intro to Kmeans",
    "section": "Number of iterations",
    "text": "Number of iterations"
  },
  {
    "objectID": "posts/kmeans-intro/index.html#number-of-start",
    "href": "posts/kmeans-intro/index.html#number-of-start",
    "title": "Intro to Kmeans",
    "section": "Number of start",
    "text": "Number of start"
  },
  {
    "objectID": "posts/kmeans-intro/index.html#con",
    "href": "posts/kmeans-intro/index.html#con",
    "title": "Intro to Kmeans",
    "section": "con",
    "text": "con\n\nYou don’t always know in advance thee number of centroids. You can use the elbow method or the silhouette method to determine the numbers of centroids you want.\n\nbecause of the random initialization stage, results might not necessarily be reproducible. If results have to be reproduced, then you need to set a seed."
  },
  {
    "objectID": "posts/xgboost-time-series/index.html",
    "href": "posts/xgboost-time-series/index.html",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "",
    "text": "This post is about using xgboost on a time-series using both R with the tidymodel framework and python. It is part of a series of articles aiming at translating python timeseries blog articles into their tidymodels equivalent.\nThe raw data is quite simple as it is energy consumption based on an hourly consumption. Original article can be found here. Minimal changes were made to better fit current python practices.\nXgboost is part of the ensemble machine learning algorithms. It can be used for both regression and classification. There are few issues in using Xgboost with time-series. This article is taking a Xgboost post in python and also translating with the new R tidymodel framework."
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-r",
    "href": "posts/xgboost-time-series/index.html#using-r",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using R",
    "text": "Using R\n\n# setting up main R libraries to start \nthe_path <- here::here()\nlibrary(glue)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\ndf0 <- read_csv(glue(the_path, \"/raw_data/AEP_hourly.csv\"))\n# let's have a quick look at what we are dealing with\nglimpse(df0)\n\nRows: 121,273\nColumns: 2\n$ Datetime <dttm> 2004-12-31 01:00:00, 2004-12-31 02:00:00, 2004-12-31 03:00:0…\n$ AEP_MW   <dbl> 13478, 12865, 12577, 12517, 12670, 13038, 13692, 14297, 14719…\n\n\nThere are only 2 variables. The Datetime being the only independ variable. And the energy consumption labelled as AEP_MW being our variable to predict.\n\n# and graphically - \n# just using a couple of years to get an idea \nggplot(df0 |> filter(Datetime > \"2014-01-01\" & Datetime < \"2016-01-01\"), aes(x =Datetime, y=AEP_MW )) + geom_line(color = \"light blue\")\n\n\n\n\nFigure 1: Graphical glimpse of our raw data\n\n\n\n\nAs Datetime is our only input variable, we’ll use the usual tricks of breaking it down into week number, months, etc. I am doing it slightly differently than in the python version here as I will first create the new time related variables then I will split it into training and testing.\n\nlibrary(lubridate)\ndf <- df0 |> \n  mutate(hour = hour(Datetime), \n         day_of_week = wday(Datetime), \n         day_of_year = yday(Datetime), \n         day_of_month = mday(Datetime), \n         week_of_year = isoweek(Datetime), \n         month = month(Datetime), \n         quarter = quarter(Datetime), \n         year = isoyear(Datetime)\n         ) \n# another glimpse now. \nglimpse(df)\n\nRows: 121,273\nColumns: 10\n$ Datetime     <dttm> 2004-12-31 01:00:00, 2004-12-31 02:00:00, 2004-12-31 03:…\n$ AEP_MW       <dbl> 13478, 12865, 12577, 12517, 12670, 13038, 13692, 14297, 1…\n$ hour         <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ day_of_week  <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, …\n$ day_of_year  <dbl> 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 36…\n$ day_of_month <int> 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 3…\n$ week_of_year <dbl> 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 5…\n$ month        <dbl> 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1…\n$ quarter      <int> 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ year         <dbl> 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 200…\n\n\nAlthough, there are only 2 variables, there are over 120,000 rows of data. That’s non-negligible."
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-python",
    "href": "posts/xgboost-time-series/index.html#using-python",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using python",
    "text": "Using python\nThis is the code from the original post.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npy_df = pd.read_csv(\"../../raw_data/AEP_hourly.csv\", index_col = [0], parse_dates = [0])\npy_df.tail()\n#plt.plot(df0)\n\n                      AEP_MW\nDatetime                    \n2018-01-01 20:00:00  21089.0\n2018-01-01 21:00:00  20999.0\n2018-01-01 22:00:00  20820.0\n2018-01-01 23:00:00  20415.0\n2018-01-02 00:00:00  19993.0\n\nsplit_date = '01-jan-2016'\npy_df_train = py_df.loc[py_df.index <= split_date].copy()\npy_df_test = py_df.loc[py_df.index > split_date].copy()\n\nThe author of the python blog first created a train / test set then created a function to add the variables then applied that function to both sets. This is a very valid way of doing things when steps include normalizing and/or scaling data before applying our ML algorithms as we don’t want any leakage from our training set into our testing set.\n\n# Create features of df\ndef create_features(df, label = None): \n  df['date'] = df.index \n  df['hour'] = df['date'].dt.hour\n  df['day_of_week'] = df['date'].dt.dayofweek\n  df['day_of_year'] = df['date'].dt.dayofyear \n  df['day_of_month'] = df['date'].dt.day \n  df['week_of_year'] = df['date'].dt.isocalendar().week \n  df['month'] = df['date'].dt.month \n  df['quarter'] = df['date'].dt.quarter \n  df['year'] = df['date'].dt.year\n  \n  X = df[['hour', 'day_of_week', 'day_of_year', 'day_of_month', 'week_of_year', 'month', 'quarter', 'year']]\n  \n  if label: \n    y = df[label]\n    return X, y\n  \n  return X\n\nCompare this way of constructing variables to the much easier and more elegant tidyverse’s way of cleaning and creating variables. The dplyr package really makes it painless to wrangle data."
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-r-1",
    "href": "posts/xgboost-time-series/index.html#using-r-1",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using R",
    "text": "Using R\nRsample is the tidymodel package that deals with creating training and testing sets. There are really many methods available to do this, but we stick to the same methods provided in the original blog post. There are out-of-the-box methods to deal with timeseries like in this case.\n\nlibrary(rsample)\nprop_split = 1 - (nrow(df |> filter(Datetime > \"2016-01-01\")) / nrow(df))\ndf_split <- initial_time_split(df |> arrange(Datetime), prop = prop_split)\ndf_train <- training(df_split)\ndf_test <- testing(df_split)"
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-python-1",
    "href": "posts/xgboost-time-series/index.html#using-python-1",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using Python",
    "text": "Using Python\n\npy_x_train, py_y_train = create_features(py_df_train, label = \"AEP_MW\")\npy_x_test, py_y_test =   create_features(py_df_test, label = \"AEP_MW\")\n#When running xgboost, I got an issue with one of the type of the variable.  \n# Let's fix this. \npy_x_train.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 98594 entries, 2004-12-31 01:00:00 to 2015-01-02 00:00:00\nData columns (total 8 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   hour          98594 non-null  int64 \n 1   day_of_week   98594 non-null  int64 \n 2   day_of_year   98594 non-null  int64 \n 3   day_of_month  98594 non-null  int64 \n 4   week_of_year  98594 non-null  UInt32\n 5   month         98594 non-null  int64 \n 6   quarter       98594 non-null  int64 \n 7   year          98594 non-null  int64 \ndtypes: UInt32(1), int64(7)\nmemory usage: 6.5 MB\n\npy_x_train = py_x_train.astype(np.int64)\npy_x_test = py_x_test.astype(np.int64)\npy_x_train.info()\n\n<class 'pandas.core.frame.DataFrame'>\nDatetimeIndex: 98594 entries, 2004-12-31 01:00:00 to 2015-01-02 00:00:00\nData columns (total 8 columns):\n #   Column        Non-Null Count  Dtype\n---  ------        --------------  -----\n 0   hour          98594 non-null  int64\n 1   day_of_week   98594 non-null  int64\n 2   day_of_year   98594 non-null  int64\n 3   day_of_month  98594 non-null  int64\n 4   week_of_year  98594 non-null  int64\n 5   month         98594 non-null  int64\n 6   quarter       98594 non-null  int64\n 7   year          98594 non-null  int64\ndtypes: int64(8)\nmemory usage: 6.8 MB"
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-r-2",
    "href": "posts/xgboost-time-series/index.html#using-r-2",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using R",
    "text": "Using R\nAgain this is a very straightforward xgboost application to a dataset. No fine tuning of models, recipe, etc.\n\nlibrary(parsnip)\nmodel_xgboost <- boost_tree(stop_iter = 50L, trees=1000L) |> \n  set_engine(\"xgboost\") |>\n  set_mode(\"regression\")\n  \nfit_xgboost <- model_xgboost |> \n  fit(AEP_MW ~., data = df_train %>% select(-Datetime))\nfit_xgboost\n\nparsnip model object\n\n##### xgb.Booster\nraw: 4.7 Mb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1, \n    subsample = 1), data = x$data, nrounds = 1000L, watchlist = x$watchlist, \n    verbose = 0, early_stopping_rounds = 50L, nthread = 1, objective = \"reg:squarederror\")\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", nthread = \"1\", objective = \"reg:squarederror\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  best_iteration, best_msg, best_ntreelimit, best_score, niter\ncallbacks:\n  cb.evaluation.log()\n  cb.early.stop(stopping_rounds = early_stopping_rounds, maximize = maximize, \n    verbose = verbose)\n# of features: 8 \nniter: 1000\nbest_iteration : 1000 \nbest_ntreelimit : 1000 \nbest_score : 242.3155 \nbest_msg : [1000]   training-rmse:242.315489 \nnfeatures : 8 \nevaluation_log:\n    iter training_rmse\n       1    11175.8839\n       2     7906.5875\n---                   \n     999      242.5272\n    1000      242.3155"
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-python-2",
    "href": "posts/xgboost-time-series/index.html#using-python-2",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using Python",
    "text": "Using Python\n\nfrom xgboost.sklearn import XGBRegressor\npy_xgboost_mod = XGBRegressor(n_estimator = 1000, early_stopping_rounds = 50)\npy_xgboost_mod.fit(py_x_train, py_y_train, \n                   eval_set = [(py_x_train, py_y_train), (py_x_test, py_y_test)], \n                   verbose = True)\n\nXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n             early_stopping_rounds=50, enable_categorical=False,\n             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n             missing=nan, monotone_constraints='()', n_estimator=1000,\n             n_estimators=100, n_jobs=0, num_parallel_tree=1, predictor='auto',\n             random_state=0, reg_alpha=0, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n             early_stopping_rounds=50, enable_categorical=False,\n             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n             missing=nan, monotone_constraints='()', n_estimator=1000,\n             n_estimators=100, n_jobs=0, num_parallel_tree=1, predictor='auto',\n             random_state=0, reg_alpha=0, ...)"
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-r-3",
    "href": "posts/xgboost-time-series/index.html#using-r-3",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using R",
    "text": "Using R\n2 ways to do this … (actually more than 2 ways, but here are 2 main ways.). First one is a straight table using the xgboost library itself.\n\nlibrary(xgboost)\nxgb.importance(model = fit_xgboost$fit)\n\n        Feature        Gain       Cover    Frequency\n1:  day_of_year 0.361828048 0.455387001 0.2800303942\n2:         hour 0.336852823 0.125331328 0.2374139102\n3:         year 0.120129969 0.129691117 0.2000679018\n4:  day_of_week 0.105250594 0.073258066 0.1489636887\n5: week_of_year 0.047083085 0.097216236 0.0462379151\n6: day_of_month 0.027801118 0.116483820 0.0864293336\n7:        month 0.001054362 0.002632432 0.0008568565\n\n#detach(xgboost)\n\nAnd also a graphic way.\n\nlibrary(vip)\nfit_xgboost %>%\n  vip(geom = \"point\")"
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-python-3",
    "href": "posts/xgboost-time-series/index.html#using-python-3",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using python",
    "text": "Using python\n\nfrom xgboost import plot_importance, plot_tree\n_ = plot_importance(py_xgboost_mod, height=0.9)\n\n\n\n\nI am a bit confused here in the output of the python graph with F-score vs the output of the R graph with importance."
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-r-4",
    "href": "posts/xgboost-time-series/index.html#using-r-4",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using R",
    "text": "Using R\nGraphing predicted power output vs actual power output could be a first way to see how our model fares in its predictions. So let’s graph our datetime vs power ouput for both actual and predicted.\n\nlibrary(tibble)  # for the add_column \nlibrary(parsnip)\ndf_test1 <- add_column(df_test,  predict(fit_xgboost, new_data = df_test)) \nggplot(df_test1, aes(x= Datetime, y = AEP_MW)) + \n  geom_line(color = \"blue\") + \n  geom_line(aes(y = .pred), color = \"yellow\", alpha = 0.5) + \n  labs(title = \"Energy Consumption in 2016-2018 (in MWh)\", y = \"Hourly consumption\")\n\n\n\n\nFigure 2: Actual Vs Predicted power consumption for 2016-2018\n\n\n\n\nWe can already see that we are not really modeling well the peaks and through.\nWe could get slightly more granular and try to see whats going on.\n\nggplot(df_test1 %>% filter(Datetime > \"2016-01-01\" & Datetime < \"2016-02-28\"), aes(x= Datetime, y = AEP_MW)) + \n  geom_line(color = \"blue\") + \n  geom_line(aes(y = .pred), color = \"yellow3\", alpha = 0.8)\n\n\n\n\nFigure 3: Actual Vs Predicted power consumption\n\n\n\n\nWe are clearly off there on the second half of February.\nNow, we can use the yardstick package to get numerical values to assess our model on the test set.\n\nlibrary(yardstick)\n# calculating the RMSE (root mean square error)\nrmse(df_test1, truth = AEP_MW, estimate = .pred, na_rm = TRUE)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       2067.\n\n# calculating the MAE (mean absolute error)\nmae(df_test1, truth = AEP_MW, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 mae     standard       1495.\n\n# calculating the MAPE (mean absolute percent error)\nmape(df_test1, truth = AEP_MW, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 mape    standard        10.0\n\n# actually much easier to use the metric_set() function !\nxgboost_mod_metrics <- metric_set(rmse, mae, mape)\nxgboost_mod_metrics(df_test1, truth = AEP_MW, estimate = .pred) \n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      2067. \n2 mae     standard      1495. \n3 mape    standard        10.0"
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html",
    "href": "posts/kmeans-regime-change/index.html",
    "title": "Kmeans with regime changes",
    "section": "",
    "text": "This post is about how to use Kmeans to classify various market regimes or to use Kmeans to classify financial observations.\nWith K-means we are trying to establish groups of data that are homegenous and distinctly different from other groups. The K- stands for the number of clusters we will create.\nThe concept of distance comes in when deciding if a data point belongs to a cluster. The most common way to measure distance is the Euclidean Distance.\nWith multivariate data set, it is important to normalize the data.\nA usual rule of thumb is to set the number of clusters as the square root of the number of observation."
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html#load-up-packages-and-read-data",
    "href": "posts/kmeans-regime-change/index.html#load-up-packages-and-read-data",
    "title": "Kmeans with regime changes",
    "section": "Load up packages and read data",
    "text": "Load up packages and read data\n\nlibrary(readr)        # load and read .csv file\nlibrary(glue)         # concatenate strings together\nlibrary(dplyr)        # the tidy plyr tool for data wrangling\nlibrary(tidyr)        # to use the drop_na function\nthe_path <- here::here()\ndf <- read_csv(glue(the_path, \"/raw_data/AMD.csv\")) |> \n  rename(adj_close = 'adjClose') |> \n  select(date, high, low, close, adj_close)\nglimpse(df)\n\nRows: 5,611\nColumns: 5\n$ date      <date> 2023-04-21, 2023-04-20, 2023-04-19, 2023-04-18, 2023-04-17,…\n$ high      <dbl> 89.8000, 91.5795, 90.5400, 92.1600, 90.6900, 92.9700, 93.160…\n$ low       <dbl> 88.0550, 88.7300, 88.2200, 89.3300, 88.3000, 90.5000, 91.830…\n$ close     <dbl> 88.43, 90.11, 89.94, 89.78, 89.87, 91.75, 92.09, 92.33, 94.0…\n$ adj_close <dbl> 88.43, 90.11, 89.94, 89.78, 89.87, 91.75, 92.09, 92.33, 94.0…"
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html#feature-engineering",
    "href": "posts/kmeans-regime-change/index.html#feature-engineering",
    "title": "Kmeans with regime changes",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nlibrary(TTR)      # The technical analysis package\nyo <- aroon(df[, c('high', 'low')], n = 23)\ndf$aroon <- yo[, 3]\nyo <- CCI(df[, c('high', 'low', 'close')], n = 17)\ndf$cci <- yo\nyo <- chaikinVolatility(df[, c('high', 'low')], n = 13)\ndf$chaikinVol <- yo\ndf1 <- df |> \n  select(date, aroon, cci, chaikinVol, adj_close) |> \n  mutate(across(c(aroon, cci, chaikinVol), ~ as.numeric(scale(.)))) |>\n  drop_na()\nskimr::skim(df1 %>% select(-date))\n\n\nData summary\n\n\nName\ndf1 %>% select(-date)\n\n\nNumber of rows\n5586\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\naroon\n0\n1\n0.00\n1.00\n-1.49\n-0.94\n-0.19\n0.90\n1.65\n▇▆▂▆▆\n\n\ncci\n0\n1\n0.00\n1.00\n-4.66\n-0.78\n-0.08\n0.80\n4.06\n▁▂▇▅▁\n\n\nchaikinVol\n0\n1\n0.00\n1.00\n-2.49\n-0.70\n-0.10\n0.59\n4.37\n▂▇▅▁▁\n\n\nadj_close\n0\n1\n22.27\n28.78\n1.62\n5.45\n11.48\n23.12\n161.91\n▇▁▁▁▁\n\n\n\n\n# also good to check for correlation between variables. \nlibrary(corrr)\ndf1 |> select(-date, -adj_close) |> \n  correlate() |> \n  rearrange() |> \n  shave()\n\n# A tibble: 3 × 4\n  term          cci  aroon chaikinVol\n  <chr>       <dbl>  <dbl>      <dbl>\n1 cci        NA     NA             NA\n2 aroon       0.564 NA             NA\n3 chaikinVol  0.212  0.223         NA\n\n\nThese 3 variables seem to complete each other well as little to-no correlation."
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html#create-clusters",
    "href": "posts/kmeans-regime-change/index.html#create-clusters",
    "title": "Kmeans with regime changes",
    "section": "Create clusters",
    "text": "Create clusters\n\nlibrary(purrr)     #use the map function\nlibrary(broom)     #use the glance function on kmeans \ndf1sc <- df1 %>% select(-date, -adj_close)\nkclusts <- tibble(k = 1:9) |> \n  mutate(kclust = map(k, ~kmeans(df1sc, centers = .x, nstart = 30, iter.max = 50L)), \n         glanced = map(kclust, glance), \n         augmented = map(kclust, augment, df1))\nkclusts |> unnest(cols = c('glanced'))\n\n# A tibble: 9 × 7\n      k kclust    totss tot.withinss betweenss  iter augmented           \n  <int> <list>    <dbl>        <dbl>     <dbl> <int> <list>              \n1     1 <kmeans> 16755.       16755. -2.04e-10     1 <tibble [5,586 × 6]>\n2     2 <kmeans> 16755.        9842.  6.91e+ 3     1 <tibble [5,586 × 6]>\n3     3 <kmeans> 16755.        7916.  8.84e+ 3     3 <tibble [5,586 × 6]>\n4     4 <kmeans> 16755.        6425.  1.03e+ 4     4 <tibble [5,586 × 6]>\n5     5 <kmeans> 16755.        5492.  1.13e+ 4     4 <tibble [5,586 × 6]>\n6     6 <kmeans> 16755.        4626.  1.21e+ 4     6 <tibble [5,586 × 6]>\n7     7 <kmeans> 16755.        4199.  1.26e+ 4     4 <tibble [5,586 × 6]>\n8     8 <kmeans> 16755.        3791.  1.30e+ 4     7 <tibble [5,586 × 6]>\n9     9 <kmeans> 16755.        3477.  1.33e+ 4     4 <tibble [5,586 × 6]>\n\n\nThere are several ways to choose the ideal number of clusters. One of them is the elbow method, another one is the Silhouette Method.\nThe tot.withinss is the total within-cluster sum of square. This is the value used for the eblow method.\nFor the Silhouette Method, we can use the cluster package.\n\navg_sil <- function(k) { \n  kmeans_object <- kmeans(df1sc, centers = k, iter.max = 50L)\n  silh = cluster::silhouette(kmeans_object$cluster, dist(df1sc))\n  mean(silh[, 3])\n  }\n# Compute and plot wss for k = 2 to k = 15\nyo <- tibble(k_values =  2:9) |> \n  mutate(avg_sil_values = map_dbl(k_values, avg_sil))\nyo\n\n# A tibble: 8 × 2\n  k_values avg_sil_values\n     <int>          <dbl>\n1        2          0.378\n2        3          0.345\n3        4          0.286\n4        5          0.312\n5        6          0.296\n6        7          0.284\n7        8          0.295\n8        9          0.279\n\n\nA more elegant way to do that, using this post from SO\n\nyo <- kclusts |> \n  mutate(silhouetted = map(augmented, ~ cluster::silhouette(as.numeric(levels(.x$.cluster))[.x$.cluster], dist(df1sc)))) |> \n  select(k, silhouetted) |> unnest(cols=c('silhouetted')) |> \n  group_by(k) %>% \n  summarise(avg_sil_values = mean(silhouetted[,3]))\nyo\n\n# A tibble: 9 × 2\n      k avg_sil_values\n  <int>          <dbl>\n1     1         NA    \n2     2          0.378\n3     3          0.345\n4     4          0.293\n5     5          0.313\n6     6          0.320\n7     7          0.305\n8     8          0.298\n9     9          0.273"
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html#some-visualizations",
    "href": "posts/kmeans-regime-change/index.html#some-visualizations",
    "title": "Kmeans with regime changes",
    "section": "Some visualizations",
    "text": "Some visualizations\n\nElbow method\n\nlibrary(ggplot2)\nkclusts |> \n  unnest(cols = c('glanced')) |> \n  ggplot(aes(k, tot.withinss)) + \n  geom_line(alpha = 0.5, size = 1.2, color = 'midnightblue') + \n  geom_point(size = 2, color = 'midnightblue')\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nTotal within-cluster sum of square for k-cluster\n\n\n\n\nBased on the elbow method, I would be tempted to choose to 5 clusters (2 seems another obvious one).\n\n\nSilhouette Method\n\nyo |> ggplot(aes(k, avg_sil_values)) + \n  geom_line(alpha = 0.5, size = 1.2, color = 'midnightblue') + \n  geom_point(size = 2, color = 'midnightblue')\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nSilhouette score for k-clusters\n\n\n\n\n2 is the winner ;-)\n\n\nPlotting the stocks with clustered observations\n\nlibrary(lubridate)\nyo <- kmeans(df1 |> select(-date, -adj_close), centers = 2)\naugment(yo, df1) |> filter(date >= today() - 500) |> \n  ggplot(aes(x = date, y = adj_close)) + \n    geom_line(alpha = 0.5, color = 'midnightblue') + \n    geom_point(aes(color = .cluster)) + \n    theme(legend.position = 'none')\n\n\n\n\nPlotting adjusted close price with only 2 clusters\n\n\n\n\n\nyo <- kmeans(df1 |> select(-date, -adj_close), centers = 3)\naugment(yo, df1) |> filter(date >= today() - 500) |> \n  ggplot(aes(x = date, y = adj_close)) + \n    geom_line(alpha = 0.5, color = 'midnightblue') + \n    geom_point(aes(color = .cluster)) + \n    theme(legend.position = 'none')\n\n\n\n\nPlotting adjusted close price with only 3 clusters\n\n\n\n\n\nyo <- kmeans(df1 |> select(-date, -adj_close), centers = 6)\naugment(yo, df1) |> filter(date >= today() - 500) |> \n  ggplot(aes(x = date, y = adj_close)) + \n    geom_line(alpha = 0.5, color = 'midnightblue') + \n    geom_point(aes(color = .cluster)) + \n    theme(legend.position = 'none')\n\n\n\n\nPlotting adjusted close price with only 6 clusters"
  },
  {
    "objectID": "posts/autocorrelation/index.html",
    "href": "posts/autocorrelation/index.html",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "",
    "text": "This post is to set up the basic concepts of time-series analysis."
  },
  {
    "objectID": "posts/stochastic_processes/index.html",
    "href": "posts/stochastic_processes/index.html",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "",
    "text": "This post is an introduction to Markov Chain with a presentation of Discrete Time Markov Chains."
  },
  {
    "objectID": "posts/stochastic_processes/index.html#definition",
    "href": "posts/stochastic_processes/index.html#definition",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Definition",
    "text": "Definition\nA stochastic process is \\(\\{ X(t), t \\in T \\}\\) is a collection of random variables indexed by a parameter t that belongs to a set T.\n\nt is generally the time\n\\(X(t)\\) is the state of the process at time t\nThe state space \\(S\\) of a stochastic process is all possible state \\(X(t)\\) for any \\(t \\in T\\)\nif T is a countable set, we call this a discrete-time process\n\nA discrete-time Markov Chain is a discrete-time stochastic process which state space S is finite such that: \\[\\mathbb{P}(X_{n+1} = j | X_0 = i_0, X_1 = i_1, X_2 = i_2, \\dots, x_n = i) = \\mathbb{P}(X_{n+1} = j | X_n = i) = P_{ij}\\]\nthat is, the conditional probability of the process being in state j at time n + 1 given all the previous states depends only on the last-known position (state i at time n)."
  },
  {
    "objectID": "posts/stochastic_processes/index.html#some-other-teminology",
    "href": "posts/stochastic_processes/index.html#some-other-teminology",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Some other teminology",
    "text": "Some other teminology\n\nA state is called absorbing if the chain cannot leave it once it enters it. An absorbing Markov chain has at least one absorbing state.\nA state is termed reflecting if once the chain leaves it, it cannot return to it.\nThe period d of a state i is the number such that, starting in i, the chain can return to i only in the number of steps that are multiples of d. A state with period d = 1 is called aperiodic. Periodicity is a class property.\n\nFor a reflecting state, the period is infinite, since the chain never comes back to this state.\nAbsorbing states necessarily have loops and thus are aperiodic states.\n\na state is called recurrent if with probability 1 the chain ever reenters that state. Otherwise, the state is called transient."
  },
  {
    "objectID": "posts/stochastic_processes/index.html#chapman-kolmogorov-equations",
    "href": "posts/stochastic_processes/index.html#chapman-kolmogorov-equations",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Chapman-Kolmogorov equations",
    "text": "Chapman-Kolmogorov equations\nWe denote the probability to go from state \\(i\\) to state \\(j\\) in n-steps by \\(\\bf{P}_{ij}^{(n)}\\). It is also denoted as the n-steps transition probability matrix. That is for any time \\(m >= 0, \\bf{P}_{ij}^n = \\mathbb{P}(X_{m+n} = j | X_m = i)\\) . \\(\\bf{P}^{(n)} = \\bf{P}^n\\) based on the Chapman-Kolmogorov equation.\nThe Chapman-Kolmogorov equation states that for all positive integers \\(m\\) and \\(n\\) , \\(\\bf{P}^{(m+n)} = \\bf{P}^m \\cdot \\bf{P}^n\\) where P is a one-step probability transition matrix (a square matrix)"
  },
  {
    "objectID": "posts/stochastic_processes/index.html#example-1",
    "href": "posts/stochastic_processes/index.html#example-1",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Example 1",
    "text": "Example 1\nTo model a Markov Chain, let’s first set up a one-step probability transition matrix (called here osptm).\nWe start with an easy 3 possible state process. That is the state space \\(S = \\{1, 2, 3\\}\\). The osptm will provide the probability to go from one state to another.\n\nosptm = matrix(c(0.7,0.1,0.2, 0,0.6,0.4, 0.5,0.2,0.3), nrow = 3, byrow = TRUE)\nosptm\n\n     [,1] [,2] [,3]\n[1,]  0.7  0.1  0.2\n[2,]  0.0  0.6  0.4\n[3,]  0.5  0.2  0.3\n\n\nWe can always have a look at how the osptm looks like.\n\n# note we have to transpose the osptm matrix first. \nosptm_transposed = t(osptm)\nosptm_transposed\n\n     [,1] [,2] [,3]\n[1,]  0.7  0.0  0.5\n[2,]  0.1  0.6  0.2\n[3,]  0.2  0.4  0.3\n\ndiagram::plotmat(osptm_transposed, pos = c(1, 2), arr.length = 0.3, \n                 box.col = \"lightblue\", box.prop = 0.5, box.size = 0.12, box.type=\"circle\", \n                 self.cex = 0.6, self.shifty=-0.01, self.shiftx = 0.15)\n\n\n\n\nThe markovchain package can provide us with all the state characteristics of a one-step probabilty transition matrix.\n\nlibrary(markovchain)\nosptm_mc <- new(\"markovchain\", transitionMatrix = osptm)\n\nrecurrentClasses(osptm_mc)\n\n[[1]]\n[1] \"1\" \"2\" \"3\"\n\ntransientClasses(osptm_mc)\n\nlist()\n\nabsorbingStates(osptm_mc)\n\ncharacter(0)\n\nperiod(osptm_mc)\n\n[1] 1\n\nround(steadyStates(osptm_mc), 4)\n\n          1      2      3\n[1,] 0.4651 0.2558 0.2791\n\n\nThe next step is to calculate, for instance, what is the probability to go from state 1 to state 3 in 4 steps.\n\nlibrary(expm)\n\n# the expm library brings in the \" %^%\" operator for power. \nosptm %^% 4\n\n       [,1]   [,2]   [,3]\n[1,] 0.5021 0.2303 0.2676\n[2,] 0.3860 0.3104 0.3036\n[3,] 0.4760 0.2483 0.2757\n\n\nLooking at the result, we can see that the probability to go from State 1 to State 3 in 4 steps is 0.2676\nWe can also calculate the unconditional distribution after 4 steps\n\ninitial_pro <- c(1/3, 1/3, 1/3)\ninitial_pro %*% (osptm %^% 4)\n\n       [,1]  [,2]   [,3]\n[1,] 0.4547 0.263 0.2823"
  },
  {
    "objectID": "posts/stochastic_processes/index.html#example-2",
    "href": "posts/stochastic_processes/index.html#example-2",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Example 2",
    "text": "Example 2\nUsing a slightly more interesting one-step probability transition matrix having 6 different states.\n\n#specifying transition probability matrix\nosptm<- matrix(c(0.3,0.7,0,0,0,0,1,0,0,0,0,0,0.5,0,0,0,0,0.5, 0,0,0.6,0,0,0.4,0,0,0,0,0.1,0.9,0,0,0,0,0.7,0.3), nrow=6, byrow=TRUE)\nosptm\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  0.3  0.7  0.0    0  0.0  0.0\n[2,]  1.0  0.0  0.0    0  0.0  0.0\n[3,]  0.5  0.0  0.0    0  0.0  0.5\n[4,]  0.0  0.0  0.6    0  0.0  0.4\n[5,]  0.0  0.0  0.0    0  0.1  0.9\n[6,]  0.0  0.0  0.0    0  0.7  0.3\n\nosptm_transposed = t(osptm)\ndiagram::plotmat(osptm_transposed, arr.length = 0.3, arr.width = 0.1, \n                 box.col = \"lightblue\", box.prop = 0.5, box.size = 0.09, box.type=\"circle\", \n                 cex.txt = 0.8, self.cex = 0.6, self.shifty=-0.01, self.shiftx = 0.13)\n\n\n\nosptm_mc <- new(\"markovchain\", transitionMatrix = osptm)\n\nrecurrentClasses(osptm_mc)\n\n[[1]]\n[1] \"1\" \"2\"\n\n[[2]]\n[1] \"5\" \"6\"\n\ntransientClasses(osptm_mc)\n\n[[1]]\n[1] \"3\"\n\n[[2]]\n[1] \"4\"\n\nabsorbingStates(osptm_mc)\n\ncharacter(0)\n\nperiod(osptm_mc)\n\nWarning in period(osptm_mc): The matrix is not irreducible\n\n\n[1] 0\n\nround(steadyStates(osptm_mc), 4)\n\n          1      2 3 4      5      6\n[1,] 0.0000 0.0000 0 0 0.4375 0.5625\n[2,] 0.5882 0.4118 0 0 0.0000 0.0000"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is François de Ryckel. I have grown up in Belgium then I emigrated from there to finish up my study and start working. I lived in several places over the last 20 years. I’m a math / philosophy teacher with some stunts at business. I created 2 companies in Zambia: a fruits farm and a fresh produce trading company.\n\nLived in Paris, France, for 2 years to complete my undergrad and start my master\nLived in Freiburg & Leipzig, Germany, for 3 years to complete my master and do some teaching gigs at Alliance Française, Leipzig Universitat and Leipzig International School\nLived in Dhaka, Bangladesh for 3 years to teach at the International School Dhaka\nLived in Zambia for 9 years to teach math, stats and philosophy at the American international School of Lusaka. I also started a citrus & mangoes farm (over 9,000 trees) and a produce (fresh fish, fruits, and meat) trading company\nLived Thuwal, Saudi Arabia on the shore of the beautiful Red Sea for 5 years. I worked as math teacher on the KAUST university Campus at TKS (The KAUST School)\nCurrently living in Bangkok (Thailand) as a curriculum coordinator for the KIS School.\n\nAs a teacher, I’m always looking for good examples to incorporate in my practices. Lately I am especially interested in machine learning applications in finance and education.\n\nEducation\n\nMaster in Philosophy, 2001\nParis I - Pantheon Sorbonne\n\n\nBachelor in Logic, 2000\nParis I - Pantheon Sorbonne\n\n\nBachelor in Philosophy, 1999\nParis I - Pantheon Sorbonne\n\n\n\nInterests\n\nModeling and machine learning\nMath education\nOutdoor Living and active lifestyle (Swimming, Cycling, Running)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Calculus - Part 3\n\n\n\n\n\n\n\ntime-series\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuant Puzzle #01\n\n\n\n\n\n\n\nR-code\n\n\nquant-finance\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n  \n\n\n\n\nBinomials models for Quantitative Finance\n\n\n\n\n\n\n\nR-code\n\n\nquant-finance\n\n\nbinomial_models\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaive-Bayes - Part 1\n\n\n\n\n\n\n\nML\n\n\nNaive-Bayes\n\n\nsentiment analysis\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n  \n\n\n\n\nNormality of asset returns\n\n\n\n\n\n\n\nquant-finance\n\n\nnormal-distribution\n\n\nqq-plot\n\n\nkurtosis\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\nR-code\n\n\nlinear-regression\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n  \n\n\n\n\nStatistical Moments\n\n\n\n\n\n\n\nstatistics\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2022\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n  \n\n\n\n\nRandom Behavior of Financial Assets\n\n\n\n\n\n\n\nR-code\n\n\nquant-finance\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2023\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Algebra for Quantitative Finance\n\n\n\n\n\n\n\nR-code\n\n\nquant-finance\n\n\nLinear-Algebra\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPortfolio Optimization Part I (in R)\n\n\n\n\n\n\n\nportfolio\n\n\nR-code\n\n\nquant-finance\n\n\nmodel\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic processes - Discrete Time Markov Chain\n\n\n\n\n\n\n\nMarkov Chain\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Kmeans\n\n\n\n\n\n\n\nkmeans\n\n\ncode\n\n\nanalysis\n\n\nmodel\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Chains\n\n\n\n\n\n\n\nRandom Walk\n\n\nMarkov Chain\n\n\ncode\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKmeans with regime changes\n\n\n\n\n\n\n\nkmeans\n\n\ncode\n\n\nanalysis\n\n\ntidymodel\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlgebra For Quant\n\n\n\n\n\n\n\nstatistics\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTranslating Python Part 1 - Xgboost with Time-Series\n\n\n\n\n\n\n\nxgboost\n\n\ntidymodel\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability For Quant\n\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2022\n\n\nFrancois de Ryckel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutoCorrelation, Stationarity and Random-Walk - Part 1\n\n\n\n\n\n\n\ntime-series\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2022\n\n\nFrancois de Ryckel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/portfolio_optimization_parti_inR/index.html",
    "href": "posts/portfolio_optimization_parti_inR/index.html",
    "title": "Portfolio Optimization Part I (in R)",
    "section": "",
    "text": "First post on portfolio optimization from a quantitative finance lense.\nWe are optimizing a portfolio with N assets, where \\(N \\geq 2\\) (N is a positive integer)\nOf course, the sum of all the weights should be equal to 1.\n\\[\\sum_{i = 1}^{N} W_i = 1  \\tag{2}\\]\nFew assumptions made on the assets.\nThe Mean-Variance Optimization problem can be formulated in 2 ways:\nReturn is the expected return of the asset and risk is the variance of the returns of the asset.\nThe Risk-Free Asset (RFA) is the money deposited in a bank account-ish (a secure term-deposit) at a fixed rate R. The expected return is thus R and volatitliy is 0. Also the correlation between the RFA and any other assets is 0.\nEach asset can be represented on a 2D-plane with the risk on the x-axis and returns on the y-axis.\nThere are some other assumptions made when trying to construct a mean-variance optimum portfolio:"
  },
  {
    "objectID": "posts/portfolio_optimization_parti_inR/index.html#in-practice",
    "href": "posts/portfolio_optimization_parti_inR/index.html#in-practice",
    "title": "Portfolio Optimization Part I (in R)",
    "section": "In practice",
    "text": "In practice\nLet’s get 5 different financial assets: AA, LLY, AMD, SBUX, FDX. Although they are from different industries, it is not a very diverse bunch as they are all from US big companies.\nWe already have the assets downloaded and we’ll use their closing prices.\n\nlibrary(readr)      # read_csv()\nlibrary(dplyr)      # select(), arrange(), filter(), mutate()\nlibrary(purrr)      # map()\nlibrary(tidyr)      # pivot_wider()\nlibrary(glue)       # glue()\n\n# read adjusted closing prices and compute annualized daily returns and sd\nread_prices <- function(ticker) { \n  df <- read_csv(glue('../../raw_data/', {ticker}, '.csv')) |> \n    arrange(date) |> \n    select(date, adj_close = adjClose) |> \n    filter(date > '2018-01-01') |> \n    mutate(ret1d = (adj_close / lag(adj_close, 1)) - 1) |> \n    summarize(mean_ret = mean(ret1d, na.rm = T) * 252 * 100, \n              std_ret = sd(ret1d, na.rm = T) * sqrt(252) * 100)\n}\n\nassets <- c('AA', 'LLY', 'AMD', 'SBUX', 'FDX')\ndf <- tibble(ticker = assets, \n             prices = map(ticker, read_prices)) |> \n  unnest()\n\n\n\n\n\nTable 1:  Mean and Standard Deviation of returns \n \n  \n    Ticker \n    Mean Ret \n    Std of Ret \n  \n \n\n  \n    AA \n    15.90 \n    60.72 \n  \n  \n    LLY \n    31.79 \n    29.78 \n  \n  \n    AMD \n    55.31 \n    56.70 \n  \n  \n    SBUX \n    17.47 \n    30.66 \n  \n  \n    FDX \n    4.40 \n    37.53"
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "Correlation between assets",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\nlibrary(roll)"
  },
  {
    "objectID": "correlation.html#random-walk-coding-practices",
    "href": "correlation.html#random-walk-coding-practices",
    "title": "Correlation between assets",
    "section": "Random walk coding practices",
    "text": "Random walk coding practices\n\n# Set the number of steps\nn_steps <- 100\n\n# Set the probability of moving up or down\nprob_up <- 0.5\nprob_down <- 1 - prob_up\n\n# Initialize the random walk\nrandom_walk <- numeric(n_steps)\nrandom_walk[1] <- 0 # Starting point\n\n# Simulate the random walk\nfor (i in 2:n_steps) {\n  # Generate a random step up or down\n  step <- rbinom(1, 1, prob_up)\n\n  # Update the position in the random walk\n  random_walk[i] <- random_walk[i-1] + (2*step-1)\n}\n\n# Plot the random walk\nplot(random_walk, type=\"l\", xlab=\"Time\", ylab=\"Position\", main=\"Random Walk Simulation\")\n\n\n\n\nThis is one realization of a random walk.\n\n# create a binomial model.  \n# You either get 1 dollars or loose 1 dollars.  (or eighter you go one step up or one step down)\nnum_steps <- 100 \nprob_up <- 0.5\ny <- numeric(num_steps)\ny[1] <- 0\n\nfor (i in 2:num_steps) {\n  step <- rbinom(1, 1, prob_up)\n  y[i] <- y[i-1] + ((2 * step) - 1)\n}\ny\n\n  [1]   0  -1  -2  -3  -2  -3  -4  -3  -4  -5  -6  -5  -4  -3  -2  -1  -2  -3\n [19]  -4  -3  -2  -3  -4  -5  -6  -5  -6  -7  -8  -7  -6  -5  -4  -5  -4  -3\n [37]  -2  -3  -4  -3  -4  -5  -6  -7  -6  -7  -6  -7  -8  -9  -8  -7  -6  -5\n [55]  -4  -5  -4  -5  -4  -3  -2  -1   0   1   2   3   2   1   0  -1   0  -1\n [73]   0  -1  -2  -3  -4  -3  -2  -3  -4  -3  -4  -5  -4  -3  -4  -5  -4  -5\n [91]  -6  -5  -6  -7  -8  -9 -10 -11 -12 -11\n\nplot(y, type=\"l\", xlab=\"Time\", ylab=\"Position\", main=\"Random Walk Simulation\")\n\n\n\n\nUsing the tidy approach. One realization of a binomial random walk\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(ggplot2)\nnum_steps = 100\nprob_up = 0.5\n\ndf <- tibble(steps = 1:num_steps, \n             binom_up_down = rbinom(n = 100, size = 1, prob = prob_up)) %>% \n  mutate(up_down = if_else(binom_up_down == 1, 1, -1), \n         position = cumsum(up_down))\n\nggplot(df, aes(x = steps, y = position)) + \n  geom_line()\n\n\n\n\nUsing a trinomial distribution now\n\nlibrary(purrr)\nnum_steps = 1000\nalpha = 0.28\nprob_up = alpha\nprob_down = alpha\nprob_same = 1 - 2*alpha\n\n# runif(1) take a number between 0 and 1 using a uniform distribution.  \n# we then use the prob properties to say if it +1, 0 or -1\n\ndf = tibble(steps = 1:num_steps, \n            ones = rep(1, num_steps)) |> \n  mutate(change = map_dbl(ones, runif), \n         up_down = if_else(change < alpha, -1, if_else(change > 1 - alpha, 1, 0)), \n         position = cumsum(up_down))\n\nggplot(df, aes(x = steps, y = position)) + \n  geom_line()"
  },
  {
    "objectID": "posts/portfolio_optimization_parti_inR/index.html#create-mc-simulation-of-portfolio",
    "href": "posts/portfolio_optimization_parti_inR/index.html#create-mc-simulation-of-portfolio",
    "title": "Portfolio Optimization Part I (in R)",
    "section": "Create MC simulation of portfolio",
    "text": "Create MC simulation of portfolio\nFirst need to create the df of returns\n\n# functions to get daily returns of each assets\ncreate_returns_df <- function(ticker) { \n  df <- read_csv(glue('../../raw_data/', {ticker}, '.csv')) |> \n    arrange(date) |> \n    select(date, adj_close = adjClose) |> \n    filter(date > '2018-01-01') |> \n    mutate(ret1d = (adj_close / lag(adj_close, 1)) - 1) |> \n    select(date, ret1d)\n}\n\n# df of each assets and all their daily returns\ndf <- tibble(ticker = assets, \n             prices = map(ticker, create_returns_df)) |> \n  unnest(cols = c(prices)) \n\nhead(df)\n\n# A tibble: 6 × 3\n  ticker date          ret1d\n  <chr>  <date>        <dbl>\n1 AA     2018-01-02 NA      \n2 AA     2018-01-03 -0.0121 \n3 AA     2018-01-04  0.00367\n4 AA     2018-01-05 -0.0112 \n5 AA     2018-01-08  0.0168 \n6 AA     2018-01-09 -0.0145 \n\n\n\n# this df to provide vectors of expected returns\ndf_stat <- df |> \n  group_by(ticker) |> \n  summarize(mean_ret = mean(ret1d, na.rm = T) * 252) |> \n  ungroup()\n\ndf_stat\n\n# A tibble: 5 × 2\n  ticker mean_ret\n  <chr>     <dbl>\n1 AA       0.159 \n2 AMD      0.553 \n3 FDX      0.0440\n4 LLY      0.318 \n5 SBUX     0.175 \n\n# this df to provide the covariance matrix\n# note how we have also multiplied it by 252\nsigma <- df |> \n  pivot_wider(names_from = ticker, values_from = ret1d) |>  \n  drop_na() |> \n  select(-date) |> cov() * 252\n\nsigma\n\n             AA        LLY        AMD       SBUX        FDX\nAA   0.36863464 0.02245473 0.10701604 0.06858293 0.09284847\nLLY  0.02245473 0.08866268 0.04103420 0.02756020 0.02329557\nAMD  0.10701604 0.04103420 0.32149066 0.07364100 0.08414403\nSBUX 0.06858293 0.02756020 0.07364100 0.09402057 0.05421996\nFDX  0.09284847 0.02329557 0.08414403 0.05421996 0.14086886\n\n# this function to create one simulation using random weights\ncreate_one_portfolio_simulation <- function(n) { \n  # pick random weights\n  weights_rand = runif(5)\n  weights = matrix(weights_rand / sum(weights_rand), nrow = length(assets))\n  \n  #these are textbook formula for return and volat of a portfolio\n  return_pi = as.numeric(t(weights) %*% df_stat$mean_ret)\n  volat_pi = as.numeric((t(weights) %*% sigma) %*% weights)\n  sharpe_ratio = return_pi / volat_pi\n  \n  # wrap everything into a df for later checks / analysis\n  df <- tibble(portf_ret = return_pi, portf_volat = volat_pi, \n               weights = t(weights), sharpe_ratio = sharpe_ratio)\n\n  return(df)\n}\n\n#this is really the only inputs to get \nnum_portfolio = 5000\n\nmc_simu = tibble(id = 1:num_portfolio) |> \n  mutate(simul = map(id, create_one_portfolio_simulation)) |> \n  unnest(cols=c(simul)) |> \n  arrange(desc(sharpe_ratio))\n\nhead(mc_simu)\n\n# A tibble: 6 × 5\n     id portf_ret portf_volat weights[,1]  [,2]    [,3]   [,4]    [,5] sharpe_…¹\n  <int>     <dbl>       <dbl>       <dbl> <dbl>   <dbl>  <dbl>   <dbl>     <dbl>\n1  2863     0.436      0.0575      0.0217 0.566 0.00990 0.340  0.0623       7.58\n2   465     0.440      0.0595      0.0806 0.588 0.00452 0.310  0.0169       7.39\n3  1263     0.438      0.0598      0.0471 0.590 0.0385  0.317  0.00748      7.31\n4  3724     0.429      0.0601      0.0727 0.615 0.0388  0.193  0.0806       7.13\n5  2200     0.458      0.0644      0.0325 0.728 0.0251  0.0800 0.134        7.11\n6  3102     0.404      0.0580      0.0549 0.502 0.0532  0.327  0.0625       6.96\n# … with abbreviated variable name ¹​sharpe_ratio"
  },
  {
    "objectID": "posts/portfolio_optimization_parti_inR/index.html#in-practice-create-mc-simulations-of-weights-to-assess-mean-variance-of-a-portfolio",
    "href": "posts/portfolio_optimization_parti_inR/index.html#in-practice-create-mc-simulations-of-weights-to-assess-mean-variance-of-a-portfolio",
    "title": "Portfolio Optimization Part I (in R)",
    "section": "In practice: Create MC simulations of weights to assess mean-variance of a portfolio",
    "text": "In practice: Create MC simulations of weights to assess mean-variance of a portfolio\nLet’s get 5 different financial assets: AA, LLY, AMD, SBUX, FDX. Although they are from different industries, it is not a very diverse bunch as they are all from US big companies.\nWe already have the assets downloaded and we’ll use their closing prices.\n\nlibrary(readr)      # read_csv()\nlibrary(dplyr)      # select(), arrange(), filter(), mutate(), summarize()\nlibrary(purrr)      # map()\nlibrary(tidyr)      # drop_na(), pivot_wider(), unnest()\nlibrary(glue)       # glue()\n\n# read adjusted closing prices and compute annualized daily returns and sd\nread_prices <- function(ticker) { \n  df <- read_csv(glue('../../raw_data/', {ticker}, '.csv')) |> \n    arrange(date) |> \n    select(date, adj_close = adjClose) |> \n    filter(date > '2018-01-01') |> \n    mutate(ret1d = (adj_close / lag(adj_close, 1)) - 1) |> \n    summarize(mean_ret = mean(ret1d, na.rm = T) * 252 * 100, \n              std_ret = sd(ret1d, na.rm = T) * sqrt(252) * 100)\n}\n\nassets <- c('AA', 'LLY', 'AMD', 'SBUX', 'FDX')\ndf <- tibble(ticker = assets, \n             prices = map(ticker, read_prices)) |> \n  unnest()\n\n\n\n\n\nTable 1:  Annualized Mean and Standard Deviation of daily returns \n \n  \n    Ticker \n    Mean Ret \n    Std of Ret \n  \n \n\n  \n    AA \n    15.90 \n    60.72 \n  \n  \n    LLY \n    31.79 \n    29.78 \n  \n  \n    AMD \n    55.31 \n    56.70 \n  \n  \n    SBUX \n    17.47 \n    30.66 \n  \n  \n    FDX \n    4.40 \n    37.53 \n  \n\n\n\n\n\n\nFirst, we create the df of returns:\n\nit’s a long df with only ticker, date, daily returns\none row per daily observation.\nwe drop first row with no returns\nthe returns df is a wide df with date and tickers as columns, then daily returns as row\n\n\n# functions to get daily returns of each assets\ncreate_returns_df <- function(ticker) { \n  df <- read_csv(glue('../../raw_data/', {ticker}, '.csv')) |> \n    arrange(date) |> \n    select(date, adj_close = adjClose) |> \n    filter(date > '2018-01-01') |> \n    mutate(ret1d = (adj_close / lag(adj_close, 1)) - 1) |> \n    select(date, ret1d)\n}\n\n# df of each assets and all their daily returns\ndf <- tibble(ticker = assets, \n             prices = map(ticker, create_returns_df)) |> \n  unnest(cols = c(prices)) |> \n  drop_na()\n\n\nreturns <- df |> arrange(ticker) |> \n  pivot_wider(names_from = ticker, values_from = ret1d)  \n\nhead(returns)\n\n# A tibble: 6 × 6\n  date             AA     AMD       FDX       LLY     SBUX\n  <date>        <dbl>   <dbl>     <dbl>     <dbl>    <dbl>\n1 2018-01-03 -0.0121   0.0519  0.0125    0.00543   0.0187 \n2 2018-01-04  0.00367  0.0494  0.0156    0.00446   0.00375\n3 2018-01-05 -0.0112  -0.0198  0.00393   0.0123    0.0115 \n4 2018-01-08  0.0168   0.0337  0.0103   -0.00508  -0.00503\n5 2018-01-09 -0.0145  -0.0375 -0.00339  -0.000813 -0.00219\n6 2018-01-10  0.0363   0.0118  0.000971  0.000465  0.0108 \n\n\nTo optimize the mean-variance of the portfolio, we consider the following\nWeights of each assets are \\(w = \\pmatrix{w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n}\\).\nMean returns of each assets are \\(\\mu = \\pmatrix{\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n}\\)\nThen, the expected portfolio return is \\(\\mu_{\\pi} = w^{T} \\cdot \\mu\\) where \\(w^{T}\\) is the transpose of \\(w\\) (aka transforming \\(w\\) from a column vector to a row vector in order to have right dimensions to compute the dot product).\nAnd the expected portfolio variance is computed by \\(\\sigma_{\\pi}^2 = w^T \\cdot \\Sigma \\cdot w\\) where \\(\\sigma\\) is the covariance matrix.\n\n# this df to provide vectors of expected returns\ndf_stat <- df |> \n  group_by(ticker) |> \n  summarize(mean_ret = mean(ret1d, na.rm = T) * 252) |> \n  ungroup() |> arrange(ticker) \n\nmu = as.matrix(df_stat$mean_ret, nrow = length(assets))\n\n# this df to provide the covariance matrix\n# note how we have also multiplied it by 252\nsigma <- df |> arrange(ticker) |> \n  pivot_wider(names_from = ticker, values_from = ret1d) |> \n  #drop_na() |> \n  select(-date) |> cov() * 252\n\nsigma <- as.matrix(sigma, nrow = length(assets))\n\n# this function to create one simulation using random weights\ncreate_one_portfolio_simulation <- function(n) { \n  # pick random weights\n  weights_rand = runif(length(assets))\n  weights = matrix(weights_rand / sum(weights_rand), nrow = length(assets))\n  \n  #these are textbook formula for return and volat of a portfolio\n  return_pi = as.numeric(t(weights) %*% mu)\n  volat_pi = sqrt(as.numeric((t(weights) %*% sigma) %*% weights))\n  sharpe_ratio = return_pi / volat_pi\n  \n  # wrap everything into a df for later checks / analysis\n  df <- tibble(portf_ret = return_pi * 100, portf_volat = volat_pi * 100, \n               weights = t(weights), sharpe_ratio = sharpe_ratio)\n\n  return(df)\n}\n\n#this is really the only inputs to get \nnum_portfolio = 5000\n\nmc_simu = tibble(id = 1:num_portfolio) |> \n  mutate(simul = map(id, create_one_portfolio_simulation)) |> \n  unnest(cols=c(simul)) |> \n  arrange(desc(sharpe_ratio))\n\nhead(mc_simu)\n\n# A tibble: 6 × 5\n     id portf_ret portf_volat weights[,1]  [,2]     [,3]  [,4]   [,5] sharpe_r…¹\n  <int>     <dbl>       <dbl>       <dbl> <dbl>    <dbl> <dbl>  <dbl>      <dbl>\n1    28      37.7        29.8     0.00932 0.341 0.0160   0.529 0.105        1.26\n2  2785      37.0        29.3     0.0276  0.323 0.000793 0.518 0.131        1.26\n3   825      37.5        29.9     0.0524  0.331 0.0244   0.549 0.0428       1.25\n4  3989      33.9        27.4     0.0550  0.246 0.00738  0.512 0.179        1.24\n5  1896      32.8        26.6     0.0114  0.220 0.00710  0.497 0.265        1.23\n6  3383      38.1        30.9     0.0682  0.370 0.00758  0.474 0.0803       1.23\n# … with abbreviated variable name ¹​sharpe_ratio"
  },
  {
    "objectID": "posts/portfolio_optimization_parti_inR/index.html#create-mc-simulations-of-weights-to-assess-mean-variance-of-a-portfolio",
    "href": "posts/portfolio_optimization_parti_inR/index.html#create-mc-simulations-of-weights-to-assess-mean-variance-of-a-portfolio",
    "title": "Portfolio Optimization Part I (in R)",
    "section": "Create MC simulations of weights to assess mean-variance of a portfolio",
    "text": "Create MC simulations of weights to assess mean-variance of a portfolio\nLet’s get 5 different financial assets: AA, LLY, AMD, SBUX, FDX. Although they are from different industries, it is not a very diverse bunch as they are all from US big companies.\nWe already have the assets downloaded and we’ll use their closing prices.\n\nlibrary(readr)      # read_csv()\nlibrary(dplyr)      # select(), arrange(), filter(), mutate(), summarize()\nlibrary(purrr)      # map()\nlibrary(tidyr)      # drop_na(), pivot_wider(), unnest()\nlibrary(glue)       # glue()\nlibrary(lubridate)\n\n# read adjusted closing prices and compute annualized daily returns and sd\nread_prices <- function(ticker) { \n  df <- read_csv(glue('../../raw_data/', {ticker}, '.csv')) |> \n    arrange(date) |> \n    select(date, adj_close = adjClose) |> \n    filter(date > '2018-01-01') |> \n    mutate(ret1d = (adj_close / lag(adj_close, 1)) - 1) |> \n    summarize(mean_ret = mean(ret1d, na.rm = T) * 252 * 100, \n              std_ret = sd(ret1d, na.rm = T) * sqrt(252) * 100)\n}\n\nassets <- c('AA', 'LLY', 'AMD', 'SBUX', 'FDX')\ndf <- tibble(ticker = assets, \n             prices = map(ticker, read_prices)) |> \n  unnest()\n\n\n\n\n\nTable 1:  Annualized Mean and Standard Deviation of daily returns \n \n  \n    Ticker \n    Mean Ret \n    Std of Ret \n  \n \n\n  \n    AA \n    15.90 \n    60.72 \n  \n  \n    LLY \n    31.79 \n    29.78 \n  \n  \n    AMD \n    55.20 \n    56.37 \n  \n  \n    SBUX \n    18.61 \n    30.44 \n  \n  \n    FDX \n    4.40 \n    37.53 \n  \n\n\n\n\n\n\nFirst, we create the df of returns:\n\nit’s a long df with only ticker, date, daily returns\none row per daily observation.\nwe drop first row with no returns\nthe returns df is a wide df with date and tickers as columns, then daily returns as row\n\n\n# functions to get daily returns of each assets\ncreate_returns_df <- function(ticker) { \n  df <- read_csv(glue('../../raw_data/', {ticker}, '.csv')) |> \n    arrange(date) |> \n    select(date, adj_close = adjClose) |> \n    filter(date > '2018-01-01') |> \n    mutate(ret1d = (adj_close / lag(adj_close, 1)) - 1) |> \n    select(date, ret1d)\n}\n\n# df of each assets and all their daily returns\ndf <- tibble(ticker = assets, \n             prices = map(ticker, create_returns_df)) |> \n  unnest(cols = c(prices)) |> \n  drop_na()\n\n\nreturns <- df |> arrange(ticker) |> \n  pivot_wider(names_from = ticker, values_from = ret1d)  \n\nhead(returns)\n\n# A tibble: 6 × 6\n  date             AA     AMD       FDX       LLY     SBUX\n  <date>        <dbl>   <dbl>     <dbl>     <dbl>    <dbl>\n1 2018-01-03 -0.0121   0.0519  0.0125    0.00543   0.0187 \n2 2018-01-04  0.00367  0.0494  0.0156    0.00446   0.00375\n3 2018-01-05 -0.0112  -0.0198  0.00393   0.0123    0.0115 \n4 2018-01-08  0.0168   0.0337  0.0103   -0.00508  -0.00503\n5 2018-01-09 -0.0145  -0.0375 -0.00339  -0.000813 -0.00219\n6 2018-01-10  0.0363   0.0118  0.000971  0.000465  0.0108 \n\n\nTo optimize the mean-variance of the portfolio, we consider the following\nWeights of each assets are \\(w = \\pmatrix{w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n}\\).\nMean returns of each assets are \\(\\mu = \\pmatrix{\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n}\\)\nThen, the expected portfolio return is \\(\\mu_{\\pi} = w^{T} \\cdot \\mu\\) where \\(w^{T}\\) is the transpose of \\(w\\) (aka transforming \\(w\\) from a column vector to a row vector in order to have right dimensions to compute the dot product).\nAnd the expected portfolio variance is computed by \\[\\sigma_{\\pi}^2 = w^T \\cdot \\Sigma \\cdot w\\] where \\(\\Sigma\\) is the covariance matrix. Also, don’t forget to square root the variance when using sd: \\(\\sigma_{\\pi} = \\sqrt{w^T \\cdot \\Sigma \\cdot w}\\)\nTo put this into code, we first create the matrix of returns,then create the covariance matrix. Using both matrices, we create a function that return the portfolio mean and variance using the randomly chosen weights.\n\n# this df to provide vectors of expected returns\ndf_stat <- df |> \n  group_by(ticker) |> \n  summarize(mean_ret = mean(ret1d, na.rm = T) * 252) |> \n  ungroup() |> arrange(ticker) \n\nmu = as.matrix(df_stat$mean_ret, nrow = length(assets))\n\n# this df to provide the covariance matrix\n# note how we have also multiplied it by 252\nsigma <- df |> arrange(ticker) |> \n  pivot_wider(names_from = ticker, values_from = ret1d) |> \n  drop_na() |> \n  select(-date) |> cov() * 252\n\nsigma <- as.matrix(sigma, nrow = length(assets))\n\n# this function to create one simulation using random weights\ncreate_one_portfolio_simulation <- function(n) { \n  # pick random weights\n  weights_rand = runif(length(assets))\n  weights = matrix(weights_rand / sum(weights_rand), nrow = length(assets))\n  \n  #these are textbook formula for return and volat of a portfolio\n  return_pi = as.numeric(t(weights) %*% mu)\n  volat_pi = sqrt(as.numeric((t(weights) %*% sigma) %*% weights))\n  sharpe_ratio = return_pi / volat_pi\n  \n  # wrap everything into a df for later checks / analysis\n  df <- tibble(portf_ret = round(return_pi * 100, 4), portf_volat = round(volat_pi * 100, 4),  \n               weights = round(t(weights) * 100, 4), sharpe_ratio = sharpe_ratio)\n\n  return(df)\n}\n\n#this is really the only inputs to get \nnum_portfolio = 7000\n\nmc_simu = tibble(id = 1:num_portfolio) |> \n  mutate(simul = map(id, create_one_portfolio_simulation)) |> \n  unnest(cols=c(simul)) |> \n  arrange(desc(sharpe_ratio))\n\nhead(mc_simu)\n\n# A tibble: 6 × 5\n     id portf_ret portf_volat weights[,1]  [,2]  [,3]  [,4]   [,5] sharpe_ratio\n  <int>     <dbl>       <dbl>       <dbl> <dbl> <dbl> <dbl>  <dbl>        <dbl>\n1  1844      37.3        29.4        2.76  30.3  3.96  62.3  0.616         1.27\n2  5813      40.0        31.7        1.60  39.6  2.21  55.0  1.53          1.26\n3  3840      32.6        26.2        1.85  19.3  3.63  57.2 18.1           1.24\n4  1551      35.9        28.9        4.64  29.5  5.75  56.6  3.46          1.24\n5     5      37.0        29.9        3.26  34.0  5.34  51.5  5.90          1.24\n6  5384      37.9        30.7        3.28  36.2  6.39  53.5  0.722         1.24\n\n\n\nVizualisation of mean-variances points\nWe have highlithed the portfolio with best mean-variance returns with a red square around its dot.\n\nlibrary(ggplot2)\n\nggplot(mc_simu, aes(x = portf_volat, y = portf_ret)) + \n  geom_point(aes(colour = sharpe_ratio)) + \n  scale_color_distiller(palette=\"Set1\") + \n  geom_point(data = mc_simu[1,], aes(x = portf_volat, y = portf_ret), \n             color = 'red', size = 6, shape=5) + \n  xlab('Portfolio Volatility') + \n  ylab('Portfolio Returns') + \n  labs(title = 'MC simulation for 5 stocks', color = 'Sharpe \\n Ratio')"
  },
  {
    "objectID": "posts/lin-algebra-quant/index.html",
    "href": "posts/lin-algebra-quant/index.html",
    "title": "Linear Algebra for Quantitative Finance",
    "section": "",
    "text": "On this post, I am just sharing some linear algebra tools and tricks useful in quantitative finance. This is mainly a post for myself to have a place where I can remember them for when I need them.\n\nFinding the inverse of a 3x3 matrix\n\nFind the transpose of the cofactor matrix.\n\nfor each element of the matrix, find its minor (cross the row \\(i\\) and column \\(j\\) for element \\(ij\\), and find the determinant of the square matrix left)\nalternate the signs (diagonals are positive) \\(\\begin{pmatrix} + & - & + \\\\ - & + & - \\\\ + & - & + \\\\ \\end{pmatrix}\\)\n\nFind the determinant of the 3x3 matrix\n\n\\(a_{1,1} \\cdot (\\mbox{ cofactor of } a_{1,1}) - a_{1,2} \\cdot (\\mbox{ cofactor of } a_{1,2}) + a_{1,3} \\cdot (\\mbox{ cofactor of } a_{1,3})\\)\n\n\n\n\nGoing from Correlation to Covariance matrix\nHow to go from the correlation matrix and standard deviation vector to the covariance matrix?\nThe standard deviation vector is defined as \\(\\sigma = \\pmatrix{\\sigma_1 \\\\ \\sigma_2 \\\\ \\vdots \\\\ \\sigma_n}\\)\nThe correlation matrix is defined as \\[ R = \\begin{pmatrix} 1 & \\rho_{12} & \\cdots & \\rho_{1n} \\\\\n                       \\rho_{21} & 1 & \\cdots & \\rho_{2n} \\\\\n                        \\vdots & \\vdots & \\ddots \\\\\n                        \\rho_{n1} & \\rho_{n2} & \\cdots & 1\n                                    \\end{pmatrix} \\]\nwhere \\(\\rho_{ij}\\) is the correlation between returns of asset \\(i\\) and asset \\(j\\)\nwe create a diagonal matrix from the standard deviation vector.\n\\[ S = D(\\sigma) = \\begin{pmatrix} \\sigma_1 & 0 & \\cdots & 0 \\\\\n                                    0 & \\sigma_2 & \\cdots & 0 \\\\\n                                    \\vdots & \\vdots & \\ddots \\\\\n                                    0 & 0 & \\cdots & \\sigma_n\n                                    \\end{pmatrix} \\] (aka all other entries being 0)\nIn R, we use the function diag(x) with x being a vector! Note that S is symmetric, and so \\(S = S^T\\)\nTo get the covariance matrix \\(\\Sigma\\), we’ll pre & post-multiply the correlation matrix by the diagonal of standard deviation. Hence: \\[ S \\cdot R \\cdot S = \\Sigma = \\begin{pmatrix}\n\\sigma_1^2 & \\rho_{12} \\sigma_1 \\sigma2 & \\cdots & \\rho_{1n} \\sigma_1 \\sigma_n  \\\\\n\\rho_{21} \\sigma_2 \\sigma1 & \\sigma2^2 & \\cdots & \\rho_{2n} \\sigma_2 \\sigma_n \\\\\n\\vdots & \\vdots & \\ddots \\\\\n\\rho_{n1} \\sigma_n \\sigma_1 & \\rho_{n2} \\sigma_n \\sigma_2 & \\cdots & \\sigma_n^2\n                                    \\end{pmatrix} \\]"
  },
  {
    "objectID": "posts/linear_regression/index.html",
    "href": "posts/linear_regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is probably the most basic example of a machine learning algorithms."
  },
  {
    "objectID": "posts/linear_regression/index.html#finding-the-coefficients-from-scratch",
    "href": "posts/linear_regression/index.html#finding-the-coefficients-from-scratch",
    "title": "Linear Regression",
    "section": "Finding the coefficients from scratch",
    "text": "Finding the coefficients from scratch\nIn the case of simple linear regression, we just have one independent variable and one dependent variable. Let’s say we have \\(n\\) observations \\((x_i, y_i)\\) and we want to find a linear equations that predict y \\(\\hat{y_i}\\) based on a given \\(x_i\\).\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\]\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters of our model that have to be found.\n\n\\(\\beta_0\\) is the intercept (value of y when x=0)\n\\(\\beta_1\\) is the slope of our linear model\n\n\\(\\epsilon_i\\) is the residual or error term of the \\(i^{th}\\) observations\n\n\\(\\hat{y_i}\\) is the estimated or predicted value of y. In that sense \\(\\hat{y_i} = \\beta_0 + \\beta_1 x_i\\). The error term is then the difference between the actual y and the predicted y: \\(\\epsilon_i = y - \\hat{y_i}\\)\n\nThe cost function (or loss function) is to minimize the sum of squared error. In that sense, we seek to minimize \\[SSE = \\sum_{i=1}^{n} \\epsilon_i^2 = \\sum_{i=1}^{n} (y_i-\\beta_0 - \\beta_1 x_i)^2 \\tag{1}\\]\n\n\n\nTrying to minimize the sum of the squared of the vertical bars\n\n\nTo find the SSE, we will need to use partial derivatives for both coefficients and solve it for 0.\nLet’s first focus on \\(\\beta_0\\)\n\\[\\frac{\\partial SSE}{\\partial \\beta_0} = -2 \\sum_{i=1}^{n} (y_i-\\beta_0 - \\beta_1 x_i) = 0\\] Breaking down our sum: \\[\\sum_{i=1}^{n} y_i - \\sum_{i=1}^{n} \\beta_0 - \\sum_{i=1}^{n} \\beta_1 x_i = 0\\] \\(\\beta0\\) and \\(\\beta_1\\) are coefficient, hence: \\[\\sum_{i=1}^{n} y_i - n \\beta_0 - \\beta_1 \\sum_{i=1}^{n} x_i = 0\\] and \\[\\beta_0 = \\frac{\\sum_{i=1}^{n} y_i - \\beta_1 \\sum_{i=1}^{n} x_i}{n}\\] \\(\\bar{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}\\) (mean of y values) and \\(\\bar{x}=\\frac{\\sum_{i=1}^{n} x_i}{n}\\) (mean of x values).\nand our previous equation can then be simplified as \\[\\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\tag{2}\\]\nLet’s now address the second partial derivative wrt \\(\\beta_1\\).\n\\[\\frac{\\partial SSE}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i (y_i-\\beta_0 - \\beta_1 x_i) = 0\\].\nDistributing the \\(x_i\\) and substituting in the value of \\(\\beta_0\\) from Equation 2\n\\[\\sum_{i=1}^{n} (x_i y_i - x_i (\\bar{y} - \\beta_1 \\bar{x}) - \\beta_1 x_i^2) = 0\\].\nFactoring \\(\\beta_1\\) and breaking down the sum and being careful to the sign, we get: \\[\\beta_1 = \\frac{\\sum_{i=1}^{n} x_i (y_i - \\bar{y})}{\\sum_{i=1}^{n} x_i (x_i - \\bar{x})}\\]\nKnowing \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\) and \\(\\bar{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}\\), we can get one step further (have a common denominator with over n). I have also removed the index on the sum for readability.\n\\[\\beta_1 = \\frac{n \\Sigma x_i y_i - \\Sigma x_i \\Sigma y_i}{n \\Sigma x_i^2 - (\\Sigma x_i)^2} \\tag{3}\\]\nWe can now use the values of \\(\\beta_0\\) Equation 2 and \\(\\beta_1\\) Equation 3 into our estimate of y: \\(\\hat{y_i} = \\beta_0 + \\beta_1 x_i\\)"
  },
  {
    "objectID": "posts/linear_regression/index.html#consideration-when-doing-linear-regressions",
    "href": "posts/linear_regression/index.html#consideration-when-doing-linear-regressions",
    "title": "Linear Regression",
    "section": "Consideration when doing linear regressions",
    "text": "Consideration when doing linear regressions\n\nStart with a scatter plot to check if data have a linear trend. No points of doing a linear regression on a set of data, if data are not showing a linear trend.\n\nHow well the data fits the regression line (correlation) have NO incidence on causality. Correlation is no indication of causation\nVariables have to be normally distributed. This can be checked using histrogram or Q-Q plot or some other stat tests - Shapiro-Wilk test, Kolmogorov–Smirnov test. Skewness and kurtosis can also be used for that.\nHomoscedasticity in the residuals. Variance in the spread of residuals should be constant."
  },
  {
    "objectID": "posts/linear_regression/index.html#considerations-when-doing-linear-regression",
    "href": "posts/linear_regression/index.html#considerations-when-doing-linear-regression",
    "title": "Linear Regression",
    "section": "Considerations when doing linear regression",
    "text": "Considerations when doing linear regression\n\nStart with a scatter plot to check if data have a linear trend. No points of doing a linear regression on a set of data, if data are not showing a linear trend.\n\nHow well the data fits the regression line (correlation) have NO incidence on causality. Correlation is no indication of causation\nVariables have to be normally distributed. This can be checked using histrogram or Q-Q plot or some other stat tests - Shapiro-Wilk test, Kolmogorov–Smirnov test. Skewness and kurtosis can also be used for that. In case of violation of this assumption, a Box-Cox transformation could be used.\nHomoscedasticity in the residuals. Variance in the spread of residuals should be constant. \nError terms are normally distributed (visual: histogram, QQ-plot)\nIn the case of multi-variables linear regression, ensure no correlation between independent variables"
  },
  {
    "objectID": "posts/random-behavior-assets/index.html",
    "href": "posts/random-behavior-assets/index.html",
    "title": "Random Behavior of Financial Assets",
    "section": "",
    "text": "One of the main pillar of quantitative finance is the assumption that assets’ returns behave in a random manner. Assets returns are normally distributed. It is a poor assumption as asset’s return are usually not normally distributed (fat tails, skewness, etc.), but it is one that is considered when approaching finance with a quantitative finance."
  },
  {
    "objectID": "posts/binomials_models/index.html",
    "href": "posts/binomials_models/index.html",
    "title": "Binomials models for Quantitative Finance",
    "section": "",
    "text": "The idea is to develop an intuition for delta hedging and risk-neutrality when valuing an option.\n\n\n\nStock and Option Value perspectives\n\n\n\nS is the stock price at the start (time \\(t=0\\))\n\\(\\delta t\\) is a one increment of time (one unit of time)\n\\(u\\) is the factor when stock price rise\n\\(v\\) is the factor when stock price fall\n\n\\(0<v<1<u\\)\n\n\\(V\\) is the option value at time \\(t=0\\)\n\\(V^+\\) is the option value at expiration when stock is ITM\n\\(V^-\\) is the option value at expiration when stock is OTM\n\nNow we are going to introduce \\(\\Delta\\) as the amount of stock to hedge (a percentage of a stock) to be risk-neutral. At this stage, we are assuming that the probability to go up or down is the same (it’s basically irrelevant in this case).\n\n\n\nOption value with hedging\n\n\nIf we want to hedge the stock to be risk-neutral, then at expiration we should have this equation\n\\[V^+ - \\Delta us = V^--\\Delta vs\\] Solving for \\(\\Delta\\), we get:\n\\[\\Delta = \\frac{V^+-V^-}{(u-v)S} = \\frac{\\text{range of options payoff}}{\\text{range of asset prices}} \\tag{1}\\]\nIn other words we could see \\(\\Delta\\) as the rate of change of the option price in function of the stock price. \\(\\Delta = \\frac{\\partial{V}}{\\partial{S}}\\)\nOnce we found \\(\\Delta\\), we could find \\(V\\) by just making today’s value of the trade = tomorrow’s value of the trade (at expiration). Just solve for \\(V\\) \\[V - \\Delta S = V^- - \\Delta vS\\] or \\[V - \\Delta S = V^+ - \\Delta uS\\] which ever is easier to calculate.\nNow, of course, cash is not free and there is a time value associated to it. In that sense, today’s value for the trade should be equal a discounted value of tomorrow’s trade value (at expiration).\n\\[V - \\Delta S = \\frac{1}{1+r \\delta t} \\left(V^- - \\Delta vS \\right)\\]\n\n\\(r\\) is the value of the risk-free asset\nwe are dealing with annualized values, if assets expires in one month and risk-free asset is let’s say 3%, we would multiply 3% by 21 days or \\(0.03 \\cdot \\frac{21}{252}\\)\n\nUsing our value of \\(\\Delta\\) from Equation 1, we can isolate \\(V\\) as \\[V = \\left(\\frac{V^+-V^-}{u-v} \\right) + \\frac{1}{1+r \\delta t} \\left(V^- - \\Delta vS \\right) \\tag{2}\\]\n\n\n\n\n\n\nExample\n\n\n\nA stock is trading at $100. A call option with strike price of $100. Stock can either go to $103 or $98.\n\n\n\n\nflowchart LR \n  100 --> 103\n  100 --> 98\n\n\n\n\n\n\n\n\n\n\\(V^+ = 3\\)\n\\(V^- = 0\\)\n\\(\\Delta = \\frac{3-0}{103-98} = \\frac{3}{5}\\)\n\\(V - \\Delta S = V^+ - \\Delta us\\), plugging the value from above we get \\(V = \\$1.2\\)\n\n\n\n\n\nUsing the same idea as earlier and introducing some probabilities.\n\n\n\n\nflowchart LR\n  S -- p' --> uS\n  S -- 1-p' --> vS\n\n\n\n\n\n\n\n\nFrom a probabilistic perspective we could write: \\[S = p'uS + (1-p')vS\\] Or in the presence of a risk free asset, \\[S = \\frac{1}{1+r \\delta t} \\left(p' uS + (1-p')vS \\right)\\]\nWe could isolate \\(p'\\) in this last equation to get: \\[p' = \\frac{1+r \\delta t - v}{u-v} \\tag{3}\\]\n\n\n\n\nflowchart LR\n  V -- p' --> V+\n  V -- 1-p' --> V-\n\n\n\n\n\n\n\n\nTo find \\(V\\): \\[V = p' V^+ + (1-p') V^-\\]\nInteresting to note that the option price \\(V\\) is like an expectation (the sum of the probability) and \\(p'\\) is from Equation 3\n\n\n\nNow if we collide both world: the real-world with drift and volatility and the risk-free world with \\(p'\\): we can set up this 2 equations: One for the expected mean rate of change of prices and another for the variance of these rate of change.\n$$\n{\n\\[\\begin{aligned}\n  \\mu S \\delta t = puS+(1-p)vS-S \\\\\n  \\sigma^2S^2dt = S^2()^2\n\\end{aligned}\\]\n.\n$$\n(TODO re-write these 2 equations)\n2 equations and 3 unknowns, we can choose a solution (the prettiest one!) among the infinitely many:\n\n\\(u = 1 + \\sigma \\sqrt{\\delta t}\\)\n\\(v = 1 - \\sigma \\sqrt{\\delta t}\\)\n\\(p = \\frac{1}{2} + \\frac{\\mu \\sqrt{\\delta t}}{2 \\sigma}\\)\n\\(p' = \\frac{1}{2} + \\frac{r \\sqrt{\\delta t}}{2 \\sigma}\\) \\(p'\\) is the risk-neutral probability.\n\n\n\n\n\nAfter one time step \\(\\delta t\\) our stock, initially at \\(S\\) will either be at \\(uS\\) or \\(vS\\).\n\nAfter two time steps, the stock will either be at \\(u^2S\\) or \\(uvS\\) or \\(v^2S\\)\nAfter three time steps, the stock will either be at \\(u^3S\\) or \\(u^2vS\\) or \\(uv^2S\\) or finally \\(v^3S\\)\netc.\n\nA bit of a crude representation using Geogebra\n\n\n\nBinomial tree representation\n\n\nUsing the equations from the previous sections, we can now create a function to price the value of an option.\n\ncalculate_option_price <- function(spot, strike, rfr, sigma, time, steps) {\n  dt = time/steps            # get delta_t\n  u = 1 + sigma * sqrt(dt)   # multiplying factor when asset rise\n  v = 1 - sigma * sqrt(dt)\n  p_prime = 0.5 + (rfr * sqrt(dt)/(2*sigma))\n  discount_factor = 1 / (1 + rfr * dt)\n  \n  # Calculating vector of prices at maturity \n  s = rep(0, steps+1)   # initialize a vector for prices at maturity - (terminal nodes on the trees)\n  s[1] = spot * v^steps # #initialize the first end-price - (the most bottom right node of the tree)\n  for (i in 2:(steps+1)) { \n    s[i] = s[i-1] * u/v # this is the trick: to go up one leave = going back one step and then up\n  }\n  \n  # Calculating vector of options values at maturity \n  opt = rep(0, steps + 1)\n  for (i in 1:(steps+1)) { \n    opt[i] = max(0, s[i] - strike)\n  }\n  \n  # We have now to work backward and up in the trees (from bottom right and up)\n  # We know the final option value.\n  # We need know to calculate the options in the intermediates nodes\n  for (i in steps:1) {\n    for (j in 1:i) {\n      opt[j] = discount_factor * (p_prime * opt[j+1] + (1 - p_prime)*opt[j])\n    }\n  }\n  #print(glue::glue('Asset price for each time step:', s))\n  #print(glue::glue('Option price for each time step: ', opt))\n  return(opt)\n}\n\nLet’s try our function to get some results\n\ncalculate_option_price(spot = 100, strike = 100, rfr = 0.03, sigma = 0.1, time = 1, steps = 4)\n\n[1]  5.415051  8.117623 11.822154 16.506917 21.550625"
  },
  {
    "objectID": "posts/binomials_models/index.html#forward-equation",
    "href": "posts/binomials_models/index.html#forward-equation",
    "title": "Binomials models for Quantitative Finance",
    "section": "Forward equation",
    "text": "Forward equation\n\\[Prob(a<y'<b \\text{ at time t' } | \\text{ y at time t}) = \\int_a^b p(y, t; y', t') dy' \\tag{4}\\]\nThis (Equation 4) means: What is the probability that the random variable y’ lies between a and b at time t’ given it was at y at time t? In this case (y, t) are given, they are constant, they are known; while (y’, t’) are the variables.\nWe re-write this (Equation 4) for conciseness as \\(P(y, t; y', t')\\).\nHence, another way to write (Equation 4) is \\[P(y, t; y', t') = \\alpha \\cdot P(y, t; y'+\\delta y, t'-\\delta t) + (1-2\\alpha) P(y, t; y', t'-\\delta t) + \\alpha \\cdot P(y, t; y'-\\delta y, t' - \\delta t) \\tag{5}\\]\nEach terms in the sum of (Equation 5) could be evaluated using a Taylor Series Expansion. Note that \\(\\delta t^2 << \\delta t\\) as \\(\\delta t\\) is already quite small.\n\\[P(y, t; y' + \\delta y, t'-\\delta t)  \\approx P(y,t;y',t') + \\delta y \\frac{\\partial P}{\\partial y'} - \\delta t \\frac{\\partial P}{\\partial t'} + \\frac{1}{2} \\delta y^2 \\frac{\\partial^2 P}{\\partial y'^2} + \\dots\\] \\[P(y, t; y', t'-\\delta t)  \\approx P(y,t;y',t') - \\delta t \\frac{\\partial P}{\\partial t'} + \\dots\\]\n\\[P(y, t; y'- \\delta y, t'-\\delta t)  \\approx P(y,t;y',t') - \\delta y \\frac{\\partial P}{\\partial y'} - \\delta t \\frac{\\partial P}{\\partial t'} + \\frac{1}{2} \\delta y^2 \\frac{\\partial^2 P}{\\partial y'^2} + \\dots\\]\nWe have ignored all the less than \\(\\delta t\\).\nAdding the 3 equations above with their coefficients, we end up with\n\\[\\delta t \\frac{\\partial P}{\\partial t'} = \\alpha \\delta y^2 \\frac{\\partial^2 P}{\\partial y'^2}\\] \\[\\frac{\\partial P}{\\partial t'} = \\alpha \\frac{\\delta y^2}{\\delta t} \\frac{\\partial^2 P}{\\partial y'^2}\\]\nNote how \\(\\alpha\\), \\(\\delta t\\) and \\(\\delta y\\) are all positive values. Hence, we can let \\(C^2 = \\alpha \\frac{\\delta y^2}{\\delta t}\\), and we get: \\[\\frac{\\partial P}{\\partial t'} = C^2 \\frac{\\partial^2 P}{\\partial y'^2} \\tag{6}\\]\nThis last (Equation 6) can be recognized as the Forward Kolmogorov Equation or Heat-diffusion equation or also Fokker-Plank equation.\nNote that:\n\nThis is a PDE for p with 2 independent variables \\(y'\\) and \\(t'\\)\n\\(y\\) and \\(t\\) are like parameters. They are fixed, they are starting point\nThis should model a random-walk that is finite in a finite time.\n\nTo solve this PDE, we solve it by (as per the CQF) similarity reduction. We use a solution of the form \\[P = t'^a f \\left( \\frac{y'}{t'^b} \\right) \\space a, b \\in \\mathbb{R}\\]\nLetting \\(\\xi = \\frac{y'}{t'^b}\\), we are looking for a solution of the form \\[P = t'^a f(\\xi) \\]\nFinding the partial derivatives based on the above solution’s form.\n\\[\\frac{\\partial P}{\\partial y'} = t'^a \\cdot \\frac{df}{d \\xi} \\cdot \\frac{\\partial \\xi}{\\partial y'}\\] Note how f is just a function of \\(\\xi\\) while \\(\\xi\\) is a function of both \\(y'\\) and \\(t'\\); hence the difference in notation for the derivatives.\nSince \\(\\frac{\\partial \\xi}{\\partial y'} = t'^{-b}\\), we have \\[\\frac{\\partial P}{\\partial y'} = t'^{a-b} \\cdot \\frac{df}{d \\xi} \\]\nAlso, \\(\\frac{\\partial \\xi}{\\partial t'} = -b \\cdot y' \\cdot t'^{-b-1}\\). Using product rule to find \\(\\frac{\\partial P}{\\partial t'}\\), we get: \\[\\frac{\\partial P}{\\partial t'} = a t'^{a-1} f(\\xi) + t'^a \\frac{df}{d \\xi} \\frac{\\partial \\xi}{\\partial t'} = a \\cdot t'^{a-1} \\cdot f(\\xi) - b \\cdot t'^{a-b-1} \\cdot y' \\cdot \\frac{df}{d \\xi} \\]"
  },
  {
    "objectID": "posts/random-behavior-assets/index.html#ignoring-randomness",
    "href": "posts/random-behavior-assets/index.html#ignoring-randomness",
    "title": "Random Behavior of Financial Assets",
    "section": "Ignoring randomness",
    "text": "Ignoring randomness\n\\[R_i = \\frac{S_{i+1} - S_i}{S_i} = mean  = \\mu \\delta t\\] \\[S_{i+1} - S_i= S_i \\mu \\delta t\\] \\[S_{i+1} = S_i \\cdot (1 +  \\mu \\delta t) \\tag{3}\\]\nWe could also rewrite Equation 3 so it depends of the initial (starting) price, instead of the previous price.\n\\[S_n = S_0 (1+\\mu \\delta t)^n\\]\nUsing natural log:\n\\(S_n = S_0 e^{log (1+\\mu \\delta t)^n} = S_0 e^{n \\cdot log{(1+\\mu \\delta t)}}\\)\nWe could argue that \\(log(1+\\mu \\delta t) \\approx \\mu \\delta t\\) as \\(log(1+x) \\approx x\\) for small values of x.\n\\[S_n \\approx S_0 \\cdot e^{n \\mu \\delta t} \\tag{4}\\]\nNow, \\(n \\cdot \\delta t\\) is the same as \\(t\\). Hence,\n\n\n\n\n\n\n\\[S(t) \\approx S_0 \\cdot e^{\\mu t}\\]"
  },
  {
    "objectID": "posts/random-behavior-assets/index.html#considering-randomness",
    "href": "posts/random-behavior-assets/index.html#considering-randomness",
    "title": "Random Behavior of Financial Assets",
    "section": "Considering randomness",
    "text": "Considering randomness\nLet’s restart with Equation 2\n\\[R_i = \\frac{S_{i+1} - S_i}{S_i} = \\bar{R} + std \\cdot \\phi = \\mu \\delta t + \\sigma \\phi \\delta t^{1/2}\\] \\[S_{i+1} - S_i= S_i \\mu \\delta t + S_i \\sigma \\phi \\delta t^{1/2} \\tag{5}\\]\n\n\n\n\n\n\n\\[S_{i+1} = S_i \\cdot (1 +  \\mu \\delta t +  \\sigma \\phi \\sqrt{\\delta t}) \\tag{6}\\]\nThis last Equation 6 is the basis for Monte-Carlo simulation.\n\nNotice the standard deviation of return: \\(\\sigma \\sqrt{\\delta t}\\)\nunit of \\(\\mu = \\frac{1}{t}\\)\nunit of \\(\\sigma = \\frac{1}{\\sqrt{t}}\\)\nthis is because we can only add variance together (no sd). For independent variable X and Y: \\(Var(X+Y) = Var(X) + Var(Y)\\)\nthe standard deviation of returns scale up with the square root of the time step."
  },
  {
    "objectID": "posts/random-behavior-assets/index.html#going-to-continuous-time",
    "href": "posts/random-behavior-assets/index.html#going-to-continuous-time",
    "title": "Random Behavior of Financial Assets",
    "section": "Going to continuous time",
    "text": "Going to continuous time\nRestarting from Equation 5 : \\[S_{i+1} - S_i= S_i \\mu \\delta t + S_i \\sigma \\sqrt{\\delta t} \\phi\\]\n\n\\(S_{i+1} - S_i = dS\\)\n\\(S_i = S(t)\\)\n\\(\\delta t = dt\\)\n\\(\\phi \\sqrt{\\delta t} = dX\\) where \\(dX\\) is a random variable with mean = 0 and variance = dt. Hence \\(E[dX] = 0\\) and \\(E[(dX)^2] = dt\\)\n\n\n\n\n\n\n\n\\[dS = S \\mu dt + S \\sigma dX\\] This stochastic differential equation on the change of prices assume:\n\nreturns are treated as random\nreturns are assumed to be normally distributed (again not totally exact)\nprices (S) are modelled as a log-normal walk (SDE)\n\\(\\mu\\) is the drift rate or growth rate\nbecause of the different scaling of time (\\(t\\) and \\(\\sqrt{t}\\)), on a short time frame, drift is negligible and volatility matters."
  },
  {
    "objectID": "posts/normality-returns/Untitled.html",
    "href": "posts/normality-returns/Untitled.html",
    "title": "Normality of asset returns",
    "section": "",
    "text": "As mentioned in one of our previous posts, we know that in quantitative finance, assets returns are assumed to be random. That being said they are not totally normally distributed. This post is about"
  },
  {
    "objectID": "posts/random-behavior-assets/blog01.html",
    "href": "posts/random-behavior-assets/blog01.html",
    "title": "Random Behavior of Financial Assets",
    "section": "",
    "text": "One of the main pillar of quantitative finance is the assumption that assets’ returns behave in a random manner. Assets returns are normally distributed. It is a poor assumption as asset’s return are usually not normally distributed (fat tails, skewness, etc.), but it is one that is considered when approaching finance with a quantitative finance."
  },
  {
    "objectID": "posts/random-behavior-assets/blog01.html#ignoring-randomness",
    "href": "posts/random-behavior-assets/blog01.html#ignoring-randomness",
    "title": "Random Behavior of Financial Assets",
    "section": "Ignoring randomness",
    "text": "Ignoring randomness\n\\[R_i = \\frac{S_{i+1} - S_i}{S_i} = mean  = \\mu \\delta t\\] \\[S_{i+1} - S_i= S_i \\mu \\delta t\\] \\[S_{i+1} = S_i \\cdot (1 +  \\mu \\delta t) \\tag{3}\\]\nWe could also rewrite Equation 3 so it depends of the initial (starting) price, instead of the previous price.\n\\[S_n = S_0 (1+\\mu \\delta t)^n\\]\nUsing natural log:\n\\(S_n = S_0 e^{log (1+\\mu \\delta t)^n} = S_0 e^{n \\cdot log{(1+\\mu \\delta t)}}\\)\nWe could argue that \\(log(1+\\mu \\delta t) \\approx \\mu \\delta t\\) as \\(log(1+x) \\approx x\\) for small values of x.\n\\[S_n \\approx S_0 \\cdot e^{n \\mu \\delta t} \\tag{4}\\]\nNow, \\(n \\cdot \\delta t\\) is the same as \\(t\\). Hence,\n\n\n\n\n\n\n\\[S(t) \\approx S_0 \\cdot e^{\\mu t}\\]"
  },
  {
    "objectID": "posts/random-behavior-assets/blog01.html#considering-randomness",
    "href": "posts/random-behavior-assets/blog01.html#considering-randomness",
    "title": "Random Behavior of Financial Assets",
    "section": "Considering randomness",
    "text": "Considering randomness\nLet’s restart with Equation 2\n\\[R_i = \\frac{S_{i+1} - S_i}{S_i} = \\bar{R} + std \\cdot \\phi = \\mu \\delta t + \\sigma \\phi \\delta t^{1/2}\\] \\[S_{i+1} - S_i= S_i \\mu \\delta t + S_i \\sigma \\phi \\delta t^{1/2} \\tag{5}\\]\n\n\n\n\n\n\n\\[S_{i+1} = S_i \\cdot (1 +  \\mu \\delta t) + S_i \\sigma \\phi \\delta t^{1/2} \\tag{6}\\]\nThis last Equation 6 is the basis for Monte-Carlo simulation.\n\nNotice the standard deviation of return: \\(\\sigma \\sqrt{\\delta t}\\)\nunit of \\(\\mu = \\frac{1}{t}\\)\nunit of \\(\\sigma = \\frac{1}{\\sqrt{t}}\\)\nthis is because we can only variance together (no sd). For independent variable X and Y: \\(Var(X+Y) = Var(X) + Var(Y)\\)\nthe standard deviation of returns scale up with the square root of the time step."
  },
  {
    "objectID": "posts/random-behavior-assets/blog01.html#going-to-continuous-time",
    "href": "posts/random-behavior-assets/blog01.html#going-to-continuous-time",
    "title": "Random Behavior of Financial Assets",
    "section": "Going to continuous time",
    "text": "Going to continuous time\nRestarting from Equation 5 : \\[S_{i+1} - S_i= S_i \\mu \\delta t + S_i \\sigma \\sqrt{\\delta t} \\phi\\]\n\n\\(S_{i+1} - S_i = dS\\)\n\\(S_i = S(t)\\)\n\\(\\delta t = dt\\)\n\\(\\phi \\sqrt{\\delta t} = dX\\) where \\(dX\\) is a random variable with mean = 0 and variance = dt. Hence \\(E[dX] = 0\\) and \\(E[(dX)^2] = dt\\)\n\n\n\n\n\n\n\n\\[dS = S \\mu dt + S \\sigma dX\\] This stochastic differential equation on the change of prices assume:\n\nreturns are treated as random\nreturns are assumed to be normally distributed (again not totally exact)\nprices (S) are modelled as a log-normal walk (SDE)\n\\(\\mu\\) is the drift rate or growth rate\nbecause of the different scaling of time (\\(t\\) and \\(\\sqrt{t}\\)), on a short time frame, drift is negligible and volatility matters."
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is probably the most basic example of a machine learning algorithms."
  },
  {
    "objectID": "posts/linear-regression/index.html#finding-the-coefficients-from-scratch",
    "href": "posts/linear-regression/index.html#finding-the-coefficients-from-scratch",
    "title": "Linear Regression",
    "section": "Finding the coefficients from scratch",
    "text": "Finding the coefficients from scratch\nIn the case of simple linear regression, we just have one independent variable and one dependent variable. Let’s say we have \\(n\\) observations \\((x_i, y_i)\\) and we want to find a linear equations that predict y \\(\\hat{y_i}\\) based on a given \\(x_i\\).\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\]\n\n\\(x_i\\) is the independent variable (aka: predictor, explanatory variable)\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters of our model that have to be found.\n\n\\(\\beta_0\\) is the intercept (value of y when x=0)\n\\(\\beta_1\\) is the slope of our linear model\n\n\\(\\epsilon_i\\) is the residual or error term of the \\(i^{th}\\) observations\n\nFrom a probabilistic perspective, \\(\\epsilon\\) can be seen as a random variable with the following properties: \\(E(\\epsilon)=0\\) and \\(Var(\\epsilon)= \\sigma_{\\epsilon}^2 = \\sigma^2\\)\n\n\\(\\hat{y_i}\\) is the estimated or predicted value of y. In that sense \\(\\hat{y_i} = \\beta_0 + \\beta_1 x_i\\). The error term is then the difference between the actual y and the predicted y: \\(\\epsilon_i = y_i - \\hat{y_i}\\)\n\nThe cost function (or loss function) is to minimize the sum of squared error. In that sense, we seek to minimize \\[ \\text{min } SSE = min \\sum_{i=1}^{n} \\epsilon_i^2 =\n        \\underset{\\beta_0, \\beta1}{argmin} \\sum_{i=1}^{n} (y_i-\\beta_0 - \\beta_1 x_i)^2 \\tag{1}\\]\n\n\n\nTrying to minimize the sum of the squared of the vertical bars\n\n\nTo minimize the SSE, we will need to use partial derivatives for both coefficients and solve it for 0.\nLet’s first focus on \\(\\beta_0\\)\n\\[\\frac{\\partial SSE}{\\partial \\beta_0} = -2 \\sum_{i=1}^{n} (y_i-\\beta_0 - \\beta_1 x_i) = 0\\] Breaking down our sum: \\[\\sum_{i=1}^{n} y_i - \\sum_{i=1}^{n} \\beta_0 - \\sum_{i=1}^{n} \\beta_1 x_i = 0\\] \\(\\beta0\\) and \\(\\beta_1\\) are coefficient, hence: \\[\\sum_{i=1}^{n} y_i - n \\beta_0 - \\beta_1 \\sum_{i=1}^{n} x_i = 0\\] and \\[\\beta_0 = \\frac{\\sum_{i=1}^{n} y_i - \\beta_1 \\sum_{i=1}^{n} x_i}{n}\\] \\(\\bar{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}\\) (mean of y values) and \\(\\bar{x}=\\frac{\\sum_{i=1}^{n} x_i}{n}\\) (mean of x values).\nand our previous equation can then be simplified as \\[\\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\tag{2}\\]\nLet’s now address the second partial derivative wrt \\(\\beta_1\\).\n\\[\\frac{\\partial SSE}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i (y_i-\\beta_0 - \\beta_1 x_i) = 0\\].\nDistributing the \\(x_i\\) and substituting in the value of \\(\\beta_0\\) from Equation 2\n\\[\\sum_{i=1}^{n} (x_i y_i - x_i (\\bar{y} - \\beta_1 \\bar{x}) - \\beta_1 x_i^2) = 0\\].\nFactoring \\(\\beta_1\\) and breaking down the sum and being careful to the sign, we get: \\[\\beta_1 = \\frac{\\sum_{i=1}^{n} x_i (y_i - \\bar{y})}{\\sum_{i=1}^{n} x_i (x_i - \\bar{x})} \\tag{3}\\]\nKnowing \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\) and \\(\\bar{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}\\), we can get one step further (have a common denominator with over n). I have also removed the index on the sum for readability.\n\\[\\beta_1 = \\frac{n \\Sigma x_i y_i - \\Sigma x_i \\Sigma y_i}{n \\Sigma x_i^2 - (\\Sigma x_i)^2} \\tag{4}\\]\nWe can now use the values of \\(\\beta_0\\) Equation 2 and \\(\\beta_1\\) Equation 4 into our estimate of y: \\(\\hat{y_i} = \\beta_0 + \\beta_1 x_i\\)"
  },
  {
    "objectID": "posts/linear-regression/index.html#considerations-when-doing-linear-regression",
    "href": "posts/linear-regression/index.html#considerations-when-doing-linear-regression",
    "title": "Linear Regression",
    "section": "Considerations when doing linear regression",
    "text": "Considerations when doing linear regression\n\nStart with a scatter plot to check if data have a linear trend. No points of doing a linear regression on a set of data, if data are not showing a linear trend.\n\nHow well the data fits the regression line (correlation) have NO incidence on causality. Correlation is no indication of causation\nVariables have to be normally distributed. This can be checked using histogram or QQ-plot or some other stat tests - Shapiro-Wilk test, Kolmogorov–Smirnov test. Skewness and kurtosis can also be used for that. In case of violation of this assumption, a Box-Cox transformation could be used.\nHomoscedasticity in the residuals. Variance in the spread of residuals should be constant. \nError terms are normally distributed (visual: histogram, QQ-plot)\nIn the case of multi-variables linear regression, ensure no correlation between independent variables"
  },
  {
    "objectID": "posts/normality-returns/index.html",
    "href": "posts/normality-returns/index.html",
    "title": "Normality of asset returns",
    "section": "",
    "text": "As mentioned in one of our previous posts, we know that in quantitative finance, assets returns are assumed to be random. That being said they are not totally normally distributed. This post is to digg in a bit further in assessing the normality (or non-normality) of equity returns.\nLet’s consider a couple of equities: an ETF (low volatility) and a stock (higher volatility).\nFrom previous post, returns can be explained with a drift rate + some randomness: \\[R_i = \\mu \\delta t + \\phi \\sigma \\delta t^{1/2}\\]\nUsing SPY as our ETF.\nLet’s make some assumptions on our drift rate and volatility. Using all the trading sessions since 01 Jan 2020.\n\ndf_spy <- read_csv('../../raw_data/SPY.csv') |> \n  select(date, adjClose) |> \n  arrange(date) |> \n  mutate(ret = log(adjClose / lag(adjClose))) |> \n  filter(date > '2020-01-01')\n\ndrift = mean(df_spy$ret) \nsigma = sd(df_spy$ret) \n\nSo over the last 3-ish years, SPY had an annualized drift rate of 9.04% with an volatility of 24.4%\nLet’s consider now an imaginary stock with a similar drift rate and standard deviation as SPY.\n\nset.seed(20042023)\nphi = rnorm(831, mean = 0, sd = 1)\ndf <- tibble(time = 1:length(phi), phi = phi, \n             return = drift + sigma * phi)\n\nprices = c(100)\nfor (i in 2:(nrow(df))) {\n  prices[i] = prices[i-1] * (1 + df$return[i])\n}\n\ndf <- add_column(df, prices)\n\nggplot(df, aes(x = time, y = prices)) + \n  geom_line()\n\n\n\n\nWe could do some normality tests on the returns.\nWhat about your histogram"
  },
  {
    "objectID": "posts/normality-returns/index.html#visual-checks-on-imaginary-stock",
    "href": "posts/normality-returns/index.html#visual-checks-on-imaginary-stock",
    "title": "Normality of asset returns",
    "section": "Visual checks on imaginary stock",
    "text": "Visual checks on imaginary stock\nUsual visual checks for normality are the histogram and the QQ-plot.\nLet’s see how well the returns stack to our imaginary stock (with close to perfect pseudo-randomness)\n\nggplot(df, aes(return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density() + \n  stat_function(fun = dnorm, n = nrow(df), args = list(mean = drift, sd = sigma), color = 'red', size = 1) +\n  scale_y_continuous() + \n  scale_x_continuous(limits = c(-0.055, 0.055), n.breaks = 9)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWhat about your histogram"
  },
  {
    "objectID": "posts/normality-returns/index.html#visual-checks-on-imaginary-stock-vs-spy",
    "href": "posts/normality-returns/index.html#visual-checks-on-imaginary-stock-vs-spy",
    "title": "Normality of asset returns",
    "section": "Visual checks on imaginary stock vs SPY",
    "text": "Visual checks on imaginary stock vs SPY\nUsual visual checks for normality are the histogram and the QQ-plot.\n\nHistograms\nLet’s see how well the returns stack to our imaginary stock (with close to perfect pseudo-randomness)\n\nggplot(df, aes(return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density() + \n  stat_function(fun = dnorm, n = nrow(df), args = list(mean = drift, sd = sigma), color = 'red', size = 1) +\n  scale_y_continuous() + \n  scale_x_continuous(limits = c(-0.055, 0.055), n.breaks = 9)\n\n\n\n\nThe black line is the actual density of returns, while the red line is the density of the normal distribution with same drift and volatility as earlier. Lines are pretty close to each other.\nAnd now onto the histogram on SPY (again same drift and volatility) as fictitious stock above.\n\nggplot(df_spy, aes(return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density() + \n  geom_vline(xintercept = drift+sigma, color = 'blue', linetype = 3, linewidth = 1) + \n  geom_vline(xintercept = drift-(0.6*sigma),  color = 'blue', linetype = 3, linewidth = 1) + \n  stat_function(fun = dnorm, n = nrow(df), args = list(mean = drift, sd = sigma), color = 'red', size = 1) +\n  scale_y_continuous() + \n  scale_x_continuous(limits = c(-0.055, 0.055), n.breaks = 9)\n\n\n\n\nAnd here, we clearly see the big disconnect from normality: above expected number of returns at the mean (aka too peaked), less returns next to the mean (between 1 and 2 or 2 1/2 sd) and then higher number of observations than expected in the tails (aka fat tails). Distribution of returns for equity are interesting in that sense: both too peaked and fat tails.\n\n\nQQ Plots\nAnother way to visually check for normality is to use a quantile-quantile plot (aka QQ-plot). On the y-axis, we have the returns, on the x-axis the theoretical quantiles.\n\nggplot(df, aes(sample = return)) + \n  stat_qq() + \n  stat_qq_line(color = 'blue', linetype = 3, linewidth = 1) + \n  labs(title = 'QQ-Plot for fictious stock returns')\n\n\n\n\nAnd now the QQ-plot for the returs of SPY.\n\nggplot(df_spy, aes(sample = return)) + \n  stat_qq() + \n  stat_qq_line(color = 'blue', linetype = 3, linewidth = 1) + \n  labs(title = 'QQ-Plot for SPY returns')\n\n\n\n\nOh boy! Again, our second plot clearly indicate how the returns deviate from normality.\nThis QQ-plot can also be used to check for asymetry in the distribution of returns. We can see a slightly left skew distribution (a negatively skew distribution)."
  },
  {
    "objectID": "posts/normality-returns/index.html#kurtosis",
    "href": "posts/normality-returns/index.html#kurtosis",
    "title": "Normality of asset returns",
    "section": "Kurtosis",
    "text": "Kurtosis\n\nmoments::kurtosis(df$return)\n\n[1] 2.879684\n\nmoments::kurtosis(df_spy$return)\n\n[1] 12.75883\n\n\nAgain, our fictious asset has kurtosis pretty close to perfect normality (almost 3). SPY deviate very much from normality and displays leptokurotic kurtosis."
  },
  {
    "objectID": "posts/stochastic-processes/index.html",
    "href": "posts/stochastic-processes/index.html",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "",
    "text": "This post is an introduction to Markov Chain with a presentation of Discrete Time Markov Chains."
  },
  {
    "objectID": "posts/stochastic-processes/index.html#definition",
    "href": "posts/stochastic-processes/index.html#definition",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Definition",
    "text": "Definition\nA stochastic process is \\(\\{ X(t), t \\in T \\}\\) is a collection of random variables indexed by a parameter t that belongs to a set T.\n\nt is generally the time\n\\(X(t)\\) is the state of the process at time t\nThe state space \\(S\\) of a stochastic process is all possible state \\(X(t)\\) for any \\(t \\in T\\)\nif T is a countable set, we call this a discrete-time process\n\nA discrete-time Markov Chain is a discrete-time stochastic process which state space S is finite such that: \\[\\mathbb{P}(X_{n+1} = j | X_0 = i_0, X_1 = i_1, X_2 = i_2, \\dots, x_n = i) = \\mathbb{P}(X_{n+1} = j | X_n = i) = P_{ij}\\]\nthat is, the conditional probability of the process being in state j at time n + 1 given all the previous states depends only on the last-known position (state i at time n)."
  },
  {
    "objectID": "posts/stochastic-processes/index.html#some-other-teminology",
    "href": "posts/stochastic-processes/index.html#some-other-teminology",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Some other teminology",
    "text": "Some other teminology\n\nA state is called absorbing if the chain cannot leave it once it enters it. An absorbing Markov chain has at least one absorbing state.\nA state is termed reflecting if once the chain leaves it, it cannot return to it.\nThe period d of a state i is the number such that, starting in i, the chain can return to i only in the number of steps that are multiples of d. A state with period d = 1 is called aperiodic. Periodicity is a class property.\n\nFor a reflecting state, the period is infinite, since the chain never comes back to this state.\nAbsorbing states necessarily have loops and thus are aperiodic states.\n\na state is called recurrent if with probability 1 the chain ever reenters that state. Otherwise, the state is called transient.\nA Markov Chain that has a unique stationary distribution (or steady-state distribution) is called an ergodic chain."
  },
  {
    "objectID": "posts/stochastic-processes/index.html#chapman-kolmogorov-equations",
    "href": "posts/stochastic-processes/index.html#chapman-kolmogorov-equations",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Chapman-Kolmogorov equations",
    "text": "Chapman-Kolmogorov equations\nWe denote the probability to go from state \\(i\\) to state \\(j\\) in n-steps by \\(\\bf{P}_{ij}^{(n)}\\). It is also denoted as the n-steps transition probability matrix. That is for any time \\(m >= 0, \\bf{P}_{ij}^n = \\mathbb{P}(X_{m+n} = j | X_m = i)\\) . \\(\\bf{P}^{(n)} = \\bf{P}^n\\) based on the Chapman-Kolmogorov equation.\nThe Chapman-Kolmogorov equation states that for all positive integers \\(m\\) and \\(n\\) , \\(\\bf{P}^{(m+n)} = \\bf{P}^m \\cdot \\bf{P}^n\\) where P is a one-step probability transition matrix (a square matrix)"
  },
  {
    "objectID": "posts/stochastic-processes/index.html#example-1",
    "href": "posts/stochastic-processes/index.html#example-1",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Example 1",
    "text": "Example 1\nTo model a Markov Chain, let’s first set up a one-step probability transition matrix (called here osptm).\nWe start with an easy 3 possible state process. That is the state space \\(S = \\{1, 2, 3\\}\\). The osptm will provide the probability to go from one state to another.\n\nosptm = matrix(c(0.7,0.1,0.2, 0,0.6,0.4, 0.5,0.2,0.3), nrow = 3, byrow = TRUE)\nosptm\n\n     [,1] [,2] [,3]\n[1,]  0.7  0.1  0.2\n[2,]  0.0  0.6  0.4\n[3,]  0.5  0.2  0.3\n\n\nWe can always have a look at how the osptm looks like.\n\n# note we have to transpose the osptm matrix first. \nosptm_transposed = t(osptm)\nosptm_transposed\n\n     [,1] [,2] [,3]\n[1,]  0.7  0.0  0.5\n[2,]  0.1  0.6  0.2\n[3,]  0.2  0.4  0.3\n\ndiagram::plotmat(osptm_transposed, pos = c(1, 2), arr.length = 0.3, \n                 box.col = \"lightblue\", box.prop = 0.5, box.size = 0.12, box.type=\"circle\", \n                 self.cex = 0.6, self.shifty=-0.01, self.shiftx = 0.15)\n\n\n\n\nThe markovchain package can provide us with all the state characteristics of a one-step probabilty transition matrix.\n\nlibrary(markovchain)\nosptm_mc <- new(\"markovchain\", transitionMatrix = osptm)\n\nrecurrentClasses(osptm_mc)\n\n[[1]]\n[1] \"1\" \"2\" \"3\"\n\ntransientClasses(osptm_mc)\n\nlist()\n\nabsorbingStates(osptm_mc)\n\ncharacter(0)\n\nperiod(osptm_mc)\n\n[1] 1\n\nround(steadyStates(osptm_mc), 4)\n\n          1      2      3\n[1,] 0.4651 0.2558 0.2791\n\n\nThe next step is to calculate, for instance, what is the probability to go from state 1 to state 3 in 4 steps.\n\nlibrary(expm)\n\n# the expm library brings in the \" %^%\" operator for power. \nosptm %^% 4\n\n       [,1]   [,2]   [,3]\n[1,] 0.5021 0.2303 0.2676\n[2,] 0.3860 0.3104 0.3036\n[3,] 0.4760 0.2483 0.2757\n\n\nLooking at the result, we can see that the probability to go from State 1 to State 3 in 4 steps is 0.2676\nWe can also calculate the unconditional distribution after 4 steps\n\ninitial_pro <- c(1/3, 1/3, 1/3)\ninitial_pro %*% (osptm %^% 4)\n\n       [,1]  [,2]   [,3]\n[1,] 0.4547 0.263 0.2823"
  },
  {
    "objectID": "posts/stochastic-processes/index.html#example-2",
    "href": "posts/stochastic-processes/index.html#example-2",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Example 2",
    "text": "Example 2\nUsing a slightly more interesting one-step probability transition matrix having 6 different states.\n\n#specifying transition probability matrix\nosptm<- matrix(c(0.3,0.7,0,0,0,0,1,0,0,0,0,0,0.5,0,0,0,0,0.5, 0,0,0.6,0,0,0.4,0,0,0,0,0.1,0.9,0,0,0,0,0.7,0.3), nrow=6, byrow=TRUE)\nosptm\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  0.3  0.7  0.0    0  0.0  0.0\n[2,]  1.0  0.0  0.0    0  0.0  0.0\n[3,]  0.5  0.0  0.0    0  0.0  0.5\n[4,]  0.0  0.0  0.6    0  0.0  0.4\n[5,]  0.0  0.0  0.0    0  0.1  0.9\n[6,]  0.0  0.0  0.0    0  0.7  0.3\n\nosptm_transposed = t(osptm)\ndiagram::plotmat(osptm_transposed, arr.length = 0.3, arr.width = 0.1, \n                 box.col = \"lightblue\", box.prop = 0.5, box.size = 0.09, box.type=\"circle\", \n                 cex.txt = 0.8, self.cex = 0.6, self.shifty=-0.01, self.shiftx = 0.13)\n\n\n\nosptm_mc <- new(\"markovchain\", transitionMatrix = osptm)\n\nrecurrentClasses(osptm_mc)\n\n[[1]]\n[1] \"1\" \"2\"\n\n[[2]]\n[1] \"5\" \"6\"\n\ntransientClasses(osptm_mc)\n\n[[1]]\n[1] \"3\"\n\n[[2]]\n[1] \"4\"\n\nabsorbingStates(osptm_mc)\n\ncharacter(0)\n\nperiod(osptm_mc)\n\nWarning in period(osptm_mc): The matrix is not irreducible\n\n\n[1] 0\n\nround(steadyStates(osptm_mc), 4)\n\n          1      2 3 4      5      6\n[1,] 0.0000 0.0000 0 0 0.4375 0.5625\n[2,] 0.5882 0.4118 0 0 0.0000 0.0000\n\n\nWe can see that there are 2 possible steady states. Hence the Markov Chain is non-ergodic."
  },
  {
    "objectID": "posts/linear-regression/index.html#linking-the-slope-and-covariance",
    "href": "posts/linear-regression/index.html#linking-the-slope-and-covariance",
    "title": "Linear Regression",
    "section": "Linking the slope and covariance",
    "text": "Linking the slope and covariance\nWhile going over some textbooks or online resources, we find another formula for the slope of our regression line. That formula involve the covariance and or the Pearson coefficient of correlation.\n\\[\\beta_1 = \\frac{Cov(x, y)}{\\sigma^2 x} \\tag{5}\\]\nNow let’s connect both Equation 3 and Equation 5\nI’ll rewrite Equation 3 in a slightly simpler form just to lighten the notation \\[\\beta_1 = \\frac{\\sum x_i (y_i - \\bar{y})}{\\sum x_i (x_i - \\bar{x})}\\]\nNow, it can be noted that that \\(\\sum (x_i - \\bar{x}) = 0\\) or similarly \\(\\sum (y_i - \\bar{y}) = 0\\). Hence \\(\\bar{x} \\sum (y_i - \\bar{y}) = 0\\)\nConsidering \\(\\bar{x}\\) or \\(\\bar{y}\\) are constant, we could also write \\(\\sum \\bar{x} (y_i - \\bar{y}) = 0\\) and similarly \\(\\sum \\bar{x} (x_i - \\bar{x}) = 0\\).\nWith that in mind, we can now, go back on our Equation 3 \\[\\beta_1 = \\frac{\\sum x_i (y_i - \\bar{y})}{\\sum x_i (x_i - \\bar{x})} =\n\\frac{\\sum x_i (y_i - \\bar{y}) - \\sum \\bar{x} (y_i - \\bar{y})}{\\sum x_i (x_i - \\bar{x}) - \\sum \\bar{x} (x_i - \\bar{x})}\\]\n\\[\\beta1 = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\]\nDefining \\(Cov(x, y) = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{n}\\) and \\(\\sigma_x = \\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n}}\\)\nWe can finally rewrite\n\\[\\beta_1 = \\frac{Cov(x, y)}{\\sigma_x^2}\\]\nFinally, if we want to involve the Pearson coefficient of correlation \\(\\rho = \\frac{Cov(x, y)}{\\sigma_x \\sigma_y}\\), we could also re-write our slope as \\[\\beta_1 = \\rho \\frac{\\sigma_y}{\\sigma_x}\\]"
  },
  {
    "objectID": "posts/normality-returns/index.html#histograms",
    "href": "posts/normality-returns/index.html#histograms",
    "title": "Normality of asset returns",
    "section": "Histograms",
    "text": "Histograms\nLet’s see how well the returns stack to our imaginary stock (with close to perfect pseudo-randomness)\n\nggplot(df, aes(return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density() + \n  stat_function(fun = dnorm, n = nrow(df), args = list(mean = drift, sd = sigma), color = 'red', size = 1) +\n  scale_y_continuous() + \n  scale_x_continuous(limits = c(-0.055, 0.055), n.breaks = 9)\n\n\n\n\nThe black line is the actual density of returns, while the red line is the density of the normal distribution with same drift and volatility as earlier. Lines are pretty close to each other.\nAnd now onto the histogram on SPY (again same drift and volatility) as fictitious stock above.\n\nggplot(df_spy, aes(return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density() + \n  geom_vline(xintercept = drift+sigma, color = 'blue', linetype = 3, linewidth = 1) + \n  geom_vline(xintercept = drift-(0.6*sigma),  color = 'blue', linetype = 3, linewidth = 1) + \n  stat_function(fun = dnorm, n = nrow(df), args = list(mean = drift, sd = sigma), color = 'red', size = 1) +\n  scale_y_continuous() + \n  scale_x_continuous(limits = c(-0.055, 0.055), n.breaks = 9)\n\n\n\n\nAnd here, we clearly see the big disconnect from normality: above expected number of returns at the mean (aka too peaked), less returns next to the mean (between 1 and 2 or 2 1/2 sd) and then higher number of observations than expected in the tails (aka fat tails). Distribution of returns for equity are interesting in that sense: both too peaked and fat tails."
  },
  {
    "objectID": "posts/normality-returns/index.html#qq-plots",
    "href": "posts/normality-returns/index.html#qq-plots",
    "title": "Normality of asset returns",
    "section": "QQ Plots",
    "text": "QQ Plots\nAnother way to visually check for normality is to use a quantile-quantile plot (aka QQ-plot). On the y-axis, we have the returns, on the x-axis the theoretical quantiles.\n\nggplot(df, aes(sample = return)) + \n  stat_qq() + \n  stat_qq_line(color = 'blue', linetype = 3, linewidth = 1) + \n  labs(title = 'QQ-Plot for fictious stock returns')\n\n\n\n\nAnd now the QQ-plot for the returs of SPY.\n\nggplot(df_spy, aes(sample = return)) + \n  stat_qq() + \n  stat_qq_line(color = 'blue', linetype = 3, linewidth = 1) + \n  labs(title = 'QQ-Plot for SPY returns')\n\n\n\n\nOh boy! Again, our second plot clearly indicate how the returns deviate from normality.\nThis QQ-plot can also be used to check for asymetry in the distribution of returns. We can see a slightly left skew distribution (a negatively skew distribution)."
  },
  {
    "objectID": "posts/normality-returns/index.html#skewness",
    "href": "posts/normality-returns/index.html#skewness",
    "title": "Normality of asset returns",
    "section": "Skewness",
    "text": "Skewness\nIdeally, skewness as a measure of symmetry should be close to 0 (perfectly symmetric).\nLet’s test the symmetry of our 2 sets of returns. Unfortunately, we did not find any function to calculate skewness in base R (seems strange!).\n\nmoments::skewness(df$return)\n\n[1] 0.0398789\n\nmoments::skewness(df_spy$return)\n\n[1] -0.7418496\n\n\nAs expected, our fictitious stock has almost 0 skew (symmetric around the mean), while the SPY has a moderate negative skew (which we could see already on the QQ-plot and histogram.)"
  },
  {
    "objectID": "posts/normality-returns/index.html#shapiro-wilk-test",
    "href": "posts/normality-returns/index.html#shapiro-wilk-test",
    "title": "Normality of asset returns",
    "section": "Shapiro-Wilk test",
    "text": "Shapiro-Wilk test\nShapiro-Wilk test should actually not be used on large data set. Although, we use it here for demonstration purposes, results should be interpreted with a big spoon of salt.\nLet’s first test on our fictitious equity.\n\nshapiro.test(df$return)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df$return\nW = 0.99895, p-value = 0.9215\n\n\nExpected, as the randomness of our fictitious stock was randomly distributed.\nAnd then on the return of SPY\n\nshapiro.test(df_spy$return)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df_spy$return\nW = 0.894, p-value < 2.2e-16"
  },
  {
    "objectID": "posts/algebra-quant/index.html",
    "href": "posts/algebra-quant/index.html",
    "title": "Algebra For Quant",
    "section": "",
    "text": "I am storing here a few nuggets of algebra, I need for quantitative finance and machine learning.\n\n\\(e^x\\) as an infinite serie\nUsing the McLaurin series expansion, we can define \\(e^x\\) as an infinite sum.\nHere is how it goes: \\[e^x \\approx f(0) + f'(0) \\frac{x}{1} + f''(0) \\frac{x^2}{2!} + f'''(0) \\frac{x^3}{3!} + \\cdots + f^n(0) \\frac{x^n}{n!}\\]\nAs \\(f(0) = f'(0) = f''(0) = f^n(0) = e^0 = 1\\), we can rewrite our previous expression as\n\\[e^x \\approx 1 + \\frac{x}{1} + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots + \\frac{x^n}{n!}\\]\nHence: \\(e^x \\approx \\sum_{n=1}^\\infty \\frac{x^n}{n!}\\)"
  },
  {
    "objectID": "posts/proba-quant/index.html",
    "href": "posts/proba-quant/index.html",
    "title": "Probability For Quant",
    "section": "",
    "text": "I am storing here a few nuggets of probability I encountered in my quantitative finance and machine learning journey. I have a similar page on Algebra.\n\nPoisson distribution\nLet’s \\(X\\) be a Discrete Random Variable (DRV) such that \\(X \\sim Po(\\lambda)\\). Then \\(P(X = x) = \\frac{e^{-\\lambda} \\lambda^x}{x!}\\). Let’s calculate the expectation of X.\n\\[E[X]=\\sum_{x=1}^{\\infty} x P(X=x)=\\sum_{x=1}^{\\infty} x \\frac{e^{-\\lambda} \\lambda^x}{x!} = e^{-\\lambda} \\sum_{x=1}^{\\infty} \\frac{\\lambda^x}{(x-1)!}\\]\n\\[E[X]= e^{-\\lambda} \\lambda \\sum_{x=1}^{\\infty} \\frac{\\lambda^{x-1}}{(x-1)!} = e^{-\\lambda} \\lambda \\sum_{x=0}^{\\infty} \\frac{\\lambda^{x}}{(x)!}= e^{-\\lambda} \\lambda e^{\\lambda} = \\lambda\\]\nThe second to last step is just the McLaurin expansion of \\(e^x\\)."
  },
  {
    "objectID": "posts/binomials_models/index.html#the-risk-free-world",
    "href": "posts/binomials_models/index.html#the-risk-free-world",
    "title": "Binomials models for Quantitative Finance",
    "section": "The Risk-free World",
    "text": "The Risk-free World\nUsing the same idea as earlier and introducing some probabilities.\n\n\n\n\nflowchart LR\n  S -- p' --> uS\n  S -- 1-p' --> vS\n\n\n\n\n\n\n\n\nFrom a probabilistic perspective we could write: \\[S = p'uS + (1-p')vS\\] Or in the presence of a risk free asset, \\[S = \\frac{1}{1+r \\delta t} \\left(p' uS + (1-p')vS \\right)\\]\nWe could isolate \\(p'\\) in this last equation to get: \\[p' = \\frac{1+r \\delta t - v}{u-v} \\tag{3}\\]\n\n\n\n\nflowchart LR\n  V -- p' --> V+\n  V -- 1-p' --> V-\n\n\n\n\n\n\n\n\nTo find \\(V\\): \\[V = p' V^+ + (1-p') V^-\\]\nInteresting to note that the option price \\(V\\) is like an expectation (the sum of the probability) and \\(p'\\) is from Equation 3"
  },
  {
    "objectID": "posts/binomials_models/index.html#mixing-real-world-and-risk-free-world",
    "href": "posts/binomials_models/index.html#mixing-real-world-and-risk-free-world",
    "title": "Binomials models for Quantitative Finance",
    "section": "Mixing real-world and risk-free world",
    "text": "Mixing real-world and risk-free world\nNow if we collide both world: the real-world with drift and volatility and the risk-free world with \\(p'\\): we can set up this 2 equations: One for the expected mean rate of change of prices and another for the variance of these rate of change.\n$$\n{\n\\[\\begin{aligned}\n  \\mu S \\delta t = puS+(1-p)vS-S \\\\\n  \\sigma^2S^2dt = S^2()^2\n\\end{aligned}\\]\n.\n$$\n(TODO re-write these 2 equations)\n2 equations and 3 unknowns, we can choose a solution (the prettiest one!) among the infinitely many:\n\n\\(u = 1 + \\sigma \\sqrt{\\delta t}\\)\n\\(v = 1 - \\sigma \\sqrt{\\delta t}\\)\n\\(p = \\frac{1}{2} + \\frac{\\mu \\sqrt{\\delta t}}{2 \\sigma}\\)\n\\(p' = \\frac{1}{2} + \\frac{r \\sqrt{\\delta t}}{2 \\sigma}\\) \\(p'\\) is the risk-neutral probability."
  },
  {
    "objectID": "posts/binomials_models/index.html#the-binomial-tree",
    "href": "posts/binomials_models/index.html#the-binomial-tree",
    "title": "Binomials models for Quantitative Finance",
    "section": "The binomial tree",
    "text": "The binomial tree\n\nAfter one time step \\(\\delta t\\) our stock, initially at \\(S\\) will either be at \\(uS\\) or \\(vS\\).\n\nAfter two time steps, the stock will either be at \\(u^2S\\) or \\(uvS\\) or \\(v^2S\\)\nAfter three time steps, the stock will either be at \\(u^3S\\) or \\(u^2vS\\) or \\(uv^2S\\) or finally \\(v^3S\\)\netc.\n\nA bit of a crude representation using Geogebra\n\n\n\nBinomial tree representation\n\n\nUsing the equations from the previous sections, we can now create a function to price the value of an option.\n\ncalculate_option_price <- function(spot, strike, rfr, sigma, time, steps) {\n  dt = time/steps            # get delta_t\n  u = 1 + sigma * sqrt(dt)   # multiplying factor when asset rise\n  v = 1 - sigma * sqrt(dt)\n  p_prime = 0.5 + (rfr * sqrt(dt)/(2*sigma))\n  discount_factor = 1 / (1 + rfr * dt)\n  \n  # Calculating vector of prices at maturity \n  s = rep(0, steps+1)   # initialize a vector for prices at maturity - (terminal nodes on the trees)\n  s[1] = spot * v^steps # #initialize the first end-price - (the most bottom right node of the tree)\n  for (i in 2:(steps+1)) { \n    s[i] = s[i-1] * u/v # this is the trick: to go up one leave = going back one step and then up\n  }\n  \n  # Calculating vector of options values at maturity \n  opt = rep(0, steps + 1)\n  for (i in 1:(steps+1)) { \n    opt[i] = max(0, s[i] - strike)\n  }\n  \n  # We have now to work backward and up in the trees (from bottom right and up)\n  # We know the final option value.\n  # We need know to calculate the options in the intermediates nodes\n  for (i in steps:1) {\n    for (j in 1:i) {\n      opt[j] = discount_factor * (p_prime * opt[j+1] + (1 - p_prime)*opt[j])\n    }\n  }\n  #print(glue::glue('Asset price for each time step:', s))\n  #print(glue::glue('Option price for each time step: ', opt))\n  return(opt)\n}\n\nLet’s try our function to get some results\n\ncalculate_option_price(spot = 100, strike = 100, rfr = 0.03, sigma = 0.1, time = 1, steps = 4)\n\n[1]  5.415051  8.117623 11.822154 16.506917 21.550625"
  },
  {
    "objectID": "posts/kmeans-intro/index.html#kmean---hartingan-wonk-in-practice.",
    "href": "posts/kmeans-intro/index.html#kmean---hartingan-wonk-in-practice.",
    "title": "Intro to Kmeans",
    "section": "Kmean - Hartingan-Wonk in practice.",
    "text": "Kmean - Hartingan-Wonk in practice.\n\n# setting up the problem \nlibrary(tibble)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(glue)\n\nnum_obs = 50\nset.seed(1234)\ndf <- tibble(x = runif(num_obs, -10, 10), y = runif(num_obs, -10, 10))\n\n# Step 1: choose number of centroids\nk = 3\n\nset.seed(441)\n# step 2: assign randomly each point to a cluster\ndf <- df |> mutate(centroid = sample(1:k, n(), replace = TRUE))\n\nWe are going to define a couple of function we will use more than once.\n\ncalculate_centroid_mean <- function(df) {\n  yo <- df |> group_by(centroid) |> \n    summarize(mean_x = mean(x), mean_y = mean(y)) \n  #print(yo)\n  return(yo)\n}\n\ntotal_within_cluster_distance <- function(df) {\n  yo <- df |> \n    mutate(distance = (x - centroid_loc$mean_x[centroid])^2 + \n                      (y - centroid_loc$mean_y[centroid])^2)\n  return(sum(yo$distance))\n}\n\n# need to initialize an empty vector for the distance of the observation to a centroid\ndist_centroid <- c()\n\n\n# Starting the iteration process \n\n# step3: \n## a: calculate mean of each centroids\ncentroid_loc <- calculate_centroid_mean(df)\n\n## b: calculate sum of distance between each observation and its assigned cluster \nprint(glue('Initial sum of within-cluster distance is: ', \n           round(total_within_cluster_distance(df), 2)))\n\nInitial sum of within-cluster distance is: 3014.94\n\nrd = 1    # to keep track on how many round / loops we are doing\ni = 0     # to keep track on how many observations have not changed their centroids\n\n# we shall be running the loop until we have no more change of centroids  \nwhile (i < num_obs) { \n  \n  i = 0   # we keep going with the process until no more change of centroids for all the observations\n  \n  # Step 4: for each data point \n  for (obs in 1:num_obs) { \n    \n    # for each centroid \n    for (centroid in 1:k) { \n      # find distance from the observation to the centroid \n      dist_centroid[centroid] = sqrt((df$x[obs] - centroid_loc$mean_x[centroid])^2 + \n                                     (df$y[obs] - centroid_loc$mean_y[centroid])^2) \n      # print(glue('   The distance from point ', obs, ' to centroid ', centroid, ' is ', round(dist_centroid[centroid], 2))) \n      } \n    \n    # assign the observation to its new centroid (based on min distance) \n    prev_centroid = df$centroid[obs] \n    post_centroid = which.min(dist_centroid)  \n    df$centroid[obs] = post_centroid    #assign the new centroid  \n    \n    if (prev_centroid != post_centroid) {  \n      # we recaluate the centroid\n      centroid_loc <- calculate_centroid_mean(df)  \n      print(glue('  The initial centroid for point ', obs, ' was ', \n                 prev_centroid, '. It is now ', post_centroid)) \n      # print(centroid_loc) \n    } else {\n      i = i + 1\n      #print('  No change in centroid')\n    }\n  }\n  rd = rd + 1\n  print(glue('Round ', rd, '  The new sum of within-cluster distance is: ', \n             round(total_within_cluster_distance(df), 2)))\n}\n\n  The initial centroid for point 3 was 3. It is now 1\n  The initial centroid for point 5 was 2. It is now 3\n  The initial centroid for point 6 was 3. It is now 1\n  The initial centroid for point 8 was 1. It is now 2\n  The initial centroid for point 10 was 1. It is now 2\n  The initial centroid for point 11 was 1. It is now 2\n  The initial centroid for point 12 was 2. It is now 3\n  The initial centroid for point 13 was 2. It is now 3\n  The initial centroid for point 14 was 1. It is now 3\n  The initial centroid for point 15 was 2. It is now 3\n  The initial centroid for point 16 was 1. It is now 2\n  The initial centroid for point 19 was 1. It is now 3\n  The initial centroid for point 20 was 3. It is now 2\n  The initial centroid for point 21 was 1. It is now 3\n  The initial centroid for point 23 was 1. It is now 3\n  The initial centroid for point 24 was 1. It is now 2\n  The initial centroid for point 25 was 1. It is now 3\n  The initial centroid for point 26 was 2. It is now 1\n  The initial centroid for point 28 was 1. It is now 3\n  The initial centroid for point 30 was 3. It is now 2\n  The initial centroid for point 31 was 1. It is now 2\n  The initial centroid for point 32 was 1. It is now 2\n  The initial centroid for point 33 was 2. It is now 3\n  The initial centroid for point 34 was 1. It is now 2\n  The initial centroid for point 35 was 2. It is now 3\n  The initial centroid for point 37 was 3. It is now 2\n  The initial centroid for point 38 was 2. It is now 3\n  The initial centroid for point 39 was 3. It is now 1\n  The initial centroid for point 44 was 2. It is now 3\n  The initial centroid for point 45 was 2. It is now 3\n  The initial centroid for point 48 was 2. It is now 3\n  The initial centroid for point 49 was 2. It is now 3\nRound 2  The new sum of within-cluster distance is: 1594.37\n  The initial centroid for point 2 was 3. It is now 1\n  The initial centroid for point 3 was 1. It is now 2\n  The initial centroid for point 5 was 3. It is now 1\n  The initial centroid for point 9 was 3. It is now 1\n  The initial centroid for point 14 was 3. It is now 1\n  The initial centroid for point 17 was 1. It is now 3\n  The initial centroid for point 28 was 3. It is now 1\n  The initial centroid for point 37 was 2. It is now 3\n  The initial centroid for point 41 was 3. It is now 1\n  The initial centroid for point 44 was 3. It is now 1\nRound 3  The new sum of within-cluster distance is: 1146.17\n  The initial centroid for point 7 was 2. It is now 3\n  The initial centroid for point 12 was 3. It is now 1\n  The initial centroid for point 32 was 2. It is now 3\nRound 4  The new sum of within-cluster distance is: 1081.9\n  The initial centroid for point 18 was 2. It is now 3\nRound 5  The new sum of within-cluster distance is: 1071.19\nRound 6  The new sum of within-cluster distance is: 1071.19\n\nprint(centroid_loc)\n\n# A tibble: 3 × 3\n  centroid  mean_x mean_y\n     <int>   <dbl>  <dbl>\n1        1  4.41    -4.99\n2        2 -0.0601   5.27\n3        3 -5.04    -5.47\n\nggplot(df, aes(x, y)) + \n  geom_point(aes(color = as.factor(centroid))) + \n  geom_point(data = centroid_loc, aes(x = mean_x, y = mean_y)) + \n  theme(legend.position = 'none')\n\n\n\n\n\nyoo <- kmeans(df[, 1:2], centers = 3, \n              algorithm = 'Hartigan-Wong', nstart = 10)\nyoo$tot.withinss\n\n[1] 1017.417\n\nlibrary(broom)\ndf2 <- df\naugment(yoo, df2) |> \n  ggplot(aes(x, y)) + \n    geom_point(aes(color = .cluster)) + \n    geom_point(data = as_tibble(yoo$centers), aes(x, y)) + \n    theme(legend.position = 'none')"
  },
  {
    "objectID": "posts/naives-bayes-intro/index.html",
    "href": "posts/naives-bayes-intro/index.html",
    "title": "Naive-Bayes - Part 1",
    "section": "",
    "text": "Some very basic ML using Naive-Bayes and the tidymodel framework.\n\nlibrary(readr)\nlibrary(dplyr)  # mutate(), row_number()\n\ndf <- read_csv('../../raw_data/financial_news.csv', col_names = c('sentiment', 'text')) |> \n  mutate(sentiment = factor(sentiment))\n\nUsing the tidyverse, we’ll\n\nsplit the df into a training and testing set.\n\n\nlibrary(rsample)    # initial_split(), training(), testing()\nlibrary(recipes)\nlibrary(parsnip)    # naive_bayes(), set_engine()\nlibrary(workflows)  # workflow()\n\nlibrary(discrim)\nlibrary(textrecipes)\nlibrary(yardstick)\n\nlist_splits <- initial_split(df, prop = 0.8, strata = 'sentiment')\ndf_train <- training(list_splits)\ndf_test <- testing(list_splits)\n\nlist_recipe <- recipe(sentiment ~., data = df_train) |> \n  step_tokenize(text) |> \n  step_stopwords(text) |> \n  step_tokenfilter(text, max_tokens = 100) |> \n  step_tfidf(text)\n  \n\nmod_nb <- naive_Bayes() |> set_engine('naivebayes') |> set_mode('classification')\nmod_svm <- svm_poly() |> set_engine('kernlab') |> set_mode('classification')\nlist_cv <- vfold_cv(df_train, v = 5, strata = 'sentiment')\n\nwf_nb <- workflow() |> add_recipe(list_recipe) |> add_model(mod_nb)\nwf_nb\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_tokenize()\n• step_stopwords()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n\nwf_svm <- workflow() |> add_recipe(list_recipe) |> add_model(mod_svm)\n\nfit_mod_nb <- fit(wf_nb, df_train)\npred_mod_nb <- predict(fit_mod_nb, df_test)\npred_mod_nb_prob <- predict(fit_mod_nb, df_test, type = 'prob')\n\nfit_mod_svm <- fit(wf_svm, df_train)\n\n Setting default kernel parameters  \n\npred_mod_svm <- predict(fit_mod_svm, df_test)\npred_mod_svm_prob <- predict(fit_mod_svm, df_test, type = 'prob')\n\nbind_cols(df_test, pred_mod_nb) |> conf_mat(sentiment, .pred_class)\n\n          Truth\nPrediction negative neutral positive\n  negative       18       3        7\n  neutral        75     546      197\n  positive       28      27       69\n\nbind_cols(df_test, pred_mod_svm) |> conf_mat(sentiment, .pred_class)\n\n          Truth\nPrediction negative neutral positive\n  negative       33      17       20\n  neutral        71     522      179\n  positive       17      37       74\n\n#roc_nb <- bind_cols(df_test, pred_mod_nb_prob) |> roc_curve()"
  },
  {
    "objectID": "posts/quant-puzzle-01/index.html",
    "href": "posts/quant-puzzle-01/index.html",
    "title": "Quant Puzzle #01",
    "section": "",
    "text": "This is attempt to start a series of puzzles for quant. The source for these puzzles is this noteworthy substack on quant finance"
  },
  {
    "objectID": "posts/quant-puzzle-01/index.html#solving-using-r",
    "href": "posts/quant-puzzle-01/index.html#solving-using-r",
    "title": "Quant Puzzle #01",
    "section": "Solving using R",
    "text": "Solving using R\n\nUsing the mean annualized returns\n\nlibrary(dplyr)  # mutate(), tibble(), if_else(), group_by(), summarize()\nlibrary(purrr)  # pmap()\nlibrary(tidyr)  # unnest()\n\nnum_sim = 10000\n\n# Create a function to get yearly return (both average and compounded)\ncreate_simul_ret <- function(mu, sigma, num_year) { \n  df = tibble(year_num = 1:num_year, \n              ret = rnorm(num_year, mean = mu, sd = sigma)) |> \n    mutate(ann_ret = 1 + ret, \n           cum_ret = cumsum(ann_ret) / year_num,           # Function for mean annualized returns \n           compounded_ret = cumprod(ann_ret)^(1/year_num), # Function to get compounded returns\n           posit_cum_ret = if_else(cum_ret > 1, 1, 0), \n           posit_comp_ret = if_else(compounded_ret > 1, 1, 0))\n  return(df)\n}\n\n# To get all different random return numbers, I need to fetch the data from the row\n\ndf <- tibble(sim_num = 1:num_sim, mu = 0.03, sigma = 0.1, num_year = 30) |> \n  mutate(sim_ret = pmap(list(mu, sigma, num_year), create_simul_ret)) |> \n  unnest(cols = c(sim_ret)) |>      # large df with 30 rows/year per simulation\n  group_by(year_num) |>             # group by year to calculate average returns\n  summarize(avg_num_pos_ret = mean(posit_cum_ret), \n            avg_num_comp_pos_ret = mean(posit_comp_ret))\n\n\n\n\n\n\n\n  \n    \n    \n      year_num\n      avg_num_pos_ret\n      avg_num_comp_pos_ret\n    \n  \n  \n    1\n0.6196\n0.6196\n    2\n0.6547\n0.6418\n    3\n0.6935\n0.6715\n    4\n0.7205\n0.6926\n    5\n0.7413\n0.7130\n    6\n0.7604\n0.7258\n    7\n0.7794\n0.7408\n    8\n0.7910\n0.7538\n    9\n0.8090\n0.7698\n    10\n0.8230\n0.7857\n    11\n0.8326\n0.7924\n    12\n0.8443\n0.8010\n    13\n0.8538\n0.8127\n    14\n0.8607\n0.8190\n    15\n0.8748\n0.8330\n    16\n0.8838\n0.8418\n    17\n0.8918\n0.8487\n    18\n0.8989\n0.8557\n    19\n0.9019\n0.8623\n    20\n0.9074\n0.8662\n    21\n0.9133\n0.8740\n    22\n0.9157\n0.8794\n    23\n0.9198\n0.8841\n    24\n0.9228\n0.8881\n    25\n0.9298\n0.8929\n    26\n0.9368\n0.8972\n    27\n0.9384\n0.9044\n    28\n0.9427\n0.9085\n    29\n0.9471\n0.9111\n    30\n0.9510\n0.9155"
  },
  {
    "objectID": "posts/quant-puzzle-01/index.html#solving-using-python",
    "href": "posts/quant-puzzle-01/index.html#solving-using-python",
    "title": "Quant Puzzle #01",
    "section": "Solving Using Python",
    "text": "Solving Using Python\n\nimport random\n#Computes a moving average from a gaussian sample over n trials\ndef gauss_moving_average(mu,sigma,n):\n    moving_average_list = []\n    for i in range(n):\n        if i == 0:\n            moving_average_list.append(random.gauss(mu,sigma))\n        else:\n            moving_average_list.append((moving_average_list[i-1]*i+random.gauss(mu,sigma))/(i+1))\n    return moving_average_list\n\n#Outputs a list of guassian moving average time series\ndef run_sims(mu, sigma, n, years):\n    sims = []\n    for i in range(n):\n        sims.append(gauss_moving_average(mu,sigma,years))\n    return sims\n#Outputs the percentage of positive entries in a column of a matrix\ndef percent_positive_column(matrix,column):\n    n = len(matrix)\n    percent_positive = 0\n    for i in range(n):\n        if matrix[i][column] > 0:\n            percent_positive+=(1/n) \n    return percent_positive*100\n\n\n#Runs the simulation and checks for our confidence level\nmatrix = run_sims(3,10,100000, 100)\nfor i in range(100):\n    conf = percent_positive_column(matrix, i)\n    print(conf)\n    if  conf >= 91.3:\n        print(\"Year\",i+1, \"has positive returns with probability\", conf)\n        break\n\n61.91799999998169\n66.44099999996111\n69.72599999994615\n72.48299999993361\n74.79499999992308\n76.96999999991318\n78.73499999990516\n80.17699999989858\n81.61899999989203\n82.90999999988615\n83.97999999988129\n85.08399999987626\n86.07399999987176\n86.97899999986764\n87.74599999986414\n88.4789999998608\n89.25699999985727\n89.9309999998542\n90.45099999985183\n90.96599999984949\n91.56699999984676\nYear 21 has positive returns with probability 91.56699999984676"
  },
  {
    "objectID": "posts/autocorrelation/index.html#autocorrelation-plots---correlogram",
    "href": "posts/autocorrelation/index.html#autocorrelation-plots---correlogram",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Autocorrelation plots - Correlogram",
    "text": "Autocorrelation plots - Correlogram\nAs exercise, we can plot the auto-correlation of a non-stationary (aka with significant autocorrelation) time-series. We are using the Monthly Milk production (no idea where the data come from)\n\nUsing R\nIn R the standard function to plot a correlogram is the acf() function\n\nlibrary(readr)\n\nmilk <- read_csv('../../raw_data/milk.csv')\nacf(milk$milk_prod_per_cow_kg)\n\n\n\n\nGraph clearly shows some seasonality (at the 12 lags ==> yearly correlation) which indicates that our data are non-stationary (next section).\nIf we are more attached to the auto-correlation values, we can store the results in a dataframe.\n\nyo <- acf(milk$milk_prod_per_cow_kg, plot = F)\nyo\n\n\nAutocorrelations of series 'milk$milk_prod_per_cow_kg', by lag\n\n    0     1     2     3     4     5     6     7     8     9    10    11    12 \n1.000 0.892 0.778 0.620 0.487 0.428 0.376 0.415 0.454 0.562 0.687 0.769 0.845 \n   13    14    15    16    17    18    19    20    21    22 \n0.745 0.638 0.490 0.364 0.306 0.255 0.287 0.321 0.417 0.529 \n\n\nWe could use the ggplot package to create a function to draw acf and get more customization. We will re-use this function later as well.\n\n# slightly fancier version (with more customization)\nggacf <- function(series) {\n  significance_level <- qnorm((1 + 0.95)/2)/sqrt(sum(!is.na(series)))  \n  a <- acf(series, plot=F)\n  a.2 <- with(a, data.frame(lag, acf))\n  g <- ggplot(a.2[-1,], aes(x=lag,y=acf)) + \n    geom_segment(mapping = aes(xend = lag, yend = 0), linewidth = 0.8) + \n    xlab('Lag') + ylab('ACF') + \n    geom_hline(yintercept=c(significance_level,-significance_level), linetype= 'dashed', color = 'dodgerblue4');\n\n  # fix scale for integer lags\n  if (all(a.2$lag%%1 == 0)) {\n    g<- g + scale_x_discrete(limits = factor(seq(1, max(a.2$lag))));\n  }\n  return(g);\n}\n\n\nlibrary(ggplot2)\nlibrary(tibble)\n\nggacf(milk$milk_prod_per_cow_kg)\n\n\n\n\n\n\nUsing Python\nIn python, we need to use the statsmodel package.\n\nfrom pandas import read_csv\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf\n\ndf = read_csv('../../raw_data/milk.csv', index_col=0)\nplot_acf(df)\nplt.show()"
  },
  {
    "objectID": "posts/autocorrelation/index.html#statistical-test-to-check-white-noise.",
    "href": "posts/autocorrelation/index.html#statistical-test-to-check-white-noise.",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Statistical test to check white-noise.",
    "text": "Statistical test to check white-noise.\nIn R we can use the Ljung-Box test (Portmanteau ‘Q’ test).\n\nBox.test(wn, type = 'Ljung-Box', lag = 1)\n\n\n    Box-Ljung test\n\ndata:  wn\nX-squared = 4.8374, df = 1, p-value = 0.02785"
  },
  {
    "objectID": "posts/autocorrelation/index.html#application-to-a-financial-asset",
    "href": "posts/autocorrelation/index.html#application-to-a-financial-asset",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Application to a financial asset",
    "text": "Application to a financial asset\nWe know that most financial assets prices are not stationary. Let’s take SBUX for instance. That being said, the log difference of their prices is stationary. Note how \\(log(P_t) - log(P{t-1}) = log( \\frac{P_t}{P_{t-1} )\\)\n\nlibrary(dplyr)\nlibrary(lubridate)\n\ndf <- read_csv('../../raw_data/SBUX.csv') |> arrange(date) |> \n  select(date, adjClose) |> \n  mutate(ret_1d = log(adjClose / lag(adjClose)), \n         ret_5d = log(adjClose / lag(adjClose, n = 5)), \n         y_t = log(adjClose) - log(lag(adjClose)), \n         day_of_week = weekdays(date)) |> \n  filter(date > '2018-01-01' & day_of_week == 'Tuesday')\n\nggacf(df$y_t)\n\n\n\n\n\nPython code\n\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\npy_df = pd.read_csv('../../raw_data/SBUX.csv')\n\npy_df.index = py_df['date']\npy_df_ts = pd.Series(py_df['adjClose'])\nlog_ret = np.log(1 + py_df_ts.pct_change())\nlog_ret = log_ret.dropna()\n\nr, q, p = acf(log_ret, nlags = 25, qstat = True)\n\nfig = plt.figure()\nplot_acf(log_ret, lags=25)\nplt.show()\n\n\n\n\n\n# q is for the Ljung-Box test statistics\nq\n\narray([26.3136516 , 26.316042  , 26.4475469 , 27.2734633 , 28.00235211,\n       28.69472715, 28.70545674, 35.25084316, 39.48634821, 39.62923899,\n       40.19059746, 40.26948906, 40.2707996 , 40.27868851, 44.51737182,\n       44.57802557, 45.63422739, 45.70863764, 46.1791967 , 46.4744188 ,\n       47.53325326, 48.52664511, 49.81175008, 50.99884363, 54.25246436])\n\n\n\n# p is for the p-value of the Ljung-Box statistics. \np\n\narray([2.90229916e-07, 1.92994124e-06, 7.68595886e-06, 1.75019647e-05,\n       3.63602517e-05, 6.94858936e-05, 1.63715557e-04, 2.40667022e-05,\n       9.41189824e-06, 1.96904817e-05, 3.31867363e-05, 6.48649190e-05,\n       1.25022094e-04, 2.30799458e-04, 9.12196699e-05, 1.61044765e-04,\n       1.95803242e-04, 3.27130628e-04, 4.67495545e-04, 6.93532977e-04,\n       7.95682642e-04, 9.24023511e-04, 9.75159499e-04, 1.05482880e-03,\n       6.16160760e-04])\n\n\n\nq1, p1 = acorr_ljungbox(log_ret, lags =25, return_df = False, boxpierce = False)\np1\n\n'lb_pvalue'\n\n\n\nfig = plt.figure()\nplot_pacf(log_ret, lags=25)\nplt.show()\n\n\n\n\n\nlog_ret.describe()\n\ncount    5653.000000\nmean       -0.000559\nstd         0.019890\nmin        -0.168728\n25%        -0.009802\n50%        -0.000345\n75%         0.008773\nmax         0.176788\nName: adjClose, dtype: float64\n\nlog_ret.plot()\nplt.show()\n\n\n\npd.Series.idxmax(log_ret)\n\n'2020-03-13'\n\npd.Series.idxmin(log_ret)\n\n'2009-07-21'"
  },
  {
    "objectID": "posts/random-behavior-assets/index.html#the-euler-maruyana-method-to-compute-the-sde",
    "href": "posts/random-behavior-assets/index.html#the-euler-maruyana-method-to-compute-the-sde",
    "title": "Random Behavior of Financial Assets",
    "section": "The Euler-Maruyana Method to compute the SDE",
    "text": "The Euler-Maruyana Method to compute the SDE\nWe start with Equation 6 : \\(S_{i+1} = S_i \\cdot (1 + \\mu \\delta t + \\sigma \\phi \\sqrt{\\delta t})\\)\n\n# create one simulation for price \nndays <- 252 \nprice <- c()\nprice[1] <- last(df$adjClose)\n\nphi = rnorm(ndays, mean = 0, sd = 1)\n\nfor (i in 2:ndays){ \n  price[i] = price[i-1] * (1 + mu * delta_t + sigma * phi[i] * sqrt(delta_t))\n}\n\nyo <- tibble(x = 1:ndays, price = price)\nggplot(yo, aes(x, price)) + \n  geom_line()\n\n\n\n\nWe can now create 100’s such simulations re-using previous code in a function.\n\ncreate_price_simul <- function(x) {\n  price <- c() \n  price[1] <- last(df$adjClose) \n  phi = rnorm(ndays, mean = 0, sd = 1) \n  for (i in 2:ndays){ \n    price[i] = price[i-1] * (1 + mu * delta_t + sigma * phi[i] * sqrt(delta_t)) \n    } \n  yo <- tibble(x = 1:ndays, price = price)\n  return(yo)\n}\n\nlibrary(purrr)      # map()\nlibrary(RColorBrewer)\n\nnum_of_simul <- 100\ndf1 <- tibble(simul_num = 1:num_of_simul) |> \n  mutate(prices = map(simul_num, create_price_simul))\n\nyo <- df1 |> unnest(cols = c(prices))\n\ngetPalette = colorRampPalette(brewer.pal(9, \"Set1\"))\ncolourCount = num_of_simul\nggplot(yo, aes(x, price, group = simul_num)) + \n  scale_fill_manual(values = colorRampPalette(brewer.pal(9, \"Accent\"))(colourCount)) +\n  geom_line(aes(color = simul_num)) + \n  theme(legend.position = 'none')\n\nWarning in brewer.pal(9, \"Accent\"): n too large, allowed maximum for palette Accent is 8\nReturning the palette you asked for with that many colors"
  },
  {
    "objectID": "posts/autocorrelation/index.html#using-r-1",
    "href": "posts/autocorrelation/index.html#using-r-1",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Using R",
    "text": "Using R\n\nlibrary(dplyr)\nlibrary(lubridate)\n\ndf <- read_csv('../../raw_data/SBUX.csv') |> arrange(date) |> \n  select(date, adjClose) |> \n  mutate(ret_1d = log(adjClose / lag(adjClose)), \n         ret_5d = log(adjClose / lag(adjClose, n = 5)), \n         y_t = log(adjClose) - log(lag(adjClose)), \n         day_of_week = weekdays(date)) |> \n  filter(date > '2018-01-01' & day_of_week == 'Tuesday')\n\nggacf(df$y_t)"
  },
  {
    "objectID": "posts/autocorrelation/index.html#python-code",
    "href": "posts/autocorrelation/index.html#python-code",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Python code",
    "text": "Python code\n\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\npy_df = pd.read_csv('../../raw_data/SBUX.csv')\npy_df.index = py_df['date']\npy_df = py_df.sort_index()\n\npy_df_ts = pd.Series(py_df['adjClose'])\nlog_ret = np.log(1 + py_df_ts.pct_change())\nlog_ret = log_ret.dropna()\n\nr, q, p = acf(log_ret, nlags = 25, qstat = True)\n\nfig = plt.figure()\nplot_acf(log_ret, lags=25)\nplt.show()\n\n\n\n\n\n# q is for the Ljung-Box test statistics\nq\n\narray([26.3136516 , 26.316042  , 26.4475469 , 27.2734633 , 28.00235211,\n       28.69472715, 28.70545674, 35.25084316, 39.48634821, 39.62923899,\n       40.19059746, 40.26948906, 40.2707996 , 40.27868851, 44.51737182,\n       44.57802557, 45.63422739, 45.70863764, 46.1791967 , 46.4744188 ,\n       47.53325326, 48.52664511, 49.81175008, 50.99884363, 54.25246436])\n\n\n\n# p is for the p-value of the Ljung-Box statistics. \np\n\narray([2.90229916e-07, 1.92994124e-06, 7.68595886e-06, 1.75019647e-05,\n       3.63602517e-05, 6.94858936e-05, 1.63715557e-04, 2.40667022e-05,\n       9.41189824e-06, 1.96904817e-05, 3.31867363e-05, 6.48649190e-05,\n       1.25022094e-04, 2.30799458e-04, 9.12196699e-05, 1.61044765e-04,\n       1.95803242e-04, 3.27130628e-04, 4.67495545e-04, 6.93532977e-04,\n       7.95682642e-04, 9.24023511e-04, 9.75159499e-04, 1.05482880e-03,\n       6.16160760e-04])\n\n\n\nq1, p1 = acorr_ljungbox(log_ret, lags =25, return_df = False, boxpierce = False)\np1\n\n'lb_pvalue'\n\n\n\nfig = plt.figure()\nplot_pacf(log_ret, lags=25)\nplt.show()\n\n\n\n\n\nlog_ret.describe()\n\ncount    5653.000000\nmean        0.000559\nstd         0.019890\nmin        -0.176788\n25%        -0.008773\n50%         0.000345\n75%         0.009802\nmax         0.168728\nName: adjClose, dtype: float64\n\nlog_ret.plot()\nplt.show()\n\n\n\npd.Series.idxmax(log_ret)\n\n'2009-07-22'\n\npd.Series.idxmin(log_ret)\n\n'2020-03-16'"
  },
  {
    "objectID": "posts/autocorrelation/index.html#compose-deterministic-time-series-with-trend-and-seasonality-with-stochastic-component",
    "href": "posts/autocorrelation/index.html#compose-deterministic-time-series-with-trend-and-seasonality-with-stochastic-component",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Compose deterministic time-series (with trend and seasonality) with stochastic component",
    "text": "Compose deterministic time-series (with trend and seasonality) with stochastic component\n\ndf <- tibble(x = 1:252, phi = rnorm(252, mean = 0, sd = 1.5)) |> \n  mutate(y = 0.07 * x + 0.03 + phi)\n\nggplot(df, aes(x, y)) + \n  geom_line() + \n  ggtitle(label = 'Compose a linear trend with a stochastic component')\n\n\n\n\nWe can also create a seasonality with a stochastic component\n\ndf <- tibble(x = 1:252, phi = rnorm(252, mean = 0, sd = 1.5)) |> \n  mutate(y = 1.7 * sin((2 * pi * x / 50) + 0.3 * pi ) + phi)\n         \nggplot(df, aes(x, y)) + \n  geom_line() + \n  ggtitle(label = 'Compose a seasonal trend with a stochastic component')"
  },
  {
    "objectID": "posts/stochastic-calculus-part3/index.html",
    "href": "posts/stochastic-calculus-part3/index.html",
    "title": "Stochastic Calculus - Part 3",
    "section": "",
    "text": "Product rule within Stochastic Calculus\nWhen dealing with Stochastic Differential Equations, we can adapt some of the rules of classical calculus such as the product rule: \\(d(xy) = xdy + ydx\\)\nLet’s say we have 2 stochastic processes: \\[d(X(t)) = \\mu_1 X(t) dt + \\sigma_1 X(t)  dW_t\\] \\[d(Y(t)) = \\mu_2 Y(t) dt + \\sigma_2 Y(t) dW_t\\]\nAnd we define a function \\(F\\) which is a product of these 2 stochastic processes such that \\(F = F(X,Y) = XY\\).\nUsing a Taylor Series Expansion, we can write: \\[F(X + dX, Y + dY) \\approx F(X,Y) + \\frac{\\partial F}{\\partial X} dX + \\frac{\\partial F}{\\partial Y} dY + \\frac{1}{2} \\frac{\\partial^2F}{\\partial X^2} dX^2 + \\frac{1}{2} \\frac{\\partial^2F}{\\partial Y^2} dY^2 + \\frac{\\partial^2F}{\\partial X \\partial Y} dXdY + \\dots\\]\nHence, \\[dF = \\frac{\\partial F}{\\partial X} dX + \\frac{\\partial F}{\\partial Y} dY + \\frac{1}{2} \\frac{\\partial^2F}{\\partial X^2} dX^2 + \\frac{1}{2} \\frac{\\partial^2F}{\\partial Y^2} dY^2 + \\frac{\\partial^2F}{\\partial X \\partial Y} dXdY + \\dots \\tag{1}\\]\nNow, we can calculate all these partial derivatives and plugged them back in the above equation. \\(\\frac{\\partial F}{\\partial X} = Y\\) and \\(\\frac{\\partial^2 F}{\\partial X^2} = 0\\).\nSimilarly \\(\\frac{\\partial F}{\\partial Y} = X\\) and \\(\\frac{\\partial^2 F}{\\partial Y^2} = 0\\).\nFinally: \\(\\frac{\\partial^2F}{\\partial X \\partial Y} = 1\\)\nPlugging it all back in Equation 1: \\[dF = Y dX + X dY + dXdY \\tag{2}\\]\n\n\nIntegral by parts\nIn classical calculus, we re-use the product rule to come up with the integration by part: \\(d(xy) = xdy + ydx\\). That is \\(xdy = d(xy) - ydx\\) which we can integrate for and get: \\(\\int xdy = \\int d(xy) - \\int y dx\\) which is the same as \\(\\int x dy = xy - \\int y dx\\).\nLet’s bring this to stochastic calculus. Again \\(F\\) is a function of the product of 2 stochastic processes: \\(F = F(X,Y) = XY\\) Using the same logic and our previous result Equation 2, we write \\[d(XY) = Y dX + X dY + dXdY\\] \\[X dY = d(XY) - Y dX - dXdY \\] \\[\\int_0^t X_s dY_s = \\int_0^t d(X_sY_s) - \\int_0^t Y_s dX_s - \\int_0^t dX_sdY_s\\] \\[\\int_0^t X_s dY_s = X_tY_t - X_0Y_0 - \\int_0^t Y_s dX_s - \\int_o^t dX_sdY_s\\]\n\n\nQuotient Rule within Stochastic Calculus\nWe will re-use the Taylor Series Expansion (Equation 1) except this time the function \\(F\\) is a function of the quotient of 2 stochastic processes: \\(F = F(X, Y) = \\frac{X}{Y}\\). Calculating all the partial derivatives: \\(\\frac{\\partial F}{\\partial X} = \\frac{1}{Y}\\) and \\(\\frac{\\partial^2 F}{\\partial X^2} = 0\\).\nSimilarly \\(\\frac{\\partial F}{\\partial Y} = \\frac{-X}{Y^2}\\) and \\(\\frac{\\partial^2 F}{\\partial Y^2} = \\frac{2X}{Y^3}\\).\nFinally: \\(\\frac{\\partial^2F}{\\partial X \\partial Y} = \\frac{-1}{Y^2}\\)\nPutting it all back together: \\[dF = \\frac{1}{Y} dX + \\frac{-X}{Y^2} dY + \\frac{1}{2} \\frac{2X}{Y^3} dY^2+ \\frac{-1}{Y^2} dXdY\\] Which we can re-write as: \\[dF = d \\left( \\frac{X}{Y} \\right) = \\frac{X}{Y} \\cdot \\left( \\frac{1}{X} dX - \\frac{1}{Y} dY - \\frac{1}{XY} dXdY + \\frac{1}{Y^2} dY^2\\right) \\tag{3}\\]"
  }
]