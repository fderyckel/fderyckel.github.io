[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "QUANT dira-t-on?",
    "section": "",
    "text": "Series\n\nTime-series\nA series of posts related to the analysis of time-series.\n\n\nQuant - Part 1\nA series of posts when starting quant finance. Basic mathematical concepts and related code in R and Python when starting quantitative finance.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nStochastic Calculus - Part 3\n\n\n\n\n\n\n\ntime-series\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nQuant Puzzle #01\n\n\n\n\n\n\n\nR-code\n\n\nquant-finance\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nBinomials models for Quantitative Finance\n\n\n\n\n\n\n\nR-code\n\n\nquant-finance\n\n\nbinomial_models\n\n\n\n\nCreating a basic or binomial model on pricing an option.\n\n\n\n\n\n\nApr 6, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\nNaive-Bayes - Part 1\n\n\n\n\n\n\n\nML\n\n\nNaive-Bayes\n\n\nsentiment analysis\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\n0 min\n\n\n\n\n\n\n\n\nNormality of asset returns\n\n\n\n\n\n\n\nquant-finance\n\n\nnormal-distribution\n\n\nqq-plot\n\n\nkurtosis\n\n\n\n\nChecking the normality of asset returns visually and quantitatively.\n\n\n\n\n\n\nApr 19, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\nR-code\n\n\nlinear-regression\n\n\n\n\nA dive into the math behind the linear regression algorithm.\n\n\n\n\n\n\nApr 14, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nStatistical Moments\n\n\n\n\n\n\n\nstatistics\n\n\ncode\n\n\nanalysis\n\n\n\n\nIntroducing the first 4 moments of statistical analysis: mean, standard deviation, skewness and kurtosis. Showing how to use R and Python on these concepts.\n\n\n\n\n\n\nNov 2, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\nRandom Behavior of Financial Assets\n\n\n\n\n\n\n\nR-code\n\n\nquant-finance\n\n\n\n\nWe explore one of the main assumption of quantitative finance: assets returns are random.\n\n\n\n\n\n\nApr 18, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\nLinear Algebra for Quantitative Finance\n\n\n\n\n\n\n\nR-code\n\n\nquant-finance\n\n\nLinear-Algebra\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nPortfolio Optimization Part I (in R)\n\n\n\n\n\n\n\nportfolio\n\n\nR-code\n\n\nquant-finance\n\n\nmodel\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\n7 min\n\n\n\n\n\n\n\n\nStochastic processes - Discrete Time Markov Chain\n\n\n\n\n\n\n\nMarkov Chain\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nIntro to Kmeans\n\n\n\n\n\n\n\nkmeans\n\n\ncode\n\n\nanalysis\n\n\nmodel\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\nMarkov Chains\n\n\n\n\n\n\n\nRandom Walk\n\n\nMarkov Chain\n\n\ncode\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\nKmeans with regime changes\n\n\n\n\n\n\n\nkmeans\n\n\ncode\n\n\nanalysis\n\n\ntidymodel\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\nAlgebra For Quant\n\n\n\n\n\n\n\nstatistics\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\n0 min\n\n\n\n\n\n\n\n\nTranslating Python Part 1 - Xgboost with Time-Series\n\n\n\n\n\n\n\nxgboost\n\n\ntidymodel\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\nProbability For Quant\n\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2022\n\n\n0 min\n\n\n\n\n\n\n\n\nAutoCorrelation, Stationarity and Random-Walk - Part 1\n\n\n\n\n\n\n\ntime-series\n\n\n\n\nA dive into the concepts of autocorrelation and stationarity of time-series. We also get into how to plot correlogram using R and Python, random-walk, white-noise. Finally, we check how to compose and decompose time-series into their various components (trend, seasonanilty)\n\n\n\n\n\n\nSep 29, 2022\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/time-series/time-series.html",
    "href": "posts/time-series/time-series.html",
    "title": "Series: Time-series",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/time-series/autocorrelation/index.html",
    "href": "posts/time-series/autocorrelation/index.html",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "",
    "text": "This post is to set up the basic concepts of time-series analysis."
  },
  {
    "objectID": "posts/time-series/autocorrelation/index.html#autocorrelation-plots---correlogram",
    "href": "posts/time-series/autocorrelation/index.html#autocorrelation-plots---correlogram",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Autocorrelation plots - Correlogram",
    "text": "Autocorrelation plots - Correlogram\nAs exercise, we can plot the auto-correlation of a non-stationary (aka with significant autocorrelation) time-series. We are using the Monthly Milk production (no idea where the data come from)\n\nUsing R\nIn R the standard function to plot a correlogram is the acf() function\n\nlibrary(readr)\n\nmilk <- read_csv('../../../raw_data/milk.csv')\nacf(milk$milk_prod_per_cow_kg)\n\n\n\n\nGraph clearly shows some seasonality (at the 12 lags ==> yearly correlation) which indicates that our data are non-stationary (next section).\nIf we are more attached to the auto-correlation values, we can store the results in a dataframe.\n\nyo <- acf(milk$milk_prod_per_cow_kg, plot = F)\nyo\n\n\nAutocorrelations of series 'milk$milk_prod_per_cow_kg', by lag\n\n    0     1     2     3     4     5     6     7     8     9    10    11    12 \n1.000 0.892 0.778 0.620 0.487 0.428 0.376 0.415 0.454 0.562 0.687 0.769 0.845 \n   13    14    15    16    17    18    19    20    21    22 \n0.745 0.638 0.490 0.364 0.306 0.255 0.287 0.321 0.417 0.529 \n\n\nWe could use the ggplot package to create a function to draw acf and get more customization. We will re-use this function later as well.\n\n# slightly fancier version (with more customization)\nggacf <- function(series) {\n  significance_level <- qnorm((1 + 0.95)/2)/sqrt(sum(!is.na(series)))  \n  a <- acf(series, plot=F)\n  a.2 <- with(a, data.frame(lag, acf))\n  g <- ggplot(a.2[-1,], aes(x=lag,y=acf)) + \n    geom_segment(mapping = aes(xend = lag, yend = 0), linewidth = 0.8) + \n    xlab('Lag') + ylab('ACF') + \n    geom_hline(yintercept=c(significance_level,-significance_level), linetype= 'dashed', color = 'dodgerblue4');\n\n  # fix scale for integer lags\n  if (all(a.2$lag%%1 == 0)) {\n    g<- g + scale_x_discrete(limits = factor(seq(1, max(a.2$lag))));\n  }\n  return(g);\n}\n\n\nlibrary(ggplot2)\nlibrary(tibble)\n\nggacf(milk$milk_prod_per_cow_kg)\n\n\n\n\n\n\nUsing Python\nIn python, we need to use the statsmodel package.\n\nfrom pandas import read_csv\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf\n\ndf = read_csv('../../../raw_data/milk.csv', index_col=0)\nplot_acf(df)\nplt.show()"
  },
  {
    "objectID": "posts/time-series/autocorrelation/index.html#statistical-test-to-check-white-noise.",
    "href": "posts/time-series/autocorrelation/index.html#statistical-test-to-check-white-noise.",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Statistical test to check white-noise.",
    "text": "Statistical test to check white-noise.\nIn R we can use the Ljung-Box test (Portmanteau ‘Q’ test).\n\nBox.test(wn, type = 'Ljung-Box', lag = 1)\n\n\n    Box-Ljung test\n\ndata:  wn\nX-squared = 4.8374, df = 1, p-value = 0.02785"
  },
  {
    "objectID": "posts/time-series/autocorrelation/index.html#using-r-1",
    "href": "posts/time-series/autocorrelation/index.html#using-r-1",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Using R",
    "text": "Using R\n\nlibrary(dplyr)\nlibrary(lubridate)\n\ndf <- read_csv('../../../raw_data/SBUX.csv') |> arrange(date) |> \n  select(date, adjClose) |> \n  mutate(ret_1d = log(adjClose / lag(adjClose)), \n         ret_5d = log(adjClose / lag(adjClose, n = 5)), \n         y_t = log(adjClose) - log(lag(adjClose)), \n         day_of_week = weekdays(date)) |> \n  filter(date > '2018-01-01' & day_of_week == 'Tuesday')\n\nggacf(df$y_t)"
  },
  {
    "objectID": "posts/time-series/autocorrelation/index.html#python-code",
    "href": "posts/time-series/autocorrelation/index.html#python-code",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Python code",
    "text": "Python code\n\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\npy_df = pd.read_csv('../../../raw_data/SBUX.csv')\npy_df.index = py_df['date']\npy_df = py_df.sort_index()\n\npy_df_ts = pd.Series(py_df['adjClose'])\nlog_ret = np.log(1 + py_df_ts.pct_change())\nlog_ret = log_ret.dropna()\n\nr, q, p = acf(log_ret, nlags = 25, qstat = True)\n\nfig = plt.figure()\nplot_acf(log_ret, lags=25)\nplt.show()\n\n\n\n\n\n# q is for the Ljung-Box test statistics\nq\n\narray([26.3136516 , 26.316042  , 26.4475469 , 27.2734633 , 28.00235211,\n       28.69472715, 28.70545674, 35.25084316, 39.48634821, 39.62923899,\n       40.19059746, 40.26948906, 40.2707996 , 40.27868851, 44.51737182,\n       44.57802557, 45.63422739, 45.70863764, 46.1791967 , 46.4744188 ,\n       47.53325326, 48.52664511, 49.81175008, 50.99884363, 54.25246436])\n\n\n\n# p is for the p-value of the Ljung-Box statistics. \np\n\narray([2.90229916e-07, 1.92994124e-06, 7.68595886e-06, 1.75019647e-05,\n       3.63602517e-05, 6.94858936e-05, 1.63715557e-04, 2.40667022e-05,\n       9.41189824e-06, 1.96904817e-05, 3.31867363e-05, 6.48649190e-05,\n       1.25022094e-04, 2.30799458e-04, 9.12196699e-05, 1.61044765e-04,\n       1.95803242e-04, 3.27130628e-04, 4.67495545e-04, 6.93532977e-04,\n       7.95682642e-04, 9.24023511e-04, 9.75159499e-04, 1.05482880e-03,\n       6.16160760e-04])\n\n\n\nq1, p1 = acorr_ljungbox(log_ret, lags =25, return_df = False, boxpierce = False)\np1\n\n'lb_pvalue'\n\n\n\nfig = plt.figure()\nplot_pacf(log_ret, lags=25)\nplt.show()\n\n\n\n\n\nlog_ret.describe()\n\ncount    5653.000000\nmean        0.000559\nstd         0.019890\nmin        -0.176788\n25%        -0.008773\n50%         0.000345\n75%         0.009802\nmax         0.168728\nName: adjClose, dtype: float64\n\nlog_ret.plot()\nplt.show()\n\n\n\npd.Series.idxmax(log_ret)\n\n'2009-07-22'\n\npd.Series.idxmin(log_ret)\n\n'2020-03-16'"
  },
  {
    "objectID": "posts/time-series/autocorrelation/index.html#compose-deterministic-time-series-with-trend-and-seasonality-with-stochastic-component",
    "href": "posts/time-series/autocorrelation/index.html#compose-deterministic-time-series-with-trend-and-seasonality-with-stochastic-component",
    "title": "AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Compose deterministic time-series (with trend and seasonality) with stochastic component",
    "text": "Compose deterministic time-series (with trend and seasonality) with stochastic component\n\ndf <- tibble(x = 1:252, phi = rnorm(252, mean = 0, sd = 1.5)) |> \n  mutate(y = 0.07 * x + 0.03 + phi)\n\nggplot(df, aes(x, y)) + \n  geom_line() + \n  ggtitle(label = 'Compose a linear trend with a stochastic component')\n\n\n\n\nWe can also create a seasonality with a stochastic component\n\ndf <- tibble(x = 1:252, phi = rnorm(252, mean = 0, sd = 1.5)) |> \n  mutate(y = 1.7 * sin((2 * pi * x / 50) + 0.3 * pi ) + phi)\n         \nggplot(df, aes(x, y)) + \n  geom_line() + \n  ggtitle(label = 'Compose a seasonal trend with a stochastic component')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "My name is François de Ryckel. I have grown up in Belgium then I emigrated from there to finish up my study and start working. I lived in several places over the last 20 years. I’m a math / philosophy teacher with some stunts at business. I created 2 companies in Zambia: a fruits farm and a fresh produce trading company.\n\nLived in Paris, France, for 2 years to complete my undergrad and start my master\nLived in Freiburg & Leipzig, Germany, for 3 years to complete my master and do some teaching gigs at Alliance Française, Leipzig Universitat and Leipzig International School\nLived in Dhaka, Bangladesh for 3 years to teach at the International School Dhaka\nLived in Zambia for 9 years to teach math, stats and philosophy at the American International School of Lusaka. I also started a citrus & mangoes farm (over 9,000 trees) and a produce (fresh fish, fruits, and meat) trading company\nLived Thuwal, Saudi Arabia on the shore of the beautiful Red Sea for 5 years. I worked as math teacher on the KAUST university Campus at TKS (The KAUST School)\nCurrently living in Bangkok (Thailand) as a curriculum coordinator for the KIS School.\n\nAs a teacher, I’m always looking for good examples to incorporate in my practices. Lately I am especially interested in machine learning applications in finance and education.\n\nEducation\n\nMaster in Philosophy, 2001\nParis I - Pantheon Sorbonne\n\n\nBachelor in Logic, 2000\nParis I - Pantheon Sorbonne\n\n\nBachelor in Philosophy, 1999\nParis I - Pantheon Sorbonne\n\n\n\nInterests\n\nModeling and machine learning\nMath education\nOutdoor Living and active lifestyle (Swimming, Cycling, Running)"
  },
  {
    "objectID": "posts/stochastic-processes/index.html",
    "href": "posts/stochastic-processes/index.html",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "",
    "text": "This post is an introduction to Markov Chain with a presentation of Discrete Time Markov Chains."
  },
  {
    "objectID": "posts/stochastic-processes/index.html#definition",
    "href": "posts/stochastic-processes/index.html#definition",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Definition",
    "text": "Definition\nA stochastic process is \\(\\{ X(t), t \\in T \\}\\) is a collection of random variables indexed by a parameter t that belongs to a set T.\n\nt is generally the time\n\\(X(t)\\) is the state of the process at time t\nThe state space \\(S\\) of a stochastic process is all possible state \\(X(t)\\) for any \\(t \\in T\\)\nif T is a countable set, we call this a discrete-time process\n\nA discrete-time Markov Chain is a discrete-time stochastic process which state space S is finite such that: \\[\\mathbb{P}(X_{n+1} = j | X_0 = i_0, X_1 = i_1, X_2 = i_2, \\dots, x_n = i) = \\mathbb{P}(X_{n+1} = j | X_n = i) = P_{ij}\\]\nthat is, the conditional probability of the process being in state j at time n + 1 given all the previous states depends only on the last-known position (state i at time n)."
  },
  {
    "objectID": "posts/stochastic-processes/index.html#some-other-teminology",
    "href": "posts/stochastic-processes/index.html#some-other-teminology",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Some other teminology",
    "text": "Some other teminology\n\nA state is called absorbing if the chain cannot leave it once it enters it. An absorbing Markov chain has at least one absorbing state.\nA state is termed reflecting if once the chain leaves it, it cannot return to it.\nThe period d of a state i is the number such that, starting in i, the chain can return to i only in the number of steps that are multiples of d. A state with period d = 1 is called aperiodic. Periodicity is a class property.\n\nFor a reflecting state, the period is infinite, since the chain never comes back to this state.\nAbsorbing states necessarily have loops and thus are aperiodic states.\n\na state is called recurrent if with probability 1 the chain ever reenters that state. Otherwise, the state is called transient.\nA Markov Chain that has a unique stationary distribution (or steady-state distribution) is called an ergodic chain."
  },
  {
    "objectID": "posts/stochastic-processes/index.html#chapman-kolmogorov-equations",
    "href": "posts/stochastic-processes/index.html#chapman-kolmogorov-equations",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Chapman-Kolmogorov equations",
    "text": "Chapman-Kolmogorov equations\nWe denote the probability to go from state \\(i\\) to state \\(j\\) in n-steps by \\(\\bf{P}_{ij}^{(n)}\\). It is also denoted as the n-steps transition probability matrix. That is for any time \\(m >= 0, \\bf{P}_{ij}^n = \\mathbb{P}(X_{m+n} = j | X_m = i)\\) . \\(\\bf{P}^{(n)} = \\bf{P}^n\\) based on the Chapman-Kolmogorov equation.\nThe Chapman-Kolmogorov equation states that for all positive integers \\(m\\) and \\(n\\) , \\(\\bf{P}^{(m+n)} = \\bf{P}^m \\cdot \\bf{P}^n\\) where P is a one-step probability transition matrix (a square matrix)"
  },
  {
    "objectID": "posts/stochastic-processes/index.html#example-1",
    "href": "posts/stochastic-processes/index.html#example-1",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Example 1",
    "text": "Example 1\nTo model a Markov Chain, let’s first set up a one-step probability transition matrix (called here osptm).\nWe start with an easy 3 possible state process. That is the state space \\(S = \\{1, 2, 3\\}\\). The osptm will provide the probability to go from one state to another.\n\nosptm = matrix(c(0.7,0.1,0.2, 0,0.6,0.4, 0.5,0.2,0.3), nrow = 3, byrow = TRUE)\nosptm\n\n     [,1] [,2] [,3]\n[1,]  0.7  0.1  0.2\n[2,]  0.0  0.6  0.4\n[3,]  0.5  0.2  0.3\n\n\nWe can always have a look at how the osptm looks like.\n\n# note we have to transpose the osptm matrix first. \nosptm_transposed = t(osptm)\nosptm_transposed\n\n     [,1] [,2] [,3]\n[1,]  0.7  0.0  0.5\n[2,]  0.1  0.6  0.2\n[3,]  0.2  0.4  0.3\n\ndiagram::plotmat(osptm_transposed, pos = c(1, 2), arr.length = 0.3, \n                 box.col = \"lightblue\", box.prop = 0.5, box.size = 0.12, box.type=\"circle\", \n                 self.cex = 0.6, self.shifty=-0.01, self.shiftx = 0.15)\n\n\n\n\nThe markovchain package can provide us with all the state characteristics of a one-step probabilty transition matrix.\n\nlibrary(markovchain)\nosptm_mc <- new(\"markovchain\", transitionMatrix = osptm)\n\nrecurrentClasses(osptm_mc)\n\n[[1]]\n[1] \"1\" \"2\" \"3\"\n\ntransientClasses(osptm_mc)\n\nlist()\n\nabsorbingStates(osptm_mc)\n\ncharacter(0)\n\nperiod(osptm_mc)\n\n[1] 1\n\nround(steadyStates(osptm_mc), 4)\n\n          1      2      3\n[1,] 0.4651 0.2558 0.2791\n\n\nThe next step is to calculate, for instance, what is the probability to go from state 1 to state 3 in 4 steps.\n\nlibrary(expm)\n\n# the expm library brings in the \" %^%\" operator for power. \nosptm %^% 4\n\n       [,1]   [,2]   [,3]\n[1,] 0.5021 0.2303 0.2676\n[2,] 0.3860 0.3104 0.3036\n[3,] 0.4760 0.2483 0.2757\n\n\nLooking at the result, we can see that the probability to go from State 1 to State 3 in 4 steps is 0.2676\nWe can also calculate the unconditional distribution after 4 steps\n\ninitial_pro <- c(1/3, 1/3, 1/3)\ninitial_pro %*% (osptm %^% 4)\n\n       [,1]  [,2]   [,3]\n[1,] 0.4547 0.263 0.2823"
  },
  {
    "objectID": "posts/stochastic-processes/index.html#example-2",
    "href": "posts/stochastic-processes/index.html#example-2",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Example 2",
    "text": "Example 2\nUsing a slightly more interesting one-step probability transition matrix having 6 different states.\n\n#specifying transition probability matrix\nosptm<- matrix(c(0.3,0.7,0,0,0,0,1,0,0,0,0,0,0.5,0,0,0,0,0.5, 0,0,0.6,0,0,0.4,0,0,0,0,0.1,0.9,0,0,0,0,0.7,0.3), nrow=6, byrow=TRUE)\nosptm\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  0.3  0.7  0.0    0  0.0  0.0\n[2,]  1.0  0.0  0.0    0  0.0  0.0\n[3,]  0.5  0.0  0.0    0  0.0  0.5\n[4,]  0.0  0.0  0.6    0  0.0  0.4\n[5,]  0.0  0.0  0.0    0  0.1  0.9\n[6,]  0.0  0.0  0.0    0  0.7  0.3\n\nosptm_transposed = t(osptm)\ndiagram::plotmat(osptm_transposed, arr.length = 0.3, arr.width = 0.1, \n                 box.col = \"lightblue\", box.prop = 0.5, box.size = 0.09, box.type=\"circle\", \n                 cex.txt = 0.8, self.cex = 0.6, self.shifty=-0.01, self.shiftx = 0.13)\n\n\n\nosptm_mc <- new(\"markovchain\", transitionMatrix = osptm)\n\nrecurrentClasses(osptm_mc)\n\n[[1]]\n[1] \"1\" \"2\"\n\n[[2]]\n[1] \"5\" \"6\"\n\ntransientClasses(osptm_mc)\n\n[[1]]\n[1] \"3\"\n\n[[2]]\n[1] \"4\"\n\nabsorbingStates(osptm_mc)\n\ncharacter(0)\n\nperiod(osptm_mc)\n\nWarning in period(osptm_mc): The matrix is not irreducible\n\n\n[1] 0\n\nround(steadyStates(osptm_mc), 4)\n\n          1      2 3 4      5      6\n[1,] 0.0000 0.0000 0 0 0.4375 0.5625\n[2,] 0.5882 0.4118 0 0 0.0000 0.0000\n\n\nWe can see that there are 2 possible steady states. Hence the Markov Chain is non-ergodic."
  },
  {
    "objectID": "posts/naives-bayes-intro/index.html",
    "href": "posts/naives-bayes-intro/index.html",
    "title": "Naive-Bayes - Part 1",
    "section": "",
    "text": "Some very basic ML using Naive-Bayes and the tidymodel framework.\n\nlibrary(readr)\nlibrary(dplyr)  # mutate(), row_number()\n\ndf <- read_csv('../../raw_data/financial_news.csv', col_names = c('sentiment', 'text')) |> \n  mutate(sentiment = factor(sentiment))\n\nUsing the tidyverse, we’ll\n\nsplit the df into a training and testing set.\n\n\nlibrary(rsample)    # initial_split(), training(), testing()\nlibrary(recipes)\nlibrary(parsnip)    # naive_bayes(), set_engine()\nlibrary(workflows)  # workflow()\n\nlibrary(discrim)\nlibrary(textrecipes)\nlibrary(yardstick)\n\nlist_splits <- initial_split(df, prop = 0.8, strata = 'sentiment')\ndf_train <- training(list_splits)\ndf_test <- testing(list_splits)\n\nlist_recipe <- recipe(sentiment ~., data = df_train) |> \n  step_tokenize(text) |> \n  step_stopwords(text) |> \n  step_tokenfilter(text, max_tokens = 100) |> \n  step_tfidf(text)\n  \n\nmod_nb <- naive_Bayes() |> set_engine('naivebayes') |> set_mode('classification')\nmod_svm <- svm_poly() |> set_engine('kernlab') |> set_mode('classification')\nlist_cv <- vfold_cv(df_train, v = 5, strata = 'sentiment')\n\nwf_nb <- workflow() |> add_recipe(list_recipe) |> add_model(mod_nb)\nwf_nb\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_tokenize()\n• step_stopwords()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n\nwf_svm <- workflow() |> add_recipe(list_recipe) |> add_model(mod_svm)\n\nfit_mod_nb <- fit(wf_nb, df_train)\npred_mod_nb <- predict(fit_mod_nb, df_test)\npred_mod_nb_prob <- predict(fit_mod_nb, df_test, type = 'prob')\n\nfit_mod_svm <- fit(wf_svm, df_train)\n\n Setting default kernel parameters  \n\npred_mod_svm <- predict(fit_mod_svm, df_test)\npred_mod_svm_prob <- predict(fit_mod_svm, df_test, type = 'prob')\n\nbind_cols(df_test, pred_mod_nb) |> conf_mat(sentiment, .pred_class)\n\n          Truth\nPrediction negative neutral positive\n  negative       20       9       19\n  neutral        83     550      197\n  positive       18      17       57\n\nbind_cols(df_test, pred_mod_svm) |> conf_mat(sentiment, .pred_class)\n\n          Truth\nPrediction negative neutral positive\n  negative       28       7       21\n  neutral        83     554      192\n  positive       10      15       60\n\n#roc_nb <- bind_cols(df_test, pred_mod_nb_prob) |> roc_curve()"
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html",
    "href": "posts/kmeans-regime-change/index.html",
    "title": "Kmeans with regime changes",
    "section": "",
    "text": "This post is about how to use Kmeans to classify various market regimes or to use Kmeans to classify financial observations.\nWith K-means we are trying to establish groups of data that are homegenous and distinctly different from other groups. The K- stands for the number of clusters we will create.\nThe concept of distance comes in when deciding if a data point belongs to a cluster. The most common way to measure distance is the Euclidean Distance.\nWith multivariate data set, it is important to normalize the data.\nA usual rule of thumb is to set the number of clusters as the square root of the number of observation."
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html#load-up-packages-and-read-data",
    "href": "posts/kmeans-regime-change/index.html#load-up-packages-and-read-data",
    "title": "Kmeans with regime changes",
    "section": "Load up packages and read data",
    "text": "Load up packages and read data\n\nlibrary(readr)        # load and read .csv file\nlibrary(glue)         # concatenate strings together\nlibrary(dplyr)        # the tidy plyr tool for data wrangling\nlibrary(tidyr)        # to use the drop_na function\nthe_path <- here::here()\ndf <- read_csv(glue(the_path, \"/raw_data/AMD.csv\")) |> \n  rename(adj_close = 'adjClose') |> \n  select(date, high, low, close, adj_close)\nglimpse(df)\n\nRows: 5,611\nColumns: 5\n$ date      <date> 2023-04-21, 2023-04-20, 2023-04-19, 2023-04-18, 2023-04-17,…\n$ high      <dbl> 89.8000, 91.5795, 90.5400, 92.1600, 90.6900, 92.9700, 93.160…\n$ low       <dbl> 88.0550, 88.7300, 88.2200, 89.3300, 88.3000, 90.5000, 91.830…\n$ close     <dbl> 88.43, 90.11, 89.94, 89.78, 89.87, 91.75, 92.09, 92.33, 94.0…\n$ adj_close <dbl> 88.43, 90.11, 89.94, 89.78, 89.87, 91.75, 92.09, 92.33, 94.0…"
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html#feature-engineering",
    "href": "posts/kmeans-regime-change/index.html#feature-engineering",
    "title": "Kmeans with regime changes",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nlibrary(TTR)      # The technical analysis package\nyo <- aroon(df[, c('high', 'low')], n = 23)\ndf$aroon <- yo[, 3]\nyo <- CCI(df[, c('high', 'low', 'close')], n = 17)\ndf$cci <- yo\nyo <- chaikinVolatility(df[, c('high', 'low')], n = 13)\ndf$chaikinVol <- yo\ndf1 <- df |> \n  select(date, aroon, cci, chaikinVol, adj_close) |> \n  mutate(across(c(aroon, cci, chaikinVol), ~ as.numeric(scale(.)))) |>\n  drop_na()\nskimr::skim(df1 %>% select(-date))\n\n\nData summary\n\n\nName\ndf1 %>% select(-date)\n\n\nNumber of rows\n5586\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\naroon\n0\n1\n0.00\n1.00\n-1.49\n-0.94\n-0.19\n0.90\n1.65\n▇▆▂▆▆\n\n\ncci\n0\n1\n0.00\n1.00\n-4.66\n-0.78\n-0.08\n0.80\n4.06\n▁▂▇▅▁\n\n\nchaikinVol\n0\n1\n0.00\n1.00\n-2.49\n-0.70\n-0.10\n0.59\n4.37\n▂▇▅▁▁\n\n\nadj_close\n0\n1\n22.27\n28.78\n1.62\n5.45\n11.48\n23.12\n161.91\n▇▁▁▁▁\n\n\n\n\n# also good to check for correlation between variables. \nlibrary(corrr)\ndf1 |> select(-date, -adj_close) |> \n  correlate() |> \n  rearrange() |> \n  shave()\n\n# A tibble: 3 × 4\n  term          cci  aroon chaikinVol\n  <chr>       <dbl>  <dbl>      <dbl>\n1 cci        NA     NA             NA\n2 aroon       0.564 NA             NA\n3 chaikinVol  0.212  0.223         NA\n\n\nThese 3 variables seem to complete each other well as little to-no correlation."
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html#create-clusters",
    "href": "posts/kmeans-regime-change/index.html#create-clusters",
    "title": "Kmeans with regime changes",
    "section": "Create clusters",
    "text": "Create clusters\n\nlibrary(purrr)     #use the map function\nlibrary(broom)     #use the glance function on kmeans \ndf1sc <- df1 %>% select(-date, -adj_close)\nkclusts <- tibble(k = 1:9) |> \n  mutate(kclust = map(k, ~kmeans(df1sc, centers = .x, nstart = 30, iter.max = 50L)), \n         glanced = map(kclust, glance), \n         augmented = map(kclust, augment, df1))\nkclusts |> unnest(cols = c('glanced'))\n\n# A tibble: 9 × 7\n      k kclust    totss tot.withinss betweenss  iter augmented           \n  <int> <list>    <dbl>        <dbl>     <dbl> <int> <list>              \n1     1 <kmeans> 16755.       16755. -3.27e-10     1 <tibble [5,586 × 6]>\n2     2 <kmeans> 16755.        9842.  6.91e+ 3     1 <tibble [5,586 × 6]>\n3     3 <kmeans> 16755.        7916.  8.84e+ 3     3 <tibble [5,586 × 6]>\n4     4 <kmeans> 16755.        6425.  1.03e+ 4     4 <tibble [5,586 × 6]>\n5     5 <kmeans> 16755.        5492.  1.13e+ 4     5 <tibble [5,586 × 6]>\n6     6 <kmeans> 16755.        4626.  1.21e+ 4     5 <tibble [5,586 × 6]>\n7     7 <kmeans> 16755.        4199.  1.26e+ 4     5 <tibble [5,586 × 6]>\n8     8 <kmeans> 16755.        3791.  1.30e+ 4     6 <tibble [5,586 × 6]>\n9     9 <kmeans> 16755.        3477.  1.33e+ 4     5 <tibble [5,586 × 6]>\n\n\nThere are several ways to choose the ideal number of clusters. One of them is the elbow method, another one is the Silhouette Method.\nThe tot.withinss is the total within-cluster sum of square. This is the value used for the eblow method.\nFor the Silhouette Method, we can use the cluster package.\n\navg_sil <- function(k) { \n  kmeans_object <- kmeans(df1sc, centers = k, iter.max = 50L)\n  silh = cluster::silhouette(kmeans_object$cluster, dist(df1sc))\n  mean(silh[, 3])\n  }\n# Compute and plot wss for k = 2 to k = 15\nyo <- tibble(k_values =  2:9) |> \n  mutate(avg_sil_values = map_dbl(k_values, avg_sil))\nyo\n\n# A tibble: 8 × 2\n  k_values avg_sil_values\n     <int>          <dbl>\n1        2          0.378\n2        3          0.345\n3        4          0.286\n4        5          0.312\n5        6          0.296\n6        7          0.284\n7        8          0.295\n8        9          0.279\n\n\nA more elegant way to do that, using this post from SO\n\nyo <- kclusts |> \n  mutate(silhouetted = map(augmented, ~ cluster::silhouette(as.numeric(levels(.x$.cluster))[.x$.cluster], dist(df1sc)))) |> \n  select(k, silhouetted) |> unnest(cols=c('silhouetted')) |> \n  group_by(k) %>% \n  summarise(avg_sil_values = mean(silhouetted[,3]))\nyo\n\n# A tibble: 9 × 2\n      k avg_sil_values\n  <int>          <dbl>\n1     1         NA    \n2     2          0.378\n3     3          0.345\n4     4          0.293\n5     5          0.313\n6     6          0.320\n7     7          0.305\n8     8          0.298\n9     9          0.273"
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html#some-visualizations",
    "href": "posts/kmeans-regime-change/index.html#some-visualizations",
    "title": "Kmeans with regime changes",
    "section": "Some visualizations",
    "text": "Some visualizations\n\nElbow method\n\nlibrary(ggplot2)\nkclusts |> \n  unnest(cols = c('glanced')) |> \n  ggplot(aes(k, tot.withinss)) + \n  geom_line(alpha = 0.5, size = 1.2, color = 'midnightblue') + \n  geom_point(size = 2, color = 'midnightblue')\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nTotal within-cluster sum of square for k-cluster\n\n\n\n\nBased on the elbow method, I would be tempted to choose to 5 clusters (2 seems another obvious one).\n\n\nSilhouette Method\n\nyo |> ggplot(aes(k, avg_sil_values)) + \n  geom_line(alpha = 0.5, size = 1.2, color = 'midnightblue') + \n  geom_point(size = 2, color = 'midnightblue')\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nSilhouette score for k-clusters\n\n\n\n\n2 is the winner ;-)\n\n\nPlotting the stocks with clustered observations\n\nlibrary(lubridate)\nyo <- kmeans(df1 |> select(-date, -adj_close), centers = 2)\naugment(yo, df1) |> filter(date >= today() - 500) |> \n  ggplot(aes(x = date, y = adj_close)) + \n    geom_line(alpha = 0.5, color = 'midnightblue') + \n    geom_point(aes(color = .cluster)) + \n    theme(legend.position = 'none')\n\n\n\n\nPlotting adjusted close price with only 2 clusters\n\n\n\n\n\nyo <- kmeans(df1 |> select(-date, -adj_close), centers = 3)\naugment(yo, df1) |> filter(date >= today() - 500) |> \n  ggplot(aes(x = date, y = adj_close)) + \n    geom_line(alpha = 0.5, color = 'midnightblue') + \n    geom_point(aes(color = .cluster)) + \n    theme(legend.position = 'none')\n\n\n\n\nPlotting adjusted close price with only 3 clusters\n\n\n\n\n\nyo <- kmeans(df1 |> select(-date, -adj_close), centers = 6)\naugment(yo, df1) |> filter(date >= today() - 500) |> \n  ggplot(aes(x = date, y = adj_close)) + \n    geom_line(alpha = 0.5, color = 'midnightblue') + \n    geom_point(aes(color = .cluster)) + \n    theme(legend.position = 'none')\n\n\n\n\nPlotting adjusted close price with only 6 clusters"
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is probably the most basic example of a machine learning algorithms."
  },
  {
    "objectID": "posts/linear-regression/index.html#finding-the-coefficients-from-scratch",
    "href": "posts/linear-regression/index.html#finding-the-coefficients-from-scratch",
    "title": "Linear Regression",
    "section": "Finding the coefficients from scratch",
    "text": "Finding the coefficients from scratch\nIn the case of simple linear regression, we just have one independent variable and one dependent variable. Let’s say we have \\(n\\) observations \\((x_i, y_i)\\) and we want to find a linear equations that predict y \\(\\hat{y_i}\\) based on a given \\(x_i\\).\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\]\n\n\\(x_i\\) is the independent variable (aka: predictor, explanatory variable)\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters of our model that have to be found.\n\n\\(\\beta_0\\) is the intercept (value of y when x=0)\n\\(\\beta_1\\) is the slope of our linear model\n\n\\(\\epsilon_i\\) is the residual or error term of the \\(i^{th}\\) observations\n\nFrom a probabilistic perspective, \\(\\epsilon\\) can be seen as a random variable with the following properties: \\(E(\\epsilon)=0\\) and \\(Var(\\epsilon)= \\sigma_{\\epsilon}^2 = \\sigma^2\\)\n\n\\(\\hat{y_i}\\) is the estimated or predicted value of y. In that sense \\(\\hat{y_i} = \\beta_0 + \\beta_1 x_i\\). The error term is then the difference between the actual y and the predicted y: \\(\\epsilon_i = y_i - \\hat{y_i}\\)\n\nThe cost function (or loss function) is to minimize the sum of squared error. In that sense, we seek to minimize \\[ \\text{min } SSE = min \\sum_{i=1}^{n} \\epsilon_i^2 =\n        \\underset{\\beta_0, \\beta1}{argmin} \\sum_{i=1}^{n} (y_i-\\beta_0 - \\beta_1 x_i)^2 \\tag{1}\\]\n\n\n\nTrying to minimize the sum of the squared of the vertical bars\n\n\nTo minimize the SSE, we will need to use partial derivatives for both coefficients and solve it for 0.\nLet’s first focus on \\(\\beta_0\\)\n\\[\\frac{\\partial SSE}{\\partial \\beta_0} = -2 \\sum_{i=1}^{n} (y_i-\\beta_0 - \\beta_1 x_i) = 0\\] Breaking down our sum: \\[\\sum_{i=1}^{n} y_i - \\sum_{i=1}^{n} \\beta_0 - \\sum_{i=1}^{n} \\beta_1 x_i = 0\\] \\(\\beta0\\) and \\(\\beta_1\\) are coefficient, hence: \\[\\sum_{i=1}^{n} y_i - n \\beta_0 - \\beta_1 \\sum_{i=1}^{n} x_i = 0\\] and \\[\\beta_0 = \\frac{\\sum_{i=1}^{n} y_i - \\beta_1 \\sum_{i=1}^{n} x_i}{n}\\] \\(\\bar{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}\\) (mean of y values) and \\(\\bar{x}=\\frac{\\sum_{i=1}^{n} x_i}{n}\\) (mean of x values).\nand our previous equation can then be simplified as \\[\\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\tag{2}\\]\nLet’s now address the second partial derivative wrt \\(\\beta_1\\).\n\\[\\frac{\\partial SSE}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i (y_i-\\beta_0 - \\beta_1 x_i) = 0\\].\nDistributing the \\(x_i\\) and substituting in the value of \\(\\beta_0\\) from Equation 2\n\\[\\sum_{i=1}^{n} (x_i y_i - x_i (\\bar{y} - \\beta_1 \\bar{x}) - \\beta_1 x_i^2) = 0\\].\nFactoring \\(\\beta_1\\) and breaking down the sum and being careful to the sign, we get: \\[\\beta_1 = \\frac{\\sum_{i=1}^{n} x_i (y_i - \\bar{y})}{\\sum_{i=1}^{n} x_i (x_i - \\bar{x})} \\tag{3}\\]\nKnowing \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\) and \\(\\bar{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}\\), we can get one step further (have a common denominator with over n). I have also removed the index on the sum for readability.\n\\[\\beta_1 = \\frac{n \\Sigma x_i y_i - \\Sigma x_i \\Sigma y_i}{n \\Sigma x_i^2 - (\\Sigma x_i)^2} \\tag{4}\\]\nWe can now use the values of \\(\\beta_0\\) Equation 2 and \\(\\beta_1\\) Equation 4 into our estimate of y: \\(\\hat{y_i} = \\beta_0 + \\beta_1 x_i\\)"
  },
  {
    "objectID": "posts/linear-regression/index.html#linking-the-slope-and-covariance",
    "href": "posts/linear-regression/index.html#linking-the-slope-and-covariance",
    "title": "Linear Regression",
    "section": "Linking the slope and covariance",
    "text": "Linking the slope and covariance\nWhile going over some textbooks or online resources, we find another formula for the slope of our regression line. That formula involve the covariance and or the Pearson coefficient of correlation.\n\\[\\beta_1 = \\frac{Cov(x, y)}{\\sigma^2 x} \\tag{5}\\]\nNow let’s connect both Equation 3 and Equation 5\nI’ll rewrite Equation 3 in a slightly simpler form just to lighten the notation \\[\\beta_1 = \\frac{\\sum x_i (y_i - \\bar{y})}{\\sum x_i (x_i - \\bar{x})}\\]\nNow, it can be noted that that \\(\\sum (x_i - \\bar{x}) = 0\\) or similarly \\(\\sum (y_i - \\bar{y}) = 0\\). Hence \\(\\bar{x} \\sum (y_i - \\bar{y}) = 0\\)\nConsidering \\(\\bar{x}\\) or \\(\\bar{y}\\) are constant, we could also write \\(\\sum \\bar{x} (y_i - \\bar{y}) = 0\\) and similarly \\(\\sum \\bar{x} (x_i - \\bar{x}) = 0\\).\nWith that in mind, we can now, go back on our Equation 3 \\[\\beta_1 = \\frac{\\sum x_i (y_i - \\bar{y})}{\\sum x_i (x_i - \\bar{x})} =\n\\frac{\\sum x_i (y_i - \\bar{y}) - \\sum \\bar{x} (y_i - \\bar{y})}{\\sum x_i (x_i - \\bar{x}) - \\sum \\bar{x} (x_i - \\bar{x})}\\]\n\\[\\beta1 = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\]\nDefining \\(Cov(x, y) = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{n}\\) and \\(\\sigma_x = \\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n}}\\)\nWe can finally rewrite\n\\[\\beta_1 = \\frac{Cov(x, y)}{\\sigma_x^2}\\]\nFinally, if we want to involve the Pearson coefficient of correlation \\(\\rho = \\frac{Cov(x, y)}{\\sigma_x \\sigma_y}\\), we could also re-write our slope as \\[\\beta_1 = \\rho \\frac{\\sigma_y}{\\sigma_x}\\]"
  },
  {
    "objectID": "posts/linear-regression/index.html#considerations-when-doing-linear-regression",
    "href": "posts/linear-regression/index.html#considerations-when-doing-linear-regression",
    "title": "Linear Regression",
    "section": "Considerations when doing linear regression",
    "text": "Considerations when doing linear regression\n\nStart with a scatter plot to check if data have a linear trend. No points of doing a linear regression on a set of data, if data are not showing a linear trend.\n\nHow well the data fits the regression line (correlation) have NO incidence on causality. Correlation is no indication of causation\nVariables have to be normally distributed. This can be checked using histogram or QQ-plot or some other stat tests - Shapiro-Wilk test, Kolmogorov–Smirnov test. Skewness and kurtosis can also be used for that. In case of violation of this assumption, a Box-Cox transformation could be used.\nHomoscedasticity in the residuals. Variance in the spread of residuals should be constant. \nError terms are normally distributed (visual: histogram, QQ-plot)\nIn the case of multi-variables linear regression, ensure no correlation between independent variables"
  },
  {
    "objectID": "time-series.html",
    "href": "time-series.html",
    "title": "Series: Time-series",
    "section": "",
    "text": "AutoCorrelation, Stationarity and Random-Walk - Part 1\n\n\n\n\n\nA dive into the concepts of autocorrelation and stationarity of time-series. We also get into how to plot correlogram using R and Python, random-walk, white-noise. Finally, we check how to compose and decompose time-series into their various components (trend, seasonanilty)\n\n\n\n\n\n\nSep 29, 2022\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "quant-series.html",
    "href": "quant-series.html",
    "title": "Series: Quant - Part 1",
    "section": "",
    "text": "Normality of asset returns\n\n\n\n\n\nChecking the normality of asset returns visually and quantitatively.\n\n\n\n\n\n\nApr 19, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\nRandom Behavior of Financial Assets\n\n\n\n\n\nWe explore one of the main assumption of quantitative finance: assets returns are random.\n\n\n\n\n\n\nApr 18, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\nBinomials models for Quantitative Finance\n\n\n\n\n\nCreating a basic or binomial model on pricing an option.\n\n\n\n\n\n\nApr 6, 2023\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/quant-part1/normality-returns/index.html#histograms",
    "href": "posts/quant-part1/normality-returns/index.html#histograms",
    "title": "Normality of asset returns",
    "section": "Histograms",
    "text": "Histograms\nLet’s see how well the returns stack to our imaginary stock (with close to perfect pseudo-randomness)\n\nggplot(df, aes(return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density() + \n  stat_function(fun = dnorm, n = nrow(df), args = list(mean = drift, sd = sigma), color = 'red', size = 1) +\n  scale_y_continuous() + \n  scale_x_continuous(limits = c(-0.055, 0.055), n.breaks = 9)\n\n\n\n\nThe black line is the actual density of returns, while the red line is the density of the normal distribution with same drift and volatility as earlier. Lines are pretty close to each other.\nAnd now onto the histogram on SPY (again same drift and volatility) as fictitious stock above.\n\nggplot(df_spy, aes(return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density() + \n  geom_vline(xintercept = drift+sigma, color = 'blue', linetype = 3, linewidth = 1) + \n  geom_vline(xintercept = drift-(0.6*sigma),  color = 'blue', linetype = 3, linewidth = 1) + \n  stat_function(fun = dnorm, n = nrow(df), args = list(mean = drift, sd = sigma), color = 'red', size = 1) +\n  scale_y_continuous() + \n  scale_x_continuous(limits = c(-0.055, 0.055), n.breaks = 9)\n\n\n\n\nAnd here, we clearly see the big disconnect from normality: above expected number of returns at the mean (aka too peaked), less returns next to the mean (between 1 and 2 or 2 1/2 sd) and then higher number of observations than expected in the tails (aka fat tails). Distribution of returns for equity are interesting in that sense: both too peaked and fat tails."
  },
  {
    "objectID": "posts/quant-part1/normality-returns/index.html#qq-plots",
    "href": "posts/quant-part1/normality-returns/index.html#qq-plots",
    "title": "Normality of asset returns",
    "section": "QQ Plots",
    "text": "QQ Plots\nAnother way to visually check for normality is to use a quantile-quantile plot (aka QQ-plot). On the y-axis, we have the returns, on the x-axis the theoretical quantiles.\n\nggplot(df, aes(sample = return)) + \n  stat_qq() + \n  stat_qq_line(color = 'blue', linetype = 3, linewidth = 1) + \n  labs(title = 'QQ-Plot for fictious stock returns')\n\n\n\n\nAnd now the QQ-plot for the returs of SPY.\n\nggplot(df_spy, aes(sample = return)) + \n  stat_qq() + \n  stat_qq_line(color = 'blue', linetype = 3, linewidth = 1) + \n  labs(title = 'QQ-Plot for SPY returns')\n\n\n\n\nOh boy! Again, our second plot clearly indicate how the returns deviate from normality.\nThis QQ-plot can also be used to check for asymetry in the distribution of returns. We can see a slightly left skew distribution (a negatively skew distribution)."
  },
  {
    "objectID": "posts/quant-part1/normality-returns/index.html#skewness",
    "href": "posts/quant-part1/normality-returns/index.html#skewness",
    "title": "Normality of asset returns",
    "section": "Skewness",
    "text": "Skewness\nIdeally, skewness as a measure of symmetry should be close to 0 (perfectly symmetric).\nLet’s test the symmetry of our 2 sets of returns. Unfortunately, we did not find any function to calculate skewness in base R (seems strange!).\n\nmoments::skewness(df$return)\n\n[1] 0.0398789\n\nmoments::skewness(df_spy$return)\n\n[1] -0.7418496\n\n\nAs expected, our fictitious stock has almost 0 skew (symmetric around the mean), while the SPY has a moderate negative skew (which we could see already on the QQ-plot and histogram.)"
  },
  {
    "objectID": "posts/quant-part1/normality-returns/index.html#kurtosis",
    "href": "posts/quant-part1/normality-returns/index.html#kurtosis",
    "title": "Normality of asset returns",
    "section": "Kurtosis",
    "text": "Kurtosis\n\nmoments::kurtosis(df$return)\n\n[1] 2.879684\n\nmoments::kurtosis(df_spy$return)\n\n[1] 12.75883\n\n\nAgain, our fictious asset has kurtosis pretty close to perfect normality (almost 3). SPY deviate very much from normality and displays leptokurotic kurtosis."
  },
  {
    "objectID": "posts/quant-part1/normality-returns/index.html#shapiro-wilk-test",
    "href": "posts/quant-part1/normality-returns/index.html#shapiro-wilk-test",
    "title": "Normality of asset returns",
    "section": "Shapiro-Wilk test",
    "text": "Shapiro-Wilk test\nShapiro-Wilk test should actually not be used on large data set. Although, we use it here for demonstration purposes, results should be interpreted with a big spoon of salt.\nLet’s specify our hypothesis:\n\n\\(H_0\\): the data follows a normal distribution\n\\(H_1\\): the data does not follow a normal distribution\n\nLet’s first test on our fictitious equity.\n\nshapiro.test(df$return)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df$return\nW = 0.99895, p-value = 0.9215\n\n\nExpected, as the randomness of our fictitious stock was randomly distributed.\nAnd then on the return of SPY\n\nshapiro.test(df_spy$return)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df_spy$return\nW = 0.894, p-value < 2.2e-16"
  },
  {
    "objectID": "posts/quant-part1/binomials_models/index.html",
    "href": "posts/quant-part1/binomials_models/index.html",
    "title": "Binomials models for Quantitative Finance",
    "section": "",
    "text": "The idea is to develop an intuition for delta hedging and risk-neutrality when valuing an option.\n\n\n\nStock and Option Value perspectives\n\n\n\nS is the stock price at the start (time \\(t=0\\))\n\\(\\delta t\\) is a one increment of time (one unit of time)\n\\(u\\) is the factor when stock price rise\n\\(v\\) is the factor when stock price fall\n\n\\(0<v<1<u\\)\n\n\\(V\\) is the option value at time \\(t=0\\)\n\\(V^+\\) is the option value at expiration when stock is ITM\n\\(V^-\\) is the option value at expiration when stock is OTM\n\nNow we are going to introduce \\(\\Delta\\) as the amount of stock to hedge (a percentage of a stock) to be risk-neutral. At this stage, we are assuming that the probability to go up or down is the same (it’s basically irrelevant in this case).\n\n\n\nOption value with hedging\n\n\nIf we want to hedge the stock to be risk-neutral, then at expiration we should have this equation\n\\[V^+ - \\Delta us = V^--\\Delta vs\\] Solving for \\(\\Delta\\), we get:\n\\[\\Delta = \\frac{V^+-V^-}{(u-v)S} = \\frac{\\text{range of options payoff}}{\\text{range of asset prices}} \\tag{1}\\]\nIn other words we could see \\(\\Delta\\) as the rate of change of the option price in function of the stock price. \\(\\Delta = \\frac{\\partial{V}}{\\partial{S}}\\)\nOnce we found \\(\\Delta\\), we could find \\(V\\) by just making today’s value of the trade = tomorrow’s value of the trade (at expiration). Just solve for \\(V\\) \\[V - \\Delta S = V^- - \\Delta vS\\] or \\[V - \\Delta S = V^+ - \\Delta uS\\] which ever is easier to calculate.\nNow, of course, cash is not free and there is a time value associated to it. In that sense, today’s value for the trade should be equal a discounted value of tomorrow’s trade value (at expiration).\n\\[V - \\Delta S = \\frac{1}{1+r \\delta t} \\left(V^- - \\Delta vS \\right)\\]\n\n\\(r\\) is the value of the risk-free asset\nwe are dealing with annualized values, if assets expires in one month and risk-free asset is let’s say 3%, we would multiply 3% by 21 days or \\(0.03 \\cdot \\frac{21}{252}\\)\n\nUsing our value of \\(\\Delta\\) from Equation 1, we can isolate \\(V\\) as \\[V = \\left(\\frac{V^+-V^-}{u-v} \\right) + \\frac{1}{1+r \\delta t} \\left(V^- - \\Delta vS \\right) \\tag{2}\\]\n\n\n\n\n\n\nExample\n\n\n\nA stock is trading at $100. A call option with strike price of $100. Stock can either go to $103 or $98.\n\n\n\n\nflowchart LR \n  100 --> 103\n  100 --> 98\n\n\n\n\n\n\n\n\n\n\\(V^+ = 3\\)\n\\(V^- = 0\\)\n\\(\\Delta = \\frac{3-0}{103-98} = \\frac{3}{5}\\)\n\\(V - \\Delta S = V^+ - \\Delta us\\), plugging the value from above we get \\(V = \\$1.2\\)\n\n\n\n\n\nUsing the same idea as earlier and introducing some probabilities.\n\n\n\n\nflowchart LR\n  S -- p' --> uS\n  S -- 1-p' --> vS\n\n\n\n\n\n\n\n\nFrom a probabilistic perspective we could write: \\[S = p'uS + (1-p')vS\\] Or in the presence of a risk free asset, \\[S = \\frac{1}{1+r \\delta t} \\left(p' uS + (1-p')vS \\right)\\]\nWe could isolate \\(p'\\) in this last equation to get: \\[p' = \\frac{1+r \\delta t - v}{u-v} \\tag{3}\\]\n\n\n\n\nflowchart LR\n  V -- p' --> V+\n  V -- 1-p' --> V-\n\n\n\n\n\n\n\n\nTo find \\(V\\): \\[V = p' V^+ + (1-p') V^-\\]\nInteresting to note that the option price \\(V\\) is like an expectation (the sum of the probability) and \\(p'\\) is from Equation 3\n\n\n\nNow if we collide both world: the real-world with drift and volatility and the risk-free world with \\(p'\\): we can set up this 2 equations: One for the expected mean rate of change of prices and another for the variance of these rate of change.\n$$\n{\n\\[\\begin{aligned}\n  \\mu S \\delta t = puS+(1-p)vS-S \\\\\n  \\sigma^2S^2dt = S^2()^2\n\\end{aligned}\\]\n.\n$$\n(TODO re-write these 2 equations)\n2 equations and 3 unknowns, we can choose a solution (the prettiest one!) among the infinitely many:\n\n\\(u = 1 + \\sigma \\sqrt{\\delta t}\\)\n\\(v = 1 - \\sigma \\sqrt{\\delta t}\\)\n\\(p = \\frac{1}{2} + \\frac{\\mu \\sqrt{\\delta t}}{2 \\sigma}\\)\n\\(p' = \\frac{1}{2} + \\frac{r \\sqrt{\\delta t}}{2 \\sigma}\\) \\(p'\\) is the risk-neutral probability.\n\n\n\n\n\nAfter one time step \\(\\delta t\\) our stock, initially at \\(S\\) will either be at \\(uS\\) or \\(vS\\).\n\nAfter two time steps, the stock will either be at \\(u^2S\\) or \\(uvS\\) or \\(v^2S\\)\nAfter three time steps, the stock will either be at \\(u^3S\\) or \\(u^2vS\\) or \\(uv^2S\\) or finally \\(v^3S\\)\netc.\n\nA bit of a crude representation using Geogebra\n\n\n\nBinomial tree representation\n\n\nUsing the equations from the previous sections, we can now create a function to price the value of an option.\n\ncalculate_option_price <- function(spot, strike, rfr, sigma, time, steps) {\n  dt = time/steps            # get delta_t\n  u = 1 + sigma * sqrt(dt)   # multiplying factor when asset rise\n  v = 1 - sigma * sqrt(dt)\n  p_prime = 0.5 + (rfr * sqrt(dt)/(2*sigma))\n  discount_factor = 1 / (1 + rfr * dt)\n  \n  # Calculating vector of prices at maturity \n  s = rep(0, steps+1)   # initialize a vector for prices at maturity - (terminal nodes on the trees)\n  s[1] = spot * v^steps # #initialize the first end-price - (the most bottom right node of the tree)\n  for (i in 2:(steps+1)) { \n    s[i] = s[i-1] * u/v # this is the trick: to go up one leave = going back one step and then up\n  }\n  \n  # Calculating vector of options values at maturity \n  opt = rep(0, steps + 1)\n  for (i in 1:(steps+1)) { \n    opt[i] = max(0, s[i] - strike)\n  }\n  \n  # We have now to work backward and up in the trees (from bottom right and up)\n  # We know the final option value.\n  # We need know to calculate the options in the intermediates nodes\n  for (i in steps:1) {\n    for (j in 1:i) {\n      opt[j] = discount_factor * (p_prime * opt[j+1] + (1 - p_prime)*opt[j])\n    }\n  }\n  #print(glue::glue('Asset price for each time step:', s))\n  #print(glue::glue('Option price for each time step: ', opt))\n  return(opt)\n}\n\nLet’s try our function to get some results\n\ncalculate_option_price(spot = 100, strike = 100, rfr = 0.03, sigma = 0.1, time = 1, steps = 4)\n\n[1]  5.415051  8.117623 11.822154 16.506917 21.550625"
  },
  {
    "objectID": "posts/quant-part1/binomials_models/index.html#forward-equation",
    "href": "posts/quant-part1/binomials_models/index.html#forward-equation",
    "title": "Binomials models for Quantitative Finance",
    "section": "Forward equation",
    "text": "Forward equation\n\\[Prob(a<y'<b \\text{ at time t' } | \\text{ y at time t}) = \\int_a^b p(y, t; y', t') dy' \\tag{4}\\]\nThis (Equation 4) means: What is the probability that the random variable y’ lies between a and b at time t’ given it was at y at time t? In this case (y, t) are given, they are constant, they are known; while (y’, t’) are the variables.\nWe re-write this (Equation 4) for conciseness as \\(P(y, t; y', t')\\).\nHence, another way to write (Equation 4) is \\[P(y, t; y', t') = \\alpha \\cdot P(y, t; y'+\\delta y, t'-\\delta t) + (1-2\\alpha) P(y, t; y', t'-\\delta t) + \\alpha \\cdot P(y, t; y'-\\delta y, t' - \\delta t) \\tag{5}\\]\nEach terms in the sum of (Equation 5) could be evaluated using a Taylor Series Expansion. Note that \\(\\delta t^2 << \\delta t\\) as \\(\\delta t\\) is already quite small.\n\\[P(y, t; y' + \\delta y, t'-\\delta t)  \\approx P(y,t;y',t') + \\delta y \\frac{\\partial P}{\\partial y'} - \\delta t \\frac{\\partial P}{\\partial t'} + \\frac{1}{2} \\delta y^2 \\frac{\\partial^2 P}{\\partial y'^2} + \\dots\\] \\[P(y, t; y', t'-\\delta t)  \\approx P(y,t;y',t') - \\delta t \\frac{\\partial P}{\\partial t'} + \\dots\\]\n\\[P(y, t; y'- \\delta y, t'-\\delta t)  \\approx P(y,t;y',t') - \\delta y \\frac{\\partial P}{\\partial y'} - \\delta t \\frac{\\partial P}{\\partial t'} + \\frac{1}{2} \\delta y^2 \\frac{\\partial^2 P}{\\partial y'^2} + \\dots\\]\nWe have ignored all the less than \\(\\delta t\\).\nAdding the 3 equations above with their coefficients, we end up with\n\\[\\delta t \\frac{\\partial P}{\\partial t'} = \\alpha \\delta y^2 \\frac{\\partial^2 P}{\\partial y'^2}\\] \\[\\frac{\\partial P}{\\partial t'} = \\alpha \\frac{\\delta y^2}{\\delta t} \\frac{\\partial^2 P}{\\partial y'^2}\\]\nNote how \\(\\alpha\\), \\(\\delta t\\) and \\(\\delta y\\) are all positive values. Hence, we can let \\(C^2 = \\alpha \\frac{\\delta y^2}{\\delta t}\\), and we get: \\[\\frac{\\partial P}{\\partial t'} = C^2 \\frac{\\partial^2 P}{\\partial y'^2} \\tag{6}\\]\nThis last (Equation 6) can be recognized as the Forward Kolmogorov Equation or Heat-diffusion equation or also Fokker-Plank equation.\nNote that:\n\nThis is a PDE for p with 2 independent variables \\(y'\\) and \\(t'\\)\n\\(y\\) and \\(t\\) are like parameters. They are fixed, they are starting point\nThis should model a random-walk that is finite in a finite time.\n\nTo solve this PDE, we solve it by (as per the CQF) similarity reduction. We use a solution of the form \\[P = t'^a f \\left( \\frac{y'}{t'^b} \\right) \\space a, b \\in \\mathbb{R}\\]\nLetting \\(\\xi = \\frac{y'}{t'^b}\\), we are looking for a solution of the form \\[P = t'^a f(\\xi) \\]\nFinding the partial derivatives based on the above solution’s form.\n\\[\\frac{\\partial P}{\\partial y'} = t'^a \\cdot \\frac{df}{d \\xi} \\cdot \\frac{\\partial \\xi}{\\partial y'}\\] Note how f is just a function of \\(\\xi\\) while \\(\\xi\\) is a function of both \\(y'\\) and \\(t'\\); hence the difference in notation for the derivatives.\nSince \\(\\frac{\\partial \\xi}{\\partial y'} = t'^{-b}\\), we have \\[\\frac{\\partial P}{\\partial y'} = t'^{a-b} \\cdot \\frac{df}{d \\xi} \\]\nAlso, \\(\\frac{\\partial \\xi}{\\partial t'} = -b \\cdot y' \\cdot t'^{-b-1}\\). Using product rule to find \\(\\frac{\\partial P}{\\partial t'}\\), we get: \\[\\frac{\\partial P}{\\partial t'} = a t'^{a-1} f(\\xi) + t'^a \\frac{df}{d \\xi} \\frac{\\partial \\xi}{\\partial t'} = a \\cdot t'^{a-1} \\cdot f(\\xi) - b \\cdot t'^{a-b-1} \\cdot y' \\cdot \\frac{df}{d \\xi} \\]"
  },
  {
    "objectID": "posts/algebra-quant/index.html",
    "href": "posts/algebra-quant/index.html",
    "title": "Algebra For Quant",
    "section": "",
    "text": "I am storing here a few nuggets of algebra, I need for quantitative finance and machine learning.\n\n\\(e^x\\) as an infinite serie\nUsing the McLaurin series expansion, we can define \\(e^x\\) as an infinite sum.\nHere is how it goes: \\[e^x \\approx f(0) + f'(0) \\frac{x}{1} + f''(0) \\frac{x^2}{2!} + f'''(0) \\frac{x^3}{3!} + \\cdots + f^n(0) \\frac{x^n}{n!}\\]\nAs \\(f(0) = f'(0) = f''(0) = f^n(0) = e^0 = 1\\), we can rewrite our previous expression as\n\\[e^x \\approx 1 + \\frac{x}{1} + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots + \\frac{x^n}{n!}\\]\nHence: \\(e^x \\approx \\sum_{n=1}^\\infty \\frac{x^n}{n!}\\)"
  },
  {
    "objectID": "posts/quant-part1/random-behavior-assets/index.html",
    "href": "posts/quant-part1/random-behavior-assets/index.html",
    "title": "Random Behavior of Financial Assets",
    "section": "",
    "text": "One of the main pillar of quantitative finance is the assumption that assets’ returns behave in a random manner. Assets returns are normally distributed. It is a poor assumption as asset’s return are usually not normally distributed (fat tails, skewness, etc.), but it is one that is considered when approaching finance with a quantitative finance."
  },
  {
    "objectID": "posts/quant-part1/random-behavior-assets/index.html#ignoring-randomness",
    "href": "posts/quant-part1/random-behavior-assets/index.html#ignoring-randomness",
    "title": "Random Behavior of Financial Assets",
    "section": "Ignoring randomness",
    "text": "Ignoring randomness\n\\[R_i = \\frac{S_{i+1} - S_i}{S_i} = mean  = \\mu \\delta t\\] \\[S_{i+1} - S_i= S_i \\mu \\delta t\\] \\[S_{i+1} = S_i \\cdot (1 +  \\mu \\delta t) \\tag{3}\\]\nWe could also rewrite Equation 3 so it depends of the initial (starting) price, instead of the previous price.\n\\[S_n = S_0 (1+\\mu \\delta t)^n\\]\nUsing natural log:\n\\(S_n = S_0 e^{log (1+\\mu \\delta t)^n} = S_0 e^{n \\cdot log{(1+\\mu \\delta t)}}\\)\nWe could argue that \\(log(1+\\mu \\delta t) \\approx \\mu \\delta t\\) as \\(log(1+x) \\approx x\\) for small values of x.\n\\[S_n \\approx S_0 \\cdot e^{n \\mu \\delta t} \\tag{4}\\]\nNow, \\(n \\cdot \\delta t\\) is the same as \\(t\\). Hence,\n\n\n\n\n\n\n\\[S(t) \\approx S_0 \\cdot e^{\\mu t}\\]"
  },
  {
    "objectID": "posts/quant-part1/random-behavior-assets/index.html#considering-randomness",
    "href": "posts/quant-part1/random-behavior-assets/index.html#considering-randomness",
    "title": "Random Behavior of Financial Assets",
    "section": "Considering randomness",
    "text": "Considering randomness\nLet’s restart with Equation 2\n\\[R_i = \\frac{S_{i+1} - S_i}{S_i} = \\bar{R} + std \\cdot \\phi = \\mu \\delta t + \\sigma \\phi \\delta t^{1/2}\\] \\[S_{i+1} - S_i= S_i \\mu \\delta t + S_i \\sigma \\phi \\delta t^{1/2} \\tag{5}\\]\n\n\n\n\n\n\n\\[S_{i+1} = S_i \\cdot (1 +  \\mu \\delta t +  \\sigma \\phi \\sqrt{\\delta t}) \\tag{6}\\]\nThis last Equation 6 is the basis for Monte-Carlo simulation.\n\nNotice the standard deviation of return: \\(\\sigma \\sqrt{\\delta t}\\)\nunit of \\(\\mu = \\frac{1}{t}\\)\nunit of \\(\\sigma = \\frac{1}{\\sqrt{t}}\\)\nthis is because we can only add variance together (no sd). For independent variable X and Y: \\(Var(X+Y) = Var(X) + Var(Y)\\)\nthe standard deviation of returns scale up with the square root of the time step."
  },
  {
    "objectID": "posts/quant-part1/random-behavior-assets/index.html#going-to-continuous-time",
    "href": "posts/quant-part1/random-behavior-assets/index.html#going-to-continuous-time",
    "title": "Random Behavior of Financial Assets",
    "section": "Going to continuous time",
    "text": "Going to continuous time\nRestarting from Equation 5 : \\[S_{i+1} - S_i= S_i \\mu \\delta t + S_i \\sigma \\sqrt{\\delta t} \\phi\\]\n\n\\(S_{i+1} - S_i = dS\\)\n\\(S_i = S(t)\\)\n\\(\\delta t = dt\\)\n\\(\\phi \\sqrt{\\delta t} = dX\\) where \\(dX\\) is a random variable with mean = 0 and variance = dt. Hence \\(E[dX] = 0\\) and \\(E[(dX)^2] = dt\\)\n\n\n\n\n\n\n\n\\[dS = S \\mu dt + S \\sigma dX\\] This stochastic differential equation on the change of prices assume:\n\nreturns are treated as random\nreturns are assumed to be normally distributed (again not totally exact)\nprices (S) are modelled as a log-normal walk (SDE)\n\\(\\mu\\) is the drift rate or growth rate\nbecause of the different scaling of time (\\(t\\) and \\(\\sqrt{t}\\)), on a short time frame, drift is negligible and volatility matters."
  },
  {
    "objectID": "posts/quant-part1/random-behavior-assets/index.html#the-euler-maruyana-method-to-compute-the-sde",
    "href": "posts/quant-part1/random-behavior-assets/index.html#the-euler-maruyana-method-to-compute-the-sde",
    "title": "Random Behavior of Financial Assets",
    "section": "The Euler-Maruyana Method to compute the SDE",
    "text": "The Euler-Maruyana Method to compute the SDE\nWe start with Equation 6 : \\(S_{i+1} = S_i \\cdot (1 + \\mu \\delta t + \\sigma \\phi \\sqrt{\\delta t})\\)\n\n# create one simulation for price \nndays <- 252 \nprice <- c()\nprice[1] <- last(df$adjClose)\n\nphi = rnorm(ndays, mean = 0, sd = 1)\n\nfor (i in 2:ndays){ \n  price[i] = price[i-1] * (1 + mu * delta_t + sigma * phi[i] * sqrt(delta_t))\n}\n\nyo <- tibble(x = 1:ndays, price = price)\nggplot(yo, aes(x, price)) + \n  geom_line()\n\n\n\n\nWe can now create 100’s such simulations re-using previous code in a function.\n\ncreate_price_simul <- function(x) {\n  price <- c() \n  price[1] <- last(df$adjClose) \n  phi = rnorm(ndays, mean = 0, sd = 1) \n  for (i in 2:ndays){ \n    price[i] = price[i-1] * (1 + mu * delta_t + sigma * phi[i] * sqrt(delta_t)) \n    } \n  yo <- tibble(x = 1:ndays, price = price)\n  return(yo)\n}\n\nlibrary(purrr)      # map()\nlibrary(RColorBrewer)\n\nnum_of_simul <- 100\ndf1 <- tibble(simul_num = 1:num_of_simul) |> \n  mutate(prices = map(simul_num, create_price_simul))\n\nyo <- df1 |> unnest(cols = c(prices))\n\ngetPalette = colorRampPalette(brewer.pal(9, \"Set1\"))\ncolourCount = num_of_simul\nggplot(yo, aes(x, price, group = simul_num)) + \n  scale_fill_manual(values = colorRampPalette(brewer.pal(9, \"Accent\"))(colourCount)) +\n  geom_line(aes(color = simul_num)) + \n  theme(legend.position = 'none')\n\nWarning in brewer.pal(9, \"Accent\"): n too large, allowed maximum for palette Accent is 8\nReturning the palette you asked for with that many colors"
  },
  {
    "objectID": "posts/statistical-moments/index.html",
    "href": "posts/statistical-moments/index.html",
    "title": "Statistical Moments",
    "section": "",
    "text": "This post is about summarizing the various statistical moments when doing quantitative finance. The focus is on the asset returns. From a previous post, we already know that financial asset returns do not follow a normal distribution (too peaked at the mean and fat tails).\nWe’ll show these parameters using both R and python.\nWe’ll use the SPY as a low-ish vol asset and AMD as an equity with higher vol. We only use the last 5 years of data (from 2018 and beyond)\nLet’s first load our libraries and the 2 data frame worth of prices.\n\nlibrary(readr)    # read_csv()\nlibrary(dplyr)    # mutate(), filter()\nlibrary(lubridate)\nlibrary(ggplot2)\n\ndf_spy <- read_csv('../../raw_data/SPY.csv') |> \n  select(date, adjClose) |> \n  arrange(date) |> \n  mutate(return = log(adjClose / lag(adjClose))) |> \n  filter(date > '2018-01-01')\n\ndf_amd <- read_csv('../../raw_data/AMD.csv') |> \n  select(date, adjClose) |> \n  arrange(date) |> \n  mutate(return = log(adjClose / lag(adjClose))) |> \n  filter(date > '2018-01-01')\n\n\nMean\nThe mean is our first moment. We’ll cons\nMathematically speaking, we define the mean as \\[\\sum^{n}_{i=1} \\frac{r_i}{n}\\]\n\n\\(r_i\\) is the return of the i-th observation\n\nusing log return\n\n\\(n\\) is the number of observation\n\n\n\nStandard Deviation\nStandard deviation is the second moment.\n\\[\\sigma = \\sqrt{\\frac{\\sum(x_i - \\mu)^2}{n}}\\] In the case of a sample: \\(s = \\sqrt{\\frac{(x_i - \\bar{x})^2}{n-1}}\\)\nThe variance is the square of the standard deviation. There is this neat little alternative expression of variance (I like the way it sounds).\n\nVariance is the difference between the mean of the square and the square of the mean.\n\nHere is how it goes:\n\\[\\sigma^2 = \\frac{\\sum(x_i-\\mu)^2}{n} = \\frac{\\sum(x_i)^2}{n} - 2 \\frac{\\sum(x_i \\mu)}{n} + \\frac{\\sum(\\mu)^2}{n}\\]\nConsidering\n\n\\(\\frac{\\sum(x_i \\mu)}{n} = \\mu \\frac{\\sum(x_i)}{n} = \\mu \\mu=\\mu^2\\)\nand considering \\(\\frac{\\sum(\\mu)^2}{n} = n \\frac{\\mu^2}{n} = \\mu^2\\), we can re-write our variance like this\n\n\\[\\sigma^2 = \\frac{\\sum(x_i)^2}{n} - 2 \\mu^2 + \\mu^2 = \\frac{\\sum(x_i)^2}{n} - \\mu^2\\]\n\n\nCoefficient of variation\nThe coefficient of variation is not a statistical moment, but considering it is the ratio of the first 2 moments (ratio of sd to the mean), we include it here as well. It allows to compare together 2 different distributions (that have different mean and sd).\n\\[CV = \\frac{\\sigma}{\\mu}\\]\nOr in the case of a sample \\(CV = \\frac{s}{\\bar{x}}\\)\n\n\nSkewness\n\n\n\n\n\n\nSkewness\n\n\n\nSkewness is measure of asymmetry of a distribution (or actually lack of). How symmetric around the mean is the distribution? A standard normal distribution is perfectly symmetrical and has zero skew. Other examples of zero-skewed distributions are the T Distribution, the uniform distribution and the Laplace distribution. However, other distributions don’t have zero skew.\nIn a zero skew distribution, the mean = the median and the distribution is symmetric.\n\n\nIn a sense, skewness is quantifying for us how far is the median from the mean.\nMathematically, we define skewness as\n\\[\\frac{\\frac{1}{n} \\sum(x_i - \\bar{x})^3}{\\sqrt{ \\left( \\frac{1}{n} \\sum(x_i - \\bar{x})^2 \\right)^3}} \\tag{1}\\]\nIn the case of a sample, we’ll multiply Equation 2 by a factor of \\(\\frac{\\sqrt{n(n-1)}}{n-2}\\)\n\nskenwess = 0 ==> normallly distributed\n$ -0.5 skewness $ ==> moderately skew\n$$\n\n\n\nKurtosis\n\n\n\n\n\n\nKurtosis\n\n\n\nKurtosis is a measure that describes the shape of a distribution’s tails in relation to its overall shape. A distribution can be infinitely peaked with low kurtosis, and a distribution can be perfectly flat-topped with infinite kurtosis. Thus, kurtosis measures “tailedness,” not “peakedness.”\n\n\nBecause, we raised the difference of a data point to its mean to the 4th power, it is really the data points far away from the mean that do participate to the kurtosis.\n\n“increasing kurtosis is associated with the “movement of probability mass from the shoulders of a distribution into its center and tails.” (Someone important who got published and knew his stuff!)\n\nMathematically, we define kurtosis as\n\\[\\frac{\\frac{1}{n} \\sum(x_i - \\bar{x})^4}{ \\left( \\frac{1}{n} \\sum(x_i - \\bar{x})^2 \\right)^2} \\tag{2}\\]\nSome statistical packages are providing excess kurtosis by subtracting 3 to the kurtosis value. So for a data set that is perfectly normally distributed, we expect the excess kurtosis to be 0.\nThere are 3 categories of kurtosis: leptokurtic (positive excess kurtosis), mesokurtic (aka normal distribution), platykurtic. \n\nA kurtosis greater than 3 => leptokurtic\nA kurtosis around 3 => mesokurtic\nA kurtosis less than 3 => platykurtic\n\nA Student’s T distribution with degree of freedom 4 has infinite kurtosis (huge peak and tails + narrow shoulders)\nMost equities display a leptokurtic behavior (skinny at the mean - most returns are clustered around the mean) and narrow shoulders and fat tails.\n\n\nUsing Python\n\nimport numpy as np\nimport pandas as pd\n\namd = pd.read_csv('../../raw_data/AMD.csv')\n\nx = amd['adjClose']\n\nreturns_21d = np.log(x / x.shift(21)).dropna()\n\nmean_21dret = np.mean(returns_21d)\nstd_21dret = np.std(returns_21d)\n\nprint(\"The mean rolling 21 days return is: %s\" % round(mean_21dret, 5))\n\nThe mean rolling 21 days return is: -0.00594\n\nprint(\"The standard deviation of the rolling 21 days return is: %s\" %round(std_21dret, 5))\n\nThe standard deviation of the rolling 21 days return is: 0.1832\n\n\nThe standard deviation is quite bigger than the mean. An histogram of the returns will confirm that.\n\nimport matplotlib.pyplot as plt\n\nplt.hist(returns_21d, bins = 'rice', label = 'Rolling 21-days return')\n\n(array([  2.,   0.,   3.,   4.,   5.,  26.,  31.,  56.,  66., 117., 192.,\n       269., 357., 420., 607., 637., 644., 567., 464., 380., 206., 157.,\n       109.,  98.,  73.,  31.,  28.,  14.,  10.,   8.,   2.,   1.,   0.,\n         3.,   2.,   1.]), array([-0.7537718 , -0.70724263, -0.66071346, -0.6141843 , -0.56765513,\n       -0.52112596, -0.47459679, -0.42806762, -0.38153845, -0.33500928,\n       -0.28848011, -0.24195095, -0.19542178, -0.14889261, -0.10236344,\n       -0.05583427, -0.0093051 ,  0.03722407,  0.08375324,  0.1302824 ,\n        0.17681157,  0.22334074,  0.26986991,  0.31639908,  0.36292825,\n        0.40945742,  0.45598659,  0.50251575,  0.54904492,  0.59557409,\n        0.64210326,  0.68863243,  0.7351616 ,  0.78169077,  0.82821994,\n        0.8747491 ,  0.92127827]), <BarContainer object of 36 artists>)\n\nplt.show()\n\n\n\n\nWe are seeing a larger left tail with indeed the mean looking around 0. An other to visualize this and putting emphasis on the outliers would be to plot the returns on a box-and-whiskers plot.\n\nplt.boxplot(returns_21d, labels = ['Rolling 21-days return'])\n\n{'whiskers': [<matplotlib.lines.Line2D object at 0x7fe94921c430>, <matplotlib.lines.Line2D object at 0x7fe94921c790>], 'caps': [<matplotlib.lines.Line2D object at 0x7fe94921ca60>, <matplotlib.lines.Line2D object at 0x7fe94921cbe0>], 'boxes': [<matplotlib.lines.Line2D object at 0x7fe94921c2b0>], 'medians': [<matplotlib.lines.Line2D object at 0x7fe94921cfa0>], 'fliers': [<matplotlib.lines.Line2D object at 0x7fe94921d240>], 'means': []}\n\nplt.show()\n\n\n\n\n\nfrom scipy.stats import skew, skewtest\n\nskew(returns_21d)\n\n0.21432056577181507\n\n\nThe skew value is quite large and negative which confirms the fat left tail we saw on the histogram\n\nskewtest(returns_21d)\n\nSkewtestResult(statistic=6.477804949429641, pvalue=9.306654969735504e-11)\n\n\nVery small p-value. We reject the null-hypothesis. The distribution is not symetrical."
  }
]