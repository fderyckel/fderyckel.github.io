[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Series",
    "section": "",
    "text": "A series of posts related to the analysis of time-series.\n\n\n\nA series of posts centered on basic probability concepts essentials for the study of quantitative finance.\n\n\nA series of posts when starting quant finance. Basic mathematical concepts and related code in R and Python when starting quantitative finance.\n\n\n\nA series of posts on some introductory concepts of stochastic calculus.\n\n\n\nA series of posts on the Black-Schole Equation and derivative pricing.\n\n\n\nA series of posts on machine learning algorithms with a quant finance lens."
  },
  {
    "objectID": "blog.html#probability",
    "href": "blog.html#probability",
    "title": "Series",
    "section": "",
    "text": "A series of posts centered on basic probability concepts essentials for the study of quantitative finance.\n\n\nA series of posts when starting quant finance. Basic mathematical concepts and related code in R and Python when starting quantitative finance.\n\n\n\nA series of posts on some introductory concepts of stochastic calculus.\n\n\n\nA series of posts on the Black-Schole Equation and derivative pricing.\n\n\n\nA series of posts on machine learning algorithms with a quant finance lens."
  },
  {
    "objectID": "quant-part1.html",
    "href": "quant-part1.html",
    "title": "Series: Quant - Part 1",
    "section": "",
    "text": "01 - Random Behavior of Financial Assets\n\n\n\n\n\nWe explore one of the main assumption of quantitative finance: assets returns are random.\n\n\n\n\n\n\nApr 18, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n02 - Normality of asset returns\n\n\n\n\n\nChecking the normality of asset returns visually and quantitatively.\n\n\n\n\n\n\nApr 19, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n03 - Random-walks & Brownian Motions\n\n\n\n\n\nRW (discrete) and BM (continuous) constitute the way assets returns are modelled.\n\n\n\n\n\n\nJul 20, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n04 - Binomials models for Quantitative Finance\n\n\n\n\n\nCreating a basic or binomial model on pricing an option.\n\n\n\n\n\n\nApr 6, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n05 - Trinomials models for Quantitative Finance\n\n\n\n\n\nCreating a trinomial model and deriving the Forward Kolmogorov Equation.\n\n\n\n\n\n\nApr 8, 2023\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "quant-part3.html",
    "href": "quant-part3.html",
    "title": "Series: Quant - Part 3",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nBlack-Schole Equation\n\n\n\n\n\nDeriving the Black-Schole Equation and finding its solutions. Application with R and Python\n\n\n\n\n\n\nJul 18, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\nModeling Option prices using Monte-Carlo simulations\n\n\n\n\n\nUsing the BSE and Monte-Carlo Methods to value option prices\n\n\n\n\n\n\nAug 27, 2023\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A quantitative Finance Learning Journey",
    "section": "",
    "text": "This blog / Github website is mostly to keep track of quantitative finance concepts I am going through with the CQF and beyond.\n\nAbout\nMy name is François de Ryckel. I have grown up in Belgium then I emigrated from there to finish up my study and start working. I lived in several places over the last 20 years. I’m a math / philosophy teacher with some stunts at business. I created 2 companies in Zambia: a fruits farm and a fresh produce trading company.\n\nLived in Paris, France, for 2 years to complete my undergrad and start my master\nLived in Freiburg & Leipzig, Germany, for 3 years to complete my master and do some teaching gigs at Alliance Française, Leipzig Universitat and Leipzig International School\nLived in Dhaka, Bangladesh for 3 years to teach at the International School Dhaka\nLived in Zambia for 9 years to teach math, stats and philosophy at the American International School of Lusaka. I also started a citrus & mangoes farm (over 9,000 trees) and a produce (fresh fish, fruits, and meat) trading company\nLived Thuwal, Saudi Arabia on the shore of the beautiful Red Sea for 5 years. I worked as math teacher on the KAUST university Campus at TKS (The KAUST School)\nCurrently living in Bangkok (Thailand) as a curriculum coordinator for the KIS School.\n\nAs a teacher, I’m always looking for good examples to incorporate in my practices. Lately I am especially interested in machine learning applications in finance and education.\n\n\nEducation\n\nMaster in Philosophy, 2001\nParis I - Pantheon Sorbonne\n\n\nBachelor in Logic, 2000\nParis I - Pantheon Sorbonne\n\n\nBachelor in Philosophy, 1999\nParis I - Pantheon Sorbonne\n\n\n\nInterests\n\nForecasting, modeling and machine learning\nMath education\nOutdoor Living and active lifestyle (Swimming, Cycling, Running)"
  },
  {
    "objectID": "posts/quant-puzzle-01/index.html",
    "href": "posts/quant-puzzle-01/index.html",
    "title": "Quant Puzzle #01",
    "section": "",
    "text": "This is attempt to start a series of puzzles for quant. The source for these puzzles is this noteworthy substack on quant finance"
  },
  {
    "objectID": "posts/quant-puzzle-01/index.html#solving-using-r",
    "href": "posts/quant-puzzle-01/index.html#solving-using-r",
    "title": "Quant Puzzle #01",
    "section": "Solving using R",
    "text": "Solving using R\n\nUsing the mean annualized returns\n\nlibrary(dplyr)  # mutate(), tibble(), if_else(), group_by(), summarize()\nlibrary(purrr)  # pmap()\nlibrary(tidyr)  # unnest()\n\nnum_sim = 10000\n\n# Create a function to get yearly return (both average and compounded)\ncreate_simul_ret &lt;- function(mu, sigma, num_year) { \n  df = tibble(year_num = 1:num_year, \n              ret = rnorm(num_year, mean = mu, sd = sigma)) |&gt; \n    mutate(ann_ret = 1 + ret, \n           cum_ret = cumsum(ann_ret) / year_num,           # Function for mean annualized returns \n           compounded_ret = cumprod(ann_ret)^(1/year_num), # Function to get compounded returns\n           posit_cum_ret = if_else(cum_ret &gt; 1, 1, 0), \n           posit_comp_ret = if_else(compounded_ret &gt; 1, 1, 0))\n  return(df)\n}\n\n# To get all different random return numbers, I need to fetch the data from the row\n\ndf &lt;- tibble(sim_num = 1:num_sim, mu = 0.03, sigma = 0.1, num_year = 30) |&gt; \n  mutate(sim_ret = pmap(list(mu, sigma, num_year), create_simul_ret)) |&gt; \n  unnest(cols = c(sim_ret)) |&gt;      # large df with 30 rows/year per simulation\n  group_by(year_num) |&gt;             # group by year to calculate average returns\n  summarize(avg_num_pos_ret = mean(posit_cum_ret), \n            avg_num_comp_pos_ret = mean(posit_comp_ret))\n\n\n\n\n\n\n\n  \n    \n    \n      year_num\n      avg_num_pos_ret\n      avg_num_comp_pos_ret\n    \n  \n  \n    1\n0.6091\n0.6091\n    2\n0.6605\n0.6475\n    3\n0.6962\n0.6759\n    4\n0.7210\n0.6948\n    5\n0.7429\n0.7126\n    6\n0.7660\n0.7332\n    7\n0.7830\n0.7481\n    8\n0.7982\n0.7581\n    9\n0.8114\n0.7733\n    10\n0.8254\n0.7828\n    11\n0.8374\n0.7974\n    12\n0.8489\n0.8077\n    13\n0.8613\n0.8188\n    14\n0.8680\n0.8257\n    15\n0.8742\n0.8334\n    16\n0.8803\n0.8410\n    17\n0.8896\n0.8484\n    18\n0.8950\n0.8540\n    19\n0.8990\n0.8601\n    20\n0.9083\n0.8662\n    21\n0.9109\n0.8700\n    22\n0.9156\n0.8758\n    23\n0.9201\n0.8809\n    24\n0.9295\n0.8872\n    25\n0.9325\n0.8939\n    26\n0.9355\n0.8997\n    27\n0.9400\n0.9036\n    28\n0.9431\n0.9062\n    29\n0.9447\n0.9110\n    30\n0.9464\n0.9136"
  },
  {
    "objectID": "posts/quant-puzzle-01/index.html#solving-using-python",
    "href": "posts/quant-puzzle-01/index.html#solving-using-python",
    "title": "Quant Puzzle #01",
    "section": "Solving Using Python",
    "text": "Solving Using Python\n\nimport random\n#Computes a moving average from a gaussian sample over n trials\ndef gauss_moving_average(mu,sigma,n):\n    moving_average_list = []\n    for i in range(n):\n        if i == 0:\n            moving_average_list.append(random.gauss(mu,sigma))\n        else:\n            moving_average_list.append((moving_average_list[i-1]*i+random.gauss(mu,sigma))/(i+1))\n    return moving_average_list\n\n#Outputs a list of guassian moving average time series\ndef run_sims(mu, sigma, n, years):\n    sims = []\n    for i in range(n):\n        sims.append(gauss_moving_average(mu,sigma,years))\n    return sims\n#Outputs the percentage of positive entries in a column of a matrix\ndef percent_positive_column(matrix,column):\n    n = len(matrix)\n    percent_positive = 0\n    for i in range(n):\n        if matrix[i][column] &gt; 0:\n            percent_positive+=(1/n) \n    return percent_positive*100\n\n\n#Runs the simulation and checks for our confidence level\nmatrix = run_sims(3,10,100000, 100)\nfor i in range(100):\n    conf = percent_positive_column(matrix, i)\n    print(conf)\n    if  conf &gt;= 91.3:\n        print(\"Year\",i+1, \"has positive returns with probability\", conf)\n        break\n\n61.93999999998159\n66.66699999996007\n70.11999999994435\n72.92899999993158\n75.1649999999214\n77.24099999991195\n79.06299999990367\n80.68199999989629\n81.95799999989049\n83.17599999988494\n84.27499999987994\n85.2739999998754\n86.20499999987116\n87.1399999998669\n87.92999999986331\n88.63799999986009\n89.30399999985706\n90.13199999985329\n90.65399999985091\n91.18599999984849\n91.68499999984621\nYear 21 has positive returns with probability 91.68499999984621"
  },
  {
    "objectID": "posts/quant-part1/normality-returns/index.html",
    "href": "posts/quant-part1/normality-returns/index.html",
    "title": "02 - Normality of asset returns",
    "section": "",
    "text": "As mentioned in one of our previous posts, we know that in quantitative finance, assets returns are assumed to be random. That being said, they are actually not normally distributed. This post digg in a bit further in assessing the normality (or non-normality) of equity returns.\nAs reminder, a dataset can be said to be normally distributed if its probability density function can be modeled by \\[P(X = x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\\]\nWhen testing for normality, there are many ways to get there.\n\nVisual ways: histogram, density plot and QQ-plots\n\nUsing Skewness or Kurtosis\nStatistical tests such as the Shapiro-Wilk test (small to medium sample size, \\(n \\leq 300\\)) or the Kolmogorov-Smirnov test\n\nLet’s start by considering an ETF (low-ish volatility) like the SPY500.\nTo better illustrate our point and for comparative purposes, we’ll also consider a fictitious stock with the same standard deviation and mean returns as the SPY500 but this time, with random and almost perfectly normally distributed returns.\nFrom previous post, returns can be explained with a drift rate + some randomness: \\[R_i = \\mu \\delta t + \\phi \\sigma \\delta t^{1/2}\\]\n\n\\(\\phi\\) is a random number taken from the standard normal distribution."
  },
  {
    "objectID": "posts/quant-part1/normality-returns/index.html#histograms",
    "href": "posts/quant-part1/normality-returns/index.html#histograms",
    "title": "02 - Normality of asset returns",
    "section": "Histograms",
    "text": "Histograms\nLet’s see how well the returns stack to our imaginary stock (with close to perfect pseudo-randomness)\n\nggplot(df_dummy, aes(return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density() + \n  stat_function(fun = dnorm, n = nrow(df), args = list(mean = drift, sd = sigma), color = 'red', size = 1) +\n  scale_y_continuous() + \n  scale_x_continuous(limits = c(-0.055, 0.055), n.breaks = 9)\n\n\n\n\nThe black line is the actual density of returns, while the red line is the density of the normal distribution with same drift and volatility as earlier. Lines are pretty close to each other.\nAnd now onto the histogram on SPY (again same drift and volatility) as fictitious stock above.\n\nggplot(df_spy, aes(return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density() + \n  geom_vline(xintercept = drift+sigma, color = 'blue', linetype = 3, linewidth = 1) + \n  geom_vline(xintercept = drift-(0.6*sigma),  color = 'blue', linetype = 3, linewidth = 1) + \n  stat_function(fun = dnorm, n = nrow(df), args = list(mean = drift, sd = sigma), color = 'red', size = 1) +\n  scale_y_continuous() + \n  scale_x_continuous(limits = c(-0.055, 0.055), n.breaks = 9)\n\n\n\n\nAnd here, we clearly see the big disconnect from normality: above expected number of returns at the mean (aka too peaked), less returns next to the mean (between 1 and 2 or 2 1/2 sd) and then higher number of observations than expected in the tails (aka fat tails). Distribution of returns for equity are interesting in that sense: both too peaked and fat tails."
  },
  {
    "objectID": "posts/quant-part1/normality-returns/index.html#qq-plots",
    "href": "posts/quant-part1/normality-returns/index.html#qq-plots",
    "title": "02 - Normality of asset returns",
    "section": "QQ Plots",
    "text": "QQ Plots\nAnother way to visually check for normality is to use a quantile-quantile plot (aka QQ-plot). On the y-axis, we have the returns, on the x-axis the theoretical quantiles.\n\nggplot(df_dummy, aes(sample = return)) + \n  stat_qq() + \n  stat_qq_line(color = 'blue', linetype = 3, linewidth = 1) + \n  labs(title = 'QQ-Plot for fictious stock returns')\n\n\n\n\nAnd now the QQ-plot for the returs of SPY.\n\nggplot(df_spy, aes(sample = return)) + \n  stat_qq() + \n  stat_qq_line(color = 'blue', linetype = 3, linewidth = 1) + \n  labs(title = 'QQ-Plot for SPY returns')\n\n\n\n\nOh boy! Again, our second plot clearly indicate how the returns deviate from normality.\nThis QQ-plot can also be used to check for asymetry in the distribution of returns. We can see a slightly left skew distribution (a negatively skew distribution)."
  },
  {
    "objectID": "posts/quant-part1/normality-returns/index.html#skewness",
    "href": "posts/quant-part1/normality-returns/index.html#skewness",
    "title": "02 - Normality of asset returns",
    "section": "Skewness",
    "text": "Skewness\nIdeally, skewness as a measure of symmetry should be close to 0 (perfectly symmetric).\nLet’s test the symmetry of our 2 sets of returns. Unfortunately, we did not find any function to calculate skewness in base R (seems strange!).\n\nmoments::skewness(df_dummy$return)\n\n[1] 0.0398789\n\nmoments::skewness(df_spy$return)\n\n[1] -0.7418496\n\n\nAs expected, our fictitious stock has almost 0 skew (symmetric around the mean), while the SPY has a moderate negative skew (which we could see already on the QQ-plot and histogram.)"
  },
  {
    "objectID": "posts/quant-part1/normality-returns/index.html#kurtosis",
    "href": "posts/quant-part1/normality-returns/index.html#kurtosis",
    "title": "02 - Normality of asset returns",
    "section": "Kurtosis",
    "text": "Kurtosis\n\nmoments::kurtosis(df_dummy$return)\n\n[1] 2.879684\n\nmoments::kurtosis(df_spy$return)\n\n[1] 12.75883\n\n\nAgain, our fictitious asset has kurtosis pretty close to perfect normality (almost 3). SPY deviate very much from normality and displays leptokurotic kurtosis.\n\n\n\n\n\n\nNote\n\n\n\nIn this post on the statistical moments, we have showed a couple of transformation methods (log transform and Box-Cox transform) to normalize data."
  },
  {
    "objectID": "posts/quant-part1/normality-returns/index.html#shapiro-wilk-test",
    "href": "posts/quant-part1/normality-returns/index.html#shapiro-wilk-test",
    "title": "02 - Normality of asset returns",
    "section": "Shapiro-Wilk test",
    "text": "Shapiro-Wilk test\nShapiro-Wilk test should actually not be used on large data set. Although, we use it here for demonstration purposes, results should be interpreted with a big spoon of salt.\nLet’s specify our hypothesis:\n\n\\(H_0\\): the data follows a normal distribution\n\\(H_1\\): the data does not follow a normal distribution\n\nLet’s first test on our fictitious equity.\n\nshapiro.test(df_dummy$return)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df_dummy$return\nW = 0.99895, p-value = 0.9215\n\n\nExpected, as the randomness of our fictitious stock was randomly distributed.\nAnd then on the return of SPY\n\nshapiro.test(df_spy$return)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df_spy$return\nW = 0.894, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "posts/quant-part1/binomials_models/index.html",
    "href": "posts/quant-part1/binomials_models/index.html",
    "title": "04 - Binomials models for Quantitative Finance",
    "section": "",
    "text": "The idea is to develop an intuition for delta hedging and risk-neutrality when valuing an option.\n\n\n\nStock and Option Value perspectives\n\n\n\nS is the stock price at the start (time \\(t=0\\))\n\\(\\delta t\\) is a one increment of time (one unit of time)\n\\(u\\) is the factor when stock price rise\n\\(v\\) is the factor when stock price fall\n\n\\(0&lt;v&lt;1&lt;u\\)\n\n\\(V\\) is the option value at time \\(t=0\\)\n\\(V^+\\) is the option value at expiration when stock is ITM (case of a call option)\n\\(V^-\\) is the option value at expiration when stock is OTM (case of a call option)\n\n\n\nNow we are going to introduce \\(\\Delta\\) as the amount of stock to hedge (a percentage of a stock) to be risk-free. Because the portfolio is risk-free it must return the risk-free rate to prevent arbitrage.\nWe can model our stock and option as part of a portfolio (we call it \\(\\Pi\\)). Hence, \\[\\Pi = V - \\Delta \\cdot S\\] At this stage, we are assuming that the probability to go up or down is the same (it’s basically irrelevant in this case).\n\n\n\nOption value with delta hedging\n\n\nIf we want to hedge the stock to be risk-free, then at expiration we should have this equation\n\\[V^+ - \\Delta us = V^--\\Delta vs\\] Solving for \\(\\Delta\\), we get:\n\\[\\Delta = \\frac{V^+-V^-}{(u-v)S} = \\frac{\\text{range of options payoff}}{\\text{range of asset prices}} \\tag{1}\\]\nIn other words, we could see \\(\\Delta\\) as the rate of change of the option price in function of the stock price. \\(\\Delta = \\frac{\\partial{V}}{\\partial{S}}\\)\nOnce we found \\(\\Delta\\), we could find \\(V\\) by just making today’s value of the trade = tomorrow’s value of the trade (at expiration). Just solve for \\(V\\) \\[V - \\Delta S = V^- - \\Delta vS\\] or \\[V - \\Delta S = V^+ - \\Delta uS\\] which ever is easier to calculate.\nNow, of course, cash is not free and there is a time value associated to it. In that sense, today’s value for the trade should be equal a discounted value of tomorrow’s trade value (at expiration).\nThe thinking is that if we want to hedge to be risk-free, it should return the risk-free rate otherwise there is an arbitrate opportunity. \\[\\Pi = V - \\Delta S = \\frac{1}{1+r \\delta t} \\left(V^- - \\Delta vS \\right)\\]\n\n\\(r\\) is the value of the risk-free asset\nwe are dealing with annualized values, if assets expires in one month and risk-free asset is let’s say 3%, we would multiply 3% by 21 days or \\(0.03 \\cdot \\frac{21}{252}\\)\n\nUsing our value of \\(\\Delta\\) from Equation 1, we can isolate \\(V\\) as \\[V = \\left(\\frac{V^+-V^-}{u-v} \\right) + \\frac{1}{1+r \\delta t} \\left(V^- - \\Delta vS \\right) \\tag{2}\\]\nWith delta-hedging, we say that the portfolio is deterministic (aka risk-free)\n\n\n\n\n\n\nExample\n\n\n\nA stock is trading at $100. A call option with strike price of $100. Stock can either go to $103 or $98.\n\n\n\n\nflowchart LR \n  100 --&gt; 103\n  100 --&gt; 98\n\n\n\n\n\n\n\\(V^+ = 3\\)\n\\(V^- = 0\\)\n\\(\\Delta = \\frac{3-0}{103-98} = \\frac{3}{5}\\)\n\\(V - \\Delta S = V^+ - \\Delta us\\), plugging the value from above we get \\(V = \\$1.2\\)\n\n\n\n\n\n\nUsing the same idea as earlier and introducing some probabilities. \\(p'\\) is the risk-neutral probability.\n\n\n\n\nflowchart LR\n  S -- p' --&gt; uS\n  S -- 1-p' --&gt; vS\n\n\n\n\n\nFrom a probabilistic perspective we could write: \\[S = p'uS + (1-p')vS\\] Or in the presence of a risk free asset, \\[S = \\frac{1}{1+r \\delta t} \\left(p' uS + (1-p')vS \\right)\\]\nWe could isolate \\(p'\\) in this last equation: \\[s + s r \\delta t = p' u s + vs - p'v s\\] \\[p' = \\frac{s + s r \\delta t - vs}{us - vs}\\] \\[p' = \\frac{1+r \\delta t - v}{u-v} \\tag{3}\\]\n\n\n\n\nflowchart LR\n  V -- p' --&gt; V+\n  V -- 1-p' --&gt; V-\n\n\n\n\n\nTo find \\(V\\): \\[V = p' V^+ + (1-p') V^-\\]\nInteresting to note that the option price \\(V\\) is like an expectation (the sum of the probability) and \\(p'\\) is from Equation 3\nActually, we should also include the discounted factor (V is the present value of the expectation): \\[V = \\frac{1}{1 + r \\delta t} \\cdot \\left( p' V^+ + (1-p') V^- \\right) \\tag{4}\\]\n\n\n\nNow if we collide both world: the real-world with drift and volatility (Equation 2) and the risk-free world (Equation 4) with \\(p'\\): we can set up this 2 equations: One for the expected mean rate of change of prices and another for the variance of these rate of change.\n\\[\n\\begin{equation}\n  \\begin{cases}\n    \\mu S \\delta t = puS+(1-p)vS-S \\\\\n    \\sigma^2S^2dt = S^2 (p \\cdot [u-1-(pu + (1-p)v-1)]^2 + (1-p) \\cdot [v-1-(pu + (1-p)v-1)]^2)\n  \\end{cases}\n\\end{equation}\n\\]\n(TODO re-write these 2 equations)\n2 equations and 3 unknowns, we can choose a solution (the prettiest one!) among the infinitely many:\n\n\\(u = 1 + \\sigma \\sqrt{\\delta t}\\)\n\\(v = 1 - \\sigma \\sqrt{\\delta t}\\)\n\\(p = \\frac{1}{2} + \\frac{\\mu \\sqrt{\\delta t}}{2 \\sigma}\\)\n\\(p' = \\frac{1}{2} + \\frac{r \\sqrt{\\delta t}}{2 \\sigma}\\) \\(p'\\) is the risk-neutral probability."
  },
  {
    "objectID": "posts/quant-part1/binomials_models/index.html#delta-hedging-and-no-arbitrage",
    "href": "posts/quant-part1/binomials_models/index.html#delta-hedging-and-no-arbitrage",
    "title": "04 - Binomials models for Quantitative Finance",
    "section": "",
    "text": "Now we are going to introduce \\(\\Delta\\) as the amount of stock to hedge (a percentage of a stock) to be risk-free. Because the portfolio is risk-free it must return the risk-free rate to prevent arbitrage.\nWe can model our stock and option as part of a portfolio (we call it \\(\\Pi\\)). Hence, \\[\\Pi = V - \\Delta \\cdot S\\] At this stage, we are assuming that the probability to go up or down is the same (it’s basically irrelevant in this case).\n\n\n\nOption value with delta hedging\n\n\nIf we want to hedge the stock to be risk-free, then at expiration we should have this equation\n\\[V^+ - \\Delta us = V^--\\Delta vs\\] Solving for \\(\\Delta\\), we get:\n\\[\\Delta = \\frac{V^+-V^-}{(u-v)S} = \\frac{\\text{range of options payoff}}{\\text{range of asset prices}} \\tag{1}\\]\nIn other words, we could see \\(\\Delta\\) as the rate of change of the option price in function of the stock price. \\(\\Delta = \\frac{\\partial{V}}{\\partial{S}}\\)\nOnce we found \\(\\Delta\\), we could find \\(V\\) by just making today’s value of the trade = tomorrow’s value of the trade (at expiration). Just solve for \\(V\\) \\[V - \\Delta S = V^- - \\Delta vS\\] or \\[V - \\Delta S = V^+ - \\Delta uS\\] which ever is easier to calculate.\nNow, of course, cash is not free and there is a time value associated to it. In that sense, today’s value for the trade should be equal a discounted value of tomorrow’s trade value (at expiration).\nThe thinking is that if we want to hedge to be risk-free, it should return the risk-free rate otherwise there is an arbitrate opportunity. \\[\\Pi = V - \\Delta S = \\frac{1}{1+r \\delta t} \\left(V^- - \\Delta vS \\right)\\]\n\n\\(r\\) is the value of the risk-free asset\nwe are dealing with annualized values, if assets expires in one month and risk-free asset is let’s say 3%, we would multiply 3% by 21 days or \\(0.03 \\cdot \\frac{21}{252}\\)\n\nUsing our value of \\(\\Delta\\) from Equation 1, we can isolate \\(V\\) as \\[V = \\left(\\frac{V^+-V^-}{u-v} \\right) + \\frac{1}{1+r \\delta t} \\left(V^- - \\Delta vS \\right) \\tag{2}\\]\nWith delta-hedging, we say that the portfolio is deterministic (aka risk-free)\n\n\n\n\n\n\nExample\n\n\n\nA stock is trading at $100. A call option with strike price of $100. Stock can either go to $103 or $98.\n\n\n\n\nflowchart LR \n  100 --&gt; 103\n  100 --&gt; 98\n\n\n\n\n\n\n\\(V^+ = 3\\)\n\\(V^- = 0\\)\n\\(\\Delta = \\frac{3-0}{103-98} = \\frac{3}{5}\\)\n\\(V - \\Delta S = V^+ - \\Delta us\\), plugging the value from above we get \\(V = \\$1.2\\)"
  },
  {
    "objectID": "posts/quant-part1/binomials_models/index.html#the-risk-neutral",
    "href": "posts/quant-part1/binomials_models/index.html#the-risk-neutral",
    "title": "04 - Binomials models for Quantitative Finance",
    "section": "",
    "text": "Using the same idea as earlier and introducing some probabilities. \\(p'\\) is the risk-neutral probability.\n\n\n\n\nflowchart LR\n  S -- p' --&gt; uS\n  S -- 1-p' --&gt; vS\n\n\n\n\n\nFrom a probabilistic perspective we could write: \\[S = p'uS + (1-p')vS\\] Or in the presence of a risk free asset, \\[S = \\frac{1}{1+r \\delta t} \\left(p' uS + (1-p')vS \\right)\\]\nWe could isolate \\(p'\\) in this last equation: \\[s + s r \\delta t = p' u s + vs - p'v s\\] \\[p' = \\frac{s + s r \\delta t - vs}{us - vs}\\] \\[p' = \\frac{1+r \\delta t - v}{u-v} \\tag{3}\\]\n\n\n\n\nflowchart LR\n  V -- p' --&gt; V+\n  V -- 1-p' --&gt; V-\n\n\n\n\n\nTo find \\(V\\): \\[V = p' V^+ + (1-p') V^-\\]\nInteresting to note that the option price \\(V\\) is like an expectation (the sum of the probability) and \\(p'\\) is from Equation 3\nActually, we should also include the discounted factor (V is the present value of the expectation): \\[V = \\frac{1}{1 + r \\delta t} \\cdot \\left( p' V^+ + (1-p') V^- \\right) \\tag{4}\\]"
  },
  {
    "objectID": "posts/quant-part1/binomials_models/index.html#mixing-real-world-and-risk-free-world",
    "href": "posts/quant-part1/binomials_models/index.html#mixing-real-world-and-risk-free-world",
    "title": "04 - Binomials models for Quantitative Finance",
    "section": "",
    "text": "Now if we collide both world: the real-world with drift and volatility (Equation 2) and the risk-free world (Equation 4) with \\(p'\\): we can set up this 2 equations: One for the expected mean rate of change of prices and another for the variance of these rate of change.\n\\[\n\\begin{equation}\n  \\begin{cases}\n    \\mu S \\delta t = puS+(1-p)vS-S \\\\\n    \\sigma^2S^2dt = S^2 (p \\cdot [u-1-(pu + (1-p)v-1)]^2 + (1-p) \\cdot [v-1-(pu + (1-p)v-1)]^2)\n  \\end{cases}\n\\end{equation}\n\\]\n(TODO re-write these 2 equations)\n2 equations and 3 unknowns, we can choose a solution (the prettiest one!) among the infinitely many:\n\n\\(u = 1 + \\sigma \\sqrt{\\delta t}\\)\n\\(v = 1 - \\sigma \\sqrt{\\delta t}\\)\n\\(p = \\frac{1}{2} + \\frac{\\mu \\sqrt{\\delta t}}{2 \\sigma}\\)\n\\(p' = \\frac{1}{2} + \\frac{r \\sqrt{\\delta t}}{2 \\sigma}\\) \\(p'\\) is the risk-neutral probability."
  },
  {
    "objectID": "posts/markov-chains/index.html",
    "href": "posts/markov-chains/index.html",
    "title": "Markov Chains",
    "section": "",
    "text": "Introduction to Markov Chains\nA Markov chain is a random process with the Markov property. A random process or often called stochastic property is a mathematical object defined as a collection of random variables. A Markov chain has either discrete state space (set of possible values of the random variables) or discrete index set (often representing time) - given the fact, many variations for a Markov chain exists. Usually the term “Markov chain” is reserved for a process with a discrete set of times, that is a Discrete Time Markov chain (DTMC).\nTo develop better intuition about Markov chain, the simpler version of it is to model a basic random walk."
  },
  {
    "objectID": "posts/markov-chains/index.html#from-scratch",
    "href": "posts/markov-chains/index.html#from-scratch",
    "title": "Markov Chains",
    "section": "From scratch",
    "text": "From scratch\n\nimport numpy as np\n\nstart = 0\ny = []\nn = 1000\n\nfor i in range(n): \n  step = np.random.choice([-1, 1], p = [0.5, 0.5])\n  start += step\n  y.append(start)\n\n\nimport matplotlib.pyplot as plt\n\nplt.plot(y)\n#plt.xlabel('Number of steps')\n#plt.ylabel(r'$S_{n}$')\n\n\n\n\nA random walk with a 1000 steps and equal probabilty to go left or right"
  },
  {
    "objectID": "posts/markov-chains/index.html#using-financial-data",
    "href": "posts/markov-chains/index.html#using-financial-data",
    "title": "Markov Chains",
    "section": "Using financial data",
    "text": "Using financial data\nPython code coming from this post\nA Monte-Carlo simulation of a random-walk of a stock price does assume that the returns follow a normal distribution. A second assumption is that the past volatility of returns will continue (or be very similar) in the future. This is of course not totally the case.\n\nGetting data and feature engineeriing\nGetting data using the yfinance package.\n\nimport yfinance as yf\nimport pandas as pd\n\nyo = yf.download(\"SBUX\", start = \"2005-01-01\")\n\nyo.to_csv(\"../../raw_data/sbux.csv\")\n\nyo['Adj Close'][-1]\n\nyo.info()\n\nWe have imported SBUX stock price data and stored them on disk. We’ll retrieve it using pandas and construct our returns and volatility variables.\n\nimport pandas as pd\n\nsbux = pd.read_csv(\"../../raw_data/sbux.csv\")\n\nsbux.tail() \n\n# get the daily returns and then filter on the last 2 years of trading. \n# calculate volatiliy on these last years (not realistic of course)\ndaily_returns = sbux['adjClose'].pct_change()\n#sbux = sbux.tail(505)\ndaily_volat = daily_returns.std()\n\nprint(daily_volat)\n\nsbux.info()\n\n0.01987813423096984\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5607 entries, 0 to 5606\nData columns (total 13 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   date              5607 non-null   object \n 1   open              5607 non-null   float64\n 2   high              5607 non-null   float64\n 3   low               5607 non-null   float64\n 4   close             5607 non-null   float64\n 5   adjClose          5607 non-null   float64\n 6   volume            5607 non-null   int64  \n 7   unadjustedVolume  5607 non-null   int64  \n 8   change            5607 non-null   float64\n 9   changePercent     5607 non-null   float64\n 10  vwap              5607 non-null   float64\n 11  label             5607 non-null   object \n 12  changeOverTime    5607 non-null   float64\ndtypes: float64(9), int64(2), object(2)\nmemory usage: 569.6+ KB\n\n\n\n\nSingle simulation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlook_back = 252 \ncount = 0\nprice_list = []\nlast_price = sbux['adjClose'].iloc[-1]\n\nprice = last_price * (1 + np.random.normal(0, daily_volat)) \nprice_list.append(price)\n\nfor y in range(look_back): \n  if count == 251: \n    break\n  price = price_list[count] * (1 + np.random.normal(0, daily_volat))\n  price_list.append(price) \n  count += 1\n  \nplt.plot(price_list)\nplt.show()\n\n#price_list\n\n\n\n\nAn here would be another single simulation. It will of course look vastly different although it is build from the same normal distribution with same mean \\(\\mu = 0\\) and sd \\(\\sigma = 0\\).\n\n\n\n\n\nNow we can re-use that code if we want to create 100’s of these simulations.\n\nnum_of_simulations = 100 \nmodel_ahead = 252 \n\ndf = pd.DataFrame()\nlast_price_list = []\n\nfor x in range(num_of_simulations): \n  count = 0\n  price_list = []\n  last_price = sbux.iloc[-1]['adjClose'] \n  price = last_price * (1 + np.random.normal(0, daily_volat)) \n  price_list.append(price) \n  \n  for y in range(model_ahead): \n    if count == 251: \n      break\n    price = price_list[count] * (1 + np.random.normal(0, daily_volat)) \n    price_list.append(price) \n    count += 1\n  \n  df[x] = price_list\n  last_price_list.append(price_list[-1])\n    \n  \nfig = plt.figure()\nfig.suptitle(\"Monte Carlo Simulation for SBUX\") \nplt.plot(df)\nplt.xlabel(\"Day\")\nplt.ylabel(\"Price\")\nplt.show()\n\n\n\n\nWith just 10 simulated random-walks on SBUX given the last 17 years of volatility, we can see that price could range between $40 to around $140 dollars over the next 252 trading days (one year).\n\n\nAnalysis of our MC simulation\n\nprint(\"Expected price: \", round(np.mean(last_price_list), 2))\nprint(\"Quantile (5%): \", np.percentile(last_price_list, 5))\nprint(\"Quantile (95%): \", np.percentile(last_price_list, 95))\n\nExpected price:  4.34\nQuantile (5%):  2.5041396468319865\nQuantile (95%):  6.967420228662214\n\n\n\nplt.hist(last_price_list, bins=10) \nplt.show()"
  },
  {
    "objectID": "posts/markov-chains/index.html#transition-matrix",
    "href": "posts/markov-chains/index.html#transition-matrix",
    "title": "Markov Chains",
    "section": "Transition Matrix",
    "text": "Transition Matrix\nIn a transition matrix, the rows are you starting state and columns are your end of state.\nSo with the below matrix, the probability to go from state A to state A is 0.8 and probability to go from state A to state D is 0.2. In this sense, all the rows of a transition matrix should always add up to 1.\n\nstate_A = [0.1, 0.4, 0.3, 0.2, 0]\nstate_B = [0.0, 0.5, 0.5, 0.0, 0]\nstate_C = [0.0, 0.0, 1.0, 0.0, 0]\nstate_D = [0.0, 0.0, 0.0, 0.0, 1.0]\nstate_E = [0.0, 0.0, 0.0, 0.5, 0.5]\n\ntransition_matrix = [state_A, state_B, state_C, state_D, state_E]\n\nWe could also create a function to check if a transition matrix is indeed a properly formatted transition matrix to model a markov chain.\n\ndef check_markov_chain(m): \n  for i in range(0, len(m)): \n    if sum(m[i]) != 1: \n      return False\n  print(\"This is a correctly formatted transition matrix\") \n  \ncheck_markov_chain(transition_matrix)\n\nThis is a correctly formatted transition matrix"
  },
  {
    "objectID": "posts/lin-algebra-quant/index.html",
    "href": "posts/lin-algebra-quant/index.html",
    "title": "Linear Algebra for Quantitative Finance",
    "section": "",
    "text": "On this post, I am just sharing some linear algebra tools and tricks useful in quantitative finance. This is mainly a post for myself to have a place where I can remember them for when I need them.\n\nFinding the inverse of a 3x3 matrix\n\nFind the transpose of the cofactor matrix.\n\nfor each element of the matrix, find its minor (cross the row \\(i\\) and column \\(j\\) for element \\(ij\\), and find the determinant of the square matrix left)\nalternate the signs (diagonals are positive) \\(\\begin{pmatrix} + & - & + \\\\ - & + & - \\\\ + & - & + \\\\ \\end{pmatrix}\\)\n\nFind the determinant of the 3x3 matrix\n\n\\(a_{1,1} \\cdot (\\mbox{ cofactor of } a_{1,1}) - a_{1,2} \\cdot (\\mbox{ cofactor of } a_{1,2}) + a_{1,3} \\cdot (\\mbox{ cofactor of } a_{1,3})\\)\n\n\n\n\nGoing from Correlation to Covariance matrix\nHow to go from the correlation matrix and standard deviation vector to the covariance matrix?\nThe standard deviation vector is defined as \\(\\sigma = \\pmatrix{\\sigma_1 \\\\ \\sigma_2 \\\\ \\vdots \\\\ \\sigma_n}\\)\nThe correlation matrix is defined as \\[ R = \\begin{pmatrix} 1 & \\rho_{12} & \\cdots & \\rho_{1n} \\\\\n                       \\rho_{21} & 1 & \\cdots & \\rho_{2n} \\\\\n                        \\vdots & \\vdots & \\ddots \\\\\n                        \\rho_{n1} & \\rho_{n2} & \\cdots & 1\n                                    \\end{pmatrix} \\]\nwhere \\(\\rho_{ij}\\) is the correlation between returns of asset \\(i\\) and asset \\(j\\)\nwe create a diagonal matrix from the standard deviation vector.\n\\[ S = D(\\sigma) = \\begin{pmatrix} \\sigma_1 & 0 & \\cdots & 0 \\\\\n                                    0 & \\sigma_2 & \\cdots & 0 \\\\\n                                    \\vdots & \\vdots & \\ddots \\\\\n                                    0 & 0 & \\cdots & \\sigma_n\n                                    \\end{pmatrix} \\] (aka all other entries being 0)\nIn R, we use the function diag(x) with x being a vector! Note that S is symmetric, and so \\(S = S^T\\)\nTo get the covariance matrix \\(\\Sigma\\), we’ll pre & post-multiply the correlation matrix by the diagonal of standard deviation. Hence: \\[ S \\cdot R \\cdot S = \\Sigma = \\begin{pmatrix}\n\\sigma_1^2 & \\rho_{12} \\sigma_1 \\sigma2 & \\cdots & \\rho_{1n} \\sigma_1 \\sigma_n  \\\\\n\\rho_{21} \\sigma_2 \\sigma1 & \\sigma2^2 & \\cdots & \\rho_{2n} \\sigma_2 \\sigma_n \\\\\n\\vdots & \\vdots & \\ddots \\\\\n\\rho_{n1} \\sigma_n \\sigma_1 & \\rho_{n2} \\sigma_n \\sigma_2 & \\cdots & \\sigma_n^2\n                                    \\end{pmatrix} \\]"
  },
  {
    "objectID": "posts/quant-part3/black-schole/index.html#european-vanilla",
    "href": "posts/quant-part3/black-schole/index.html#european-vanilla",
    "title": "Black-Schole Equation",
    "section": "European Vanilla",
    "text": "European Vanilla\n\\[\\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S \\frac{\\partial^2 V}{\\partial S^2} + r S \\frac{\\partial V}{\\partial S} - rV = 0\\]\n\nV is the option price at the time \\(t\\). So \\(V = V(S, t)\\)\n\nS is the asset spot price\nt is the time to expiry (in years)\n\\(\\sigma\\) is the asset diffusion term (its stochastic element)\n\\(r\\) is the annualized continuously compounded risk-free rate (imaginary friend)\n\nIn the case of a European Call Option with no-dividend, the BSE has solution:\n\\[C = S N(d_1) - K e^{-r(T-t)} N(d_2)\\]\nAnd in the case of a European Put Option with no-dividend, the BSE has solution: \\[P = K e^{-r(T-t)}N(-d_2) - SN(-d_1)\\]\nwhere, \\[d_1 = \\frac{1}{\\sigma \\sqrt{T-t}} \\left[ ln \\left( \\frac{S}{K} \\right) + \\left( r + \\frac{\\sigma^2}{2} \\right) (T-t) \\right]\\]\n\\[d_2 = \\frac{1}{\\sigma \\sqrt{T-t}} \\left[ ln \\left( \\frac{S}{K} \\right) + \\left( r - \\frac{\\sigma^2}{2} \\right) (T-t) \\right] = d1 - \\sigma \\sqrt{T-t}\\]\n\\[N(x) = \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{x} e^{\\frac{-1}{2} x^2} dx\\]\n\nK is the strike price"
  },
  {
    "objectID": "posts/quant-part3/black-schole/index.html#european-vanilla-with-dividends",
    "href": "posts/quant-part3/black-schole/index.html#european-vanilla-with-dividends",
    "title": "Black-Schole Equation",
    "section": "European Vanilla with Dividends",
    "text": "European Vanilla with Dividends\nAnd in the case of a dividend (ok … assuming continuous dividend yield):\n\\[C = S e^{-D(T-t)} N(d_1) - K e^{-r(T-t)} N(d_2)\\]\n\\[P = K e^{-r(T-t)}N(-d_2) - Se^{-D(T-t)}N(-d_1)\\]\n\\[d_1 = \\frac{1}{\\sigma \\sqrt{T-t}} \\left[ ln \\left( \\frac{S}{K} \\right) + \\left( r - D + \\frac{\\sigma^2}{2} \\right) (T-t) \\right]\\]\n\\[d_2 = \\frac{1}{\\sigma \\sqrt{T-t}} \\left[ ln \\left( \\frac{S}{K} \\right) + \\left( r - D - \\frac{\\sigma^2}{2} \\right) (T-t) \\right] = d1 - \\sigma \\sqrt{T-t}\\]"
  },
  {
    "objectID": "posts/quant-part3/black-schole/index.html#using-python",
    "href": "posts/quant-part3/black-schole/index.html#using-python",
    "title": "Black-Schole Equation",
    "section": "Using Python",
    "text": "Using Python\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt \n\n\nclass option_pricing: \n  \n  \"\"\"\n  To price European Style options without dividends\n  \"\"\"\n  \n  def __init__(self, spot, strike, rate, dte, sigma): \n    \n    # assign our variables\n    self.spot = spot\n    self.strike = strike\n    self.rate = rate\n    self.dte = dte    # days to expiration (in years)\n    self.sigma = sigma\n    \n    # to avoid zero division, let not allow strike of 0\n    if self.strike == 0: \n      raise ZeroDivisionError('The Strike Price cannot be 0')\n    else: \n      self._d1_ = (np.log(self.spot / self.strike) + (self.rate + (self.sigma**2 / 2)) * dte) / (self.sigma * self.dte**0.5)\n      self._d2_ = self._d1_ - (self.sigma * self.dte**0.5) \n    \n    for i in ['callPrice', 'putPrice', 'callDelta', 'putDelta', 'gamma']: \n      self.__dict__[i] = None\n      \n    [self.callPrice, self.putPrice] = self._price() \n    [self.callDelta, self.putDelta] = self._delta()\n    self.gamma = self._gamma()\n    \n  def _price(self): \n      if self.sigma == 0 or self.dte == 0: \n        call = maximum(0.0, self.spot - self.strike)\n        put = maximum(0.0, self.strike - self.spot) \n      else: \n        call = (self.spot * norm.cdf(self._d1_)) - (self.strike * np.e**(- self.rate * self.dte) * norm.cdf(self._d2_))\n        put = (self.strike * np.e**(- self.rate * self.dte) * norm.cdf(-self._d2_)) - (self.spot * norm.cdf(-self._d1_))\n      return [call, put] \n    \n  def _delta(self): \n    if self.sigma == 0 or self.dte == 0: \n      call = 1.0 if self.spot &gt; self.strike else 0.0\n      put = -1.0 if self.spot &lt; self.strike else 0.0\n    else: \n      call = norm.cdf(self._d1_)\n      put = -norm.cdf(-self._d1_)\n    return [call, put]\n  \n  def _gamma(self): \n    return norm.cdf(self._d1_) / (self.spot * self.sigma * self.dte**0.5)\n\n\nfrom tabulate import tabulate\n\noption = option_pricing(100, 100, 0, 1, 0.2)\n\nheader = ['Call Price', 'Put Price', 'Call Delta', 'Gamma']\ntable = [[option.callPrice, option.putPrice, option.callDelta, option.gamma]]\nprint(tabulate(table, header))\n\n  Call Price    Put Price    Call Delta      Gamma\n------------  -----------  ------------  ---------\n     7.96557      7.96557      0.539828  0.0269914"
  },
  {
    "objectID": "posts/quant-part3/black-schole/index.html#retrieving-option-data-using-yahoo-finance",
    "href": "posts/quant-part3/black-schole/index.html#retrieving-option-data-using-yahoo-finance",
    "title": "Black-Schole Equation",
    "section": "Retrieving option data using Yahoo finance",
    "text": "Retrieving option data using Yahoo finance\n\nimport yfinance as yf \n\namd = yf.Ticker('AMD')\namd_hist = amd.history(start = '2022-01-01')\noptions = amd.option_chain('2023-12-15')\n\nfrom datetime import datetime\ndte = (datetime(2023, 12, 15) - datetime.today()).days \n\nlog_returns = np.log(amd_hist['Close'] / amd_hist['Close'].shift(1)).dropna()\nhistorical_vol = log_returns.std() * np.sqrt(dte)\n\nspot = 116; strike = 120; rate = 0.05\n\namd_opt = option_pricing(spot=spot, strike=strike, rate=rate, dte=dte/365, sigma=historical_vol)\n\nprint(f'The BS model for AMD 147 days ahead is {amd_opt.callPrice:0.4f}')\n\nThe BS model for AMD 147 days ahead is 2.7995\n\n\n\ndf = options.calls[(options.calls['strike'] &gt;= 90) & (options.calls['strike'] &lt; 150)]\ndf.reset_index(drop = True, inplace = True)\n\ndf1 = pd.DataFrame({'strike': df['strike'], 'price': df['lastPrice'], 'impl_vol': df['impliedVolatility']})\ndf1['delta'] = df1['gamma'] = 0.\n\nfor i in range(len(df1)): \n  df['delta'].iloc[i] = option_pricing(spot=spot, strike=df['strike'].iloc[i], rate, dte=dte, sigma = df1['impl_vol'].iloc[i]).callDelta\n\n\nfor i in range(len(df1)):\n    \n    df1['Delta'].iloc[i] = option_pricing(spot,df1['strike'].iloc[i],rate,dte,df1['impl_vol'].iloc[i]).callDelta\n    df1['Gamma'].iloc[i] = option_pricing(spot,df1['strike'].iloc[i],rate,dte,df1['impl_vol'].iloc[i]).gamma"
  },
  {
    "objectID": "posts/quant-part2/SDE-part3/index.html",
    "href": "posts/quant-part2/SDE-part3/index.html",
    "title": "03 - Stochastic Calculus - Part III",
    "section": "",
    "text": "Recall\n\n\n\n\n\n\n\nFunction\nItô Lemma\n\n\n\n\n\\(F(X_t)\\)\n\\(dF = \\frac{1}{2} \\frac{d^2F}{dX^2} \\space dt + \\frac{dF}{dX} \\space dx\\)\n\n\n\\(F(t, X_t)\\)\n\\(dF = \\left( \\frac{\\partial F}{\\partial t} + \\frac{1}{2} \\frac{\\partial^2 F}{\\partial X^2} \\right) dt + \\frac{\\partial F}{\\partial X} dX\\)\n\n\n\\(V(S)\\) when \\(dS = \\mu S dt + \\sigma S dX\\)\n\\(dV = \\left( \\mu S \\frac{dV}{dS} + \\frac{1}{2} \\sigma^2 S^2 \\frac{d^2V}{dS^2} \\right)dt + \\left( \\sigma S \\frac{dV}{dS}\\right) dX\\)\n\n\n\n\n\nItô Integrals as non-anticipatory\nLet’s consider the stochastic integral of the form \\[\\int_0^T f(t, X(t)) dX(t)\\] where \\(X_t\\) is a Brownian motion. We’ll shorten this form to \\(\\int_0^T f(t, X) dX\\)\nWe define this integral as \\[\\int_0^T f(t, X) dX = \\lim_{N \\to \\infty} \\sum_{i=0}^{N-1} f(t_i, X_i) \\cdot \\underbrace{ (X_{i+1} - X_i) }_{dX}\\]\nIt’s important to define it this way in order for the itô integral to stay non-anticipatory. We know everything up to time \\(t_i\\) and so the only uncertainties left is \\(X_{i+1} - X_i\\) which is \\(dX\\)\n\n\nProduct rule within Stochastic Calculus\nWhen dealing with Stochastic Differential Equations, we can adapt some of the rules of classical calculus such as the product rule: \\(d(xy) = xdy + ydx\\)\nLet’s say we have 2 stochastic processes: \\[d(X(t)) = \\mu_1 X(t) dt + \\sigma_1 X(t)  dW_t\\] \\[d(Y(t)) = \\mu_2 Y(t) dt + \\sigma_2 Y(t) dW_t\\]\nAnd we define a function \\(F\\) which is a product of these 2 stochastic processes such that \\(F = F(X,Y) = XY\\).\nUsing a Taylor Series Expansion, we can write: \\[F(X + dX, Y + dY) \\approx F(X,Y) + \\frac{\\partial F}{\\partial X} dX + \\frac{\\partial F}{\\partial Y} dY + \\frac{1}{2} \\frac{\\partial^2F}{\\partial X^2} dX^2 + \\frac{1}{2} \\frac{\\partial^2F}{\\partial Y^2} dY^2 + \\frac{\\partial^2F}{\\partial X \\partial Y} dXdY + \\dots\\]\nHence, \\[dF = \\frac{\\partial F}{\\partial X} dX + \\frac{\\partial F}{\\partial Y} dY + \\frac{1}{2} \\frac{\\partial^2F}{\\partial X^2} dX^2 + \\frac{1}{2} \\frac{\\partial^2F}{\\partial Y^2} dY^2 + \\frac{\\partial^2F}{\\partial X \\partial Y} dXdY + \\dots \\tag{1}\\]\nNow, we can calculate all these partial derivatives and plugged them back in the above equation. \\(\\frac{\\partial F}{\\partial X} = Y\\) and \\(\\frac{\\partial^2 F}{\\partial X^2} = 0\\).\nSimilarly \\(\\frac{\\partial F}{\\partial Y} = X\\) and \\(\\frac{\\partial^2 F}{\\partial Y^2} = 0\\).\nFinally: \\(\\frac{\\partial^2F}{\\partial X \\partial Y} = 1\\)\nPlugging it all back in Equation 1: \\[dF = Y dX + X dY + dXdY \\tag{2}\\]\n\n\nIntegral by parts\nIn classical calculus, we re-use the product rule to come up with the integration by part: \\(d(xy) = xdy + ydx\\). That is \\(xdy = d(xy) - ydx\\) which we can integrate for and get: \\(\\int xdy = \\int d(xy) - \\int y dx\\) which is the same as \\(\\int x dy = xy - \\int y dx\\).\nLet’s bring this to stochastic calculus. Again \\(F\\) is a function of the product of 2 stochastic processes: \\(F = F(X,Y) = XY\\) Using the same logic and our previous result Equation 2, we write \\[d(XY) = Y dX + X dY + dXdY\\] \\[X dY = d(XY) - Y dX - dXdY \\] \\[\\int_0^t X_s dY_s = \\int_0^t d(X_sY_s) - \\int_0^t Y_s dX_s - \\int_0^t dX_sdY_s\\] \\[\\int_0^t X_s dY_s = X_tY_t - X_0Y_0 - \\int_0^t Y_s dX_s - \\int_o^t dX_sdY_s\\]\n\n\nQuotient Rule within Stochastic Calculus\nWe will re-use the Taylor Series Expansion (Equation 1) except this time the function \\(F\\) is a function of the quotient of 2 stochastic processes: \\(F = F(X, Y) = \\frac{X}{Y}\\). Calculating all the partial derivatives: \\(\\frac{\\partial F}{\\partial X} = \\frac{1}{Y}\\) and \\(\\frac{\\partial^2 F}{\\partial X^2} = 0\\).\nSimilarly \\(\\frac{\\partial F}{\\partial Y} = \\frac{-X}{Y^2}\\) and \\(\\frac{\\partial^2 F}{\\partial Y^2} = \\frac{2X}{Y^3}\\).\nFinally: \\(\\frac{\\partial^2F}{\\partial X \\partial Y} = \\frac{-1}{Y^2}\\)\nPutting it all back together: \\[dF = \\frac{1}{Y} dX + \\frac{-X}{Y^2} dY + \\frac{1}{2} \\frac{2X}{Y^3} dY^2+ \\frac{-1}{Y^2} dXdY\\] Which we can re-write as: \\[dF = d \\left( \\frac{X}{Y} \\right) = \\frac{X}{Y} \\cdot \\left( \\frac{1}{X} dX - \\frac{1}{Y} dY - \\frac{1}{XY} dXdY + \\frac{1}{Y^2} dY^2\\right) \\tag{3}\\]\n\n\n\n\n\n\nIn the quant world.\n\n\n\nwe can word these results in the following way - taken from here:\n\nItô product rule: we buy correlation when we have a product\nItô quotient rule: we sell correlation when we have a ratio, and we are long vol of the denominator."
  },
  {
    "objectID": "posts/quant-part2/martingales/index.html",
    "href": "posts/quant-part2/martingales/index.html",
    "title": "04 -Martingales",
    "section": "",
    "text": "This post is a collection of notes about Martingales.\nA Martingales is a stochastic process that is driftless (aka it is pure randomness or just volatility). We also say that martingales are constant mean stochastic process.\n\nContinuous time martingales\nA continuous time stochastic process \\(\\{ M_t: t \\in \\mathbb{R}^+ \\}\\) such that \\(M_t\\) is adapted to \\(\\mathcal{F}_t\\) (or is \\(\\mathcal{F}_t\\) measurable) is a martingale if:\n\nintegrability condition: \\(\\mathbb{E}[M_t] \\lt \\infty\\)\nconditional expectation condition: \\(\\mathbb{E}_s[M_{t}|\\mathcal{F}_s] = M_s, \\space 0 \\leq s \\leq t\\)\n\n\n\nLink between itô integrales and martingales\n\n\n\n\n\n\nIntuitive & Motivating example\n\n\n\nLet \\(X\\) be a stochastic process and \\(F = X^2(t)\\). Recall then \\[F(t) = t + 2 \\int_0^t X_{\\tau} \\space d{\\tau}\\] \\[X^2(t) = t + 2 \\int_0^t X_{\\tau} \\space d{\\tau}\\] Taking expectation on both side \\[\\mathbb{E} \\left[ X^2(t) \\right] = t + \\mathbb{E} \\left[ 2 \\int_0^t X_{\\tau} \\space d{\\tau} \\right]\\] With the quadratic variation We already know that \\[\\mathbb{E} \\left[ X^2(t) \\right] = t\\] Which means that \\[\\mathbb{E} \\left[ 2 \\int_0^t X_{\\tau} \\space d{\\tau} \\right]\\] should be equal to 0. Which means that the itô integral \\[\\mathbb{E} \\left[ 2 \\int_0^t X_{\\tau} \\space d{\\tau} \\right]\\] is a martingale.\n\n\nItô integrals are martingales.\nLet \\(g(t, X_t)\\) be a function of a stochastic process, then \\(\\mathbb{E} \\left[ \\int_0^t g(\\tau, x_\\tau) \\space dX_\\tau \\right] = 0\\)\n\n\nA continuous time stochastic process is a martingale\n\\(Yt)\\) is a stochastic process that satisfies the following Stochastic Differential Equation \\[dY(t) = f(Y_t, t) \\space dt + g(Y_t, t) \\space dX(t) \\tag{1}\\] with initial condition \\(Y(0) = 0\\).\nHow to tell if \\(Y(t)\\) is martingale? We will use the fact, from above, that Itô integrals are martingales. For this, by definition, we need \\[\\mathbb{E}_s[Y_t | \\mathcal{F}_s] = Y_s \\space, \\space 0 \\leq s \\leq t\\]\nIntegrating Equation 1 both side, we can get an exact form for \\(Y(t)\\) \\[Y(t) = Y(s) + \\int_s^t f(Y_u, u) \\space du + \\int_s^t g(Y_u, u) \\space dX(u)\\]\nTaking the expectation on both side: \\[\\mathbb{E}(Y_t | \\mathcal{F}_s)= \\mathbb{E} \\left[Y(s) + \\int_s^t f(Y_u, u) \\space du + \\int_s^t g(Y_u, u) \\space dX(u) \\space | \\mathcal{F} \\right]\\]\n\\[\\mathbb{E}(Y_t | \\mathcal{F}_s) = Y(s) + \\mathbb{E} \\left[\\int_s^t f(Y_u, u) \\space du \\space | \\mathcal{F_s} \\right]\\]\nThis is because, see above, ito integrals are martingales and \\(\\int_s^t g(Y_u, u) \\space dX(u)\\) is an ito integral. Hence, its expectation is 0 ==&gt; \\(\\mathbb{E} \\left[ \\int_s^t g(Y_u, u) \\space dX(u) \\right] = 0\\)\nIn order for \\(\\mathbb{E}(Y_t | \\mathcal{F}_s) = Y(s)\\), we now need \\(\\mathbb{E} \\left[\\int_s^t f(Y_u, u) \\space du \\space | \\mathcal{F}_s \\right] = 0\\). This means that \\(f(Y_t, t) = 0, \\space \\forall t\\).\nGoing back to the SDE, we can say that \\(dY(t)\\) is a martingale iff \\[dY(t) = g(Y_t, t) \\space dX(t)\\]\n\n\nExponential martingales\nLEt’s consider a stochastic process \\(Y(t)\\) that satisfies the following Stochastic Differential Equation: \\[dY(t) = f(t) \\space dt + g(t) \\space dX(t)\\] with initial condition \\(Y(0) = 0\\). \\(X(t)\\) is a Brownian Motion, \\(f(t)\\) and \\(g(t)\\) are time-dependent functions.\nWe can now define a new process such that \\[Z(t) = e^{Y(t)}\\]\nHow can we ensure \\(Z(t)\\) is a martingale? How should we choose \\(f(t)\\) such that \\(Z(t)\\) is a martingale?"
  },
  {
    "objectID": "posts/stochastic-processes/index.html",
    "href": "posts/stochastic-processes/index.html",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "",
    "text": "This post is an introduction to Markov Chain with a presentation of Discrete Time Markov Chains."
  },
  {
    "objectID": "posts/stochastic-processes/index.html#definition",
    "href": "posts/stochastic-processes/index.html#definition",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Definition",
    "text": "Definition\nA stochastic process is \\(\\{ X(t), t \\in T \\}\\) is a collection of random variables indexed by a parameter t that belongs to a set T.\n\nt is generally the time\n\\(X(t)\\) is the state of the process at time t\nThe state space \\(S\\) of a stochastic process is all possible state \\(X(t)\\) for any \\(t \\in T\\)\nif T is a countable set, we call this a discrete-time process\n\nA discrete-time Markov Chain is a discrete-time stochastic process which state space S is finite such that: \\[\\mathbb{P}(X_{n+1} = j | X_0 = i_0, X_1 = i_1, X_2 = i_2, \\dots, x_n = i) = \\mathbb{P}(X_{n+1} = j | X_n = i) = P_{ij}\\]\nthat is, the conditional probability of the process being in state j at time n + 1 given all the previous states depends only on the last-known position (state i at time n)."
  },
  {
    "objectID": "posts/stochastic-processes/index.html#some-other-teminology",
    "href": "posts/stochastic-processes/index.html#some-other-teminology",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Some other teminology",
    "text": "Some other teminology\n\nA state is called absorbing if the chain cannot leave it once it enters it. An absorbing Markov chain has at least one absorbing state.\nA state is termed reflecting if once the chain leaves it, it cannot return to it.\nThe period d of a state i is the number such that, starting in i, the chain can return to i only in the number of steps that are multiples of d. A state with period d = 1 is called aperiodic. Periodicity is a class property.\n\nFor a reflecting state, the period is infinite, since the chain never comes back to this state.\nAbsorbing states necessarily have loops and thus are aperiodic states.\n\na state is called recurrent if with probability 1 the chain ever reenters that state. Otherwise, the state is called transient.\nA Markov Chain that has a unique stationary distribution (or steady-state distribution) is called an ergodic chain."
  },
  {
    "objectID": "posts/stochastic-processes/index.html#chapman-kolmogorov-equations",
    "href": "posts/stochastic-processes/index.html#chapman-kolmogorov-equations",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Chapman-Kolmogorov equations",
    "text": "Chapman-Kolmogorov equations\nWe denote the probability to go from state \\(i\\) to state \\(j\\) in n-steps by \\(\\bf{P}_{ij}^{(n)}\\). It is also denoted as the n-steps transition probability matrix. That is for any time \\(m &gt;= 0, \\bf{P}_{ij}^n = \\mathbb{P}(X_{m+n} = j | X_m = i)\\) . \\(\\bf{P}^{(n)} = \\bf{P}^n\\) based on the Chapman-Kolmogorov equation.\nThe Chapman-Kolmogorov equation states that for all positive integers \\(m\\) and \\(n\\) , \\(\\bf{P}^{(m+n)} = \\bf{P}^m \\cdot \\bf{P}^n\\) where P is a one-step probability transition matrix (a square matrix)"
  },
  {
    "objectID": "posts/stochastic-processes/index.html#example-1",
    "href": "posts/stochastic-processes/index.html#example-1",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Example 1",
    "text": "Example 1\nTo model a Markov Chain, let’s first set up a one-step probability transition matrix (called here osptm).\nWe start with an easy 3 possible state process. That is the state space \\(S = \\{1, 2, 3\\}\\). The osptm will provide the probability to go from one state to another.\n\nosptm = matrix(c(0.7,0.1,0.2, 0,0.6,0.4, 0.5,0.2,0.3), nrow = 3, byrow = TRUE)\nosptm\n\n     [,1] [,2] [,3]\n[1,]  0.7  0.1  0.2\n[2,]  0.0  0.6  0.4\n[3,]  0.5  0.2  0.3\n\n\nWe can always have a look at how the osptm looks like.\n\n# note we have to transpose the osptm matrix first. \nosptm_transposed = t(osptm)\nosptm_transposed\n\n     [,1] [,2] [,3]\n[1,]  0.7  0.0  0.5\n[2,]  0.1  0.6  0.2\n[3,]  0.2  0.4  0.3\n\ndiagram::plotmat(osptm_transposed, pos = c(1, 2), arr.length = 0.3, \n                 box.col = \"lightblue\", box.prop = 0.5, box.size = 0.12, box.type=\"circle\", \n                 self.cex = 0.6, self.shifty=-0.01, self.shiftx = 0.15)\n\n\n\n\nThe markovchain package can provide us with all the state characteristics of a one-step probabilty transition matrix.\n\nlibrary(markovchain)\nosptm_mc &lt;- new(\"markovchain\", transitionMatrix = osptm)\n\nrecurrentClasses(osptm_mc)\n\n[[1]]\n[1] \"1\" \"2\" \"3\"\n\ntransientClasses(osptm_mc)\n\nlist()\n\nabsorbingStates(osptm_mc)\n\ncharacter(0)\n\nperiod(osptm_mc)\n\n[1] 1\n\nround(steadyStates(osptm_mc), 4)\n\n          1      2      3\n[1,] 0.4651 0.2558 0.2791\n\n\nThe next step is to calculate, for instance, what is the probability to go from state 1 to state 3 in 4 steps.\n\nlibrary(expm)\n\n# the expm library brings in the \" %^%\" operator for power. \nosptm %^% 4\n\n       [,1]   [,2]   [,3]\n[1,] 0.5021 0.2303 0.2676\n[2,] 0.3860 0.3104 0.3036\n[3,] 0.4760 0.2483 0.2757\n\n\nLooking at the result, we can see that the probability to go from State 1 to State 3 in 4 steps is 0.2676\nWe can also calculate the unconditional distribution after 4 steps\n\ninitial_pro &lt;- c(1/3, 1/3, 1/3)\ninitial_pro %*% (osptm %^% 4)\n\n       [,1]  [,2]   [,3]\n[1,] 0.4547 0.263 0.2823"
  },
  {
    "objectID": "posts/stochastic-processes/index.html#example-2",
    "href": "posts/stochastic-processes/index.html#example-2",
    "title": "Stochastic processes - Discrete Time Markov Chain",
    "section": "Example 2",
    "text": "Example 2\nUsing a slightly more interesting one-step probability transition matrix having 6 different states.\n\n#specifying transition probability matrix\nosptm&lt;- matrix(c(0.3,0.7,0,0,0,0,1,0,0,0,0,0,0.5,0,0,0,0,0.5, 0,0,0.6,0,0,0.4,0,0,0,0,0.1,0.9,0,0,0,0,0.7,0.3), nrow=6, byrow=TRUE)\nosptm\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  0.3  0.7  0.0    0  0.0  0.0\n[2,]  1.0  0.0  0.0    0  0.0  0.0\n[3,]  0.5  0.0  0.0    0  0.0  0.5\n[4,]  0.0  0.0  0.6    0  0.0  0.4\n[5,]  0.0  0.0  0.0    0  0.1  0.9\n[6,]  0.0  0.0  0.0    0  0.7  0.3\n\nosptm_transposed = t(osptm)\ndiagram::plotmat(osptm_transposed, arr.length = 0.3, arr.width = 0.1, \n                 box.col = \"lightblue\", box.prop = 0.5, box.size = 0.09, box.type=\"circle\", \n                 cex.txt = 0.8, self.cex = 0.6, self.shifty=-0.01, self.shiftx = 0.13)\n\n\n\nosptm_mc &lt;- new(\"markovchain\", transitionMatrix = osptm)\n\nrecurrentClasses(osptm_mc)\n\n[[1]]\n[1] \"1\" \"2\"\n\n[[2]]\n[1] \"5\" \"6\"\n\ntransientClasses(osptm_mc)\n\n[[1]]\n[1] \"3\"\n\n[[2]]\n[1] \"4\"\n\nabsorbingStates(osptm_mc)\n\ncharacter(0)\n\nperiod(osptm_mc)\n\nWarning in period(osptm_mc): The matrix is not irreducible\n\n\n[1] 0\n\nround(steadyStates(osptm_mc), 4)\n\n          1      2 3 4      5      6\n[1,] 0.0000 0.0000 0 0 0.4375 0.5625\n[2,] 0.5882 0.4118 0 0 0.0000 0.0000\n\n\nWe can see that there are 2 possible steady states. Hence the Markov Chain is non-ergodic."
  },
  {
    "objectID": "posts/machine-learning-part1/naives-bayes-intro/index.html",
    "href": "posts/machine-learning-part1/naives-bayes-intro/index.html",
    "title": "Naive-Bayes - Part 1",
    "section": "",
    "text": "Some very basic ML using Naive-Bayes and the tidymodel framework.\n\nlibrary(readr)\nlibrary(dplyr)  # mutate(), row_number()\n\ndf &lt;- read_csv('../../../raw_data/financial_news.csv', col_names = c('sentiment', 'text')) |&gt; \n  mutate(sentiment = factor(sentiment))\n\nUsing the tidyverse, we’ll\n\nsplit the df into a training and testing set.\n\n\nlibrary(rsample)    # initial_split(), training(), testing()\nlibrary(recipes)\nlibrary(parsnip)    # naive_bayes(), set_engine()\nlibrary(workflows)  # workflow()\n\nlibrary(discrim)\nlibrary(textrecipes)\nlibrary(yardstick)\n\nlist_splits &lt;- initial_split(df, prop = 0.8, strata = 'sentiment')\ndf_train &lt;- training(list_splits)\ndf_test &lt;- testing(list_splits)\n\nlist_recipe &lt;- recipe(sentiment ~., data = df_train) |&gt; \n  step_tokenize(text) |&gt; \n  step_stopwords(text) |&gt; \n  step_tokenfilter(text, max_tokens = 100) |&gt; \n  step_tfidf(text)\n  \n\nmod_nb &lt;- naive_Bayes() |&gt; set_engine('naivebayes') |&gt; set_mode('classification')\nmod_svm &lt;- svm_poly() |&gt; set_engine('kernlab') |&gt; set_mode('classification')\nlist_cv &lt;- vfold_cv(df_train, v = 5, strata = 'sentiment')\n\nwf_nb &lt;- workflow() |&gt; add_recipe(list_recipe) |&gt; add_model(mod_nb)\nwf_nb\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_tokenize()\n• step_stopwords()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n\nwf_svm &lt;- workflow() |&gt; add_recipe(list_recipe) |&gt; add_model(mod_svm)\n\nfit_mod_nb &lt;- fit(wf_nb, df_train)\npred_mod_nb &lt;- predict(fit_mod_nb, df_test)\npred_mod_nb_prob &lt;- predict(fit_mod_nb, df_test, type = 'prob')\n\nfit_mod_svm &lt;- fit(wf_svm, df_train)\n\n Setting default kernel parameters  \n\npred_mod_svm &lt;- predict(fit_mod_svm, df_test)\npred_mod_svm_prob &lt;- predict(fit_mod_svm, df_test, type = 'prob')\n\nbind_cols(df_test, pred_mod_nb) |&gt; conf_mat(sentiment, .pred_class)\n\n          Truth\nPrediction negative neutral positive\n  negative       65      55       81\n  neutral        53     509      168\n  positive        3      12       24\n\nbind_cols(df_test, pred_mod_svm) |&gt; conf_mat(sentiment, .pred_class)\n\n          Truth\nPrediction negative neutral positive\n  negative       31       6       17\n  neutral        71     526      181\n  positive       19      44       75\n\n#roc_nb &lt;- bind_cols(df_test, pred_mod_nb_prob) |&gt; roc_curve()"
  },
  {
    "objectID": "posts/portfolio_optimization_parti_inR/index.html",
    "href": "posts/portfolio_optimization_parti_inR/index.html",
    "title": "Portfolio Optimization Part I (in R)",
    "section": "",
    "text": "First post on portfolio optimization from a quantitative finance lense.\nWe are optimizing a portfolio with N assets, where \\(N \\geq 2\\) (N is a positive integer)\nOf course, the sum of all the weights should be equal to 1.\n\\[\\sum_{i = 1}^{N} W_i = 1  \\tag{2}\\]\nFew assumptions made on the assets.\nThe Mean-Variance Optimization problem can be formulated in 2 ways:\nReturn is the expected return of the asset and risk is the variance of the returns of the asset.\nThe Risk-Free Asset (RFA) is the money deposited in a bank account-ish (a secure term-deposit) at a fixed rate R. The expected return is thus R and volatitliy is 0. Also the correlation between the RFA and any other assets is 0.\nEach asset can be represented on a 2D-plane with the risk on the x-axis and returns on the y-axis.\nFigure 1: ?(caption)\nThere are some other assumptions made when trying to construct a mean-variance optimum portfolio:"
  },
  {
    "objectID": "posts/portfolio_optimization_parti_inR/index.html#create-mc-simulations-of-weights-to-assess-mean-variance-of-a-portfolio",
    "href": "posts/portfolio_optimization_parti_inR/index.html#create-mc-simulations-of-weights-to-assess-mean-variance-of-a-portfolio",
    "title": "Portfolio Optimization Part I (in R)",
    "section": "Create MC simulations of weights to assess mean-variance of a portfolio",
    "text": "Create MC simulations of weights to assess mean-variance of a portfolio\nLet’s get 5 different financial assets: AA, LLY, AMD, SBUX, FDX. Although they are from different industries, it is not a very diverse bunch as they are all from US big companies.\nWe already have the assets downloaded and we’ll use their closing prices.\n\nlibrary(readr)      # read_csv()\nlibrary(dplyr)      # select(), arrange(), filter(), mutate(), summarize()\nlibrary(purrr)      # map()\nlibrary(tidyr)      # drop_na(), pivot_wider(), unnest()\nlibrary(glue)       # glue()\nlibrary(lubridate)\n\n# read adjusted closing prices and compute annualized daily returns and sd\nread_prices &lt;- function(ticker) { \n  df &lt;- read_csv(glue('../../raw_data/', {ticker}, '.csv')) |&gt; \n    arrange(date) |&gt; \n    select(date, adj_close = adjClose) |&gt; \n    filter(date &gt; '2018-01-01') |&gt; \n    mutate(ret1d = (adj_close / lag(adj_close, 1)) - 1) |&gt; \n    summarize(mean_ret = mean(ret1d, na.rm = T) * 252 * 100, \n              std_ret = sd(ret1d, na.rm = T) * sqrt(252) * 100)\n}\n\nassets &lt;- c('AA', 'LLY', 'AMD', 'SBUX', 'FDX')\ndf &lt;- tibble(ticker = assets, \n             prices = map(ticker, read_prices)) |&gt; \n  unnest()\n\n\n\n\n\nTable 1: Annualized Mean and Standard Deviation of daily returns\n\n\nTicker\nMean Ret\nStd of Ret\n\n\n\n\nAA\n15.90\n60.72\n\n\nLLY\n31.79\n29.78\n\n\nAMD\n55.20\n56.37\n\n\nSBUX\n16.43\n30.36\n\n\nFDX\n4.40\n37.53\n\n\n\n\n\n\n\n\nFirst, we create the df of returns:\n\nit’s a long df with only ticker, date, daily returns\none row per daily observation.\nwe drop first row with no returns\nthe returns df is a wide df with date and tickers as columns, then daily returns as row\n\n\n# functions to get daily returns of each assets\ncreate_returns_df &lt;- function(ticker) { \n  df &lt;- read_csv(glue('../../raw_data/', {ticker}, '.csv')) |&gt; \n    arrange(date) |&gt; \n    select(date, adj_close = adjClose) |&gt; \n    filter(date &gt; '2018-01-01') |&gt; \n    mutate(ret1d = (adj_close / lag(adj_close, 1)) - 1) |&gt; \n    select(date, ret1d)\n}\n\n# df of each assets and all their daily returns\ndf &lt;- tibble(ticker = assets, \n             prices = map(ticker, create_returns_df)) |&gt; \n  unnest(cols = c(prices)) |&gt; \n  drop_na()\n\n\nreturns &lt;- df |&gt; arrange(ticker) |&gt; \n  pivot_wider(names_from = ticker, values_from = ret1d)  \n\nhead(returns)\n\n# A tibble: 6 × 6\n  date             AA     AMD       FDX       LLY     SBUX\n  &lt;date&gt;        &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 2018-01-03 -0.0121   0.0519  0.0125    0.00543   0.0187 \n2 2018-01-04  0.00367  0.0494  0.0156    0.00446   0.00375\n3 2018-01-05 -0.0112  -0.0198  0.00393   0.0123    0.0115 \n4 2018-01-08  0.0168   0.0337  0.0103   -0.00508  -0.00503\n5 2018-01-09 -0.0145  -0.0375 -0.00339  -0.000813 -0.00219\n6 2018-01-10  0.0363   0.0118  0.000971  0.000465  0.0108 \n\n\nTo optimize the mean-variance of the portfolio, we consider the following\nWeights of each assets are \\(w = \\pmatrix{w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n}\\).\nMean returns of each assets are \\(\\mu = \\pmatrix{\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_n}\\)\nThen, the expected portfolio return is \\(\\mu_{\\pi} = w^{T} \\cdot \\mu\\) where \\(w^{T}\\) is the transpose of \\(w\\) (aka transforming \\(w\\) from a column vector to a row vector in order to have right dimensions to compute the dot product).\nAnd the expected portfolio variance is computed by \\[\\sigma_{\\pi}^2 = w^T \\cdot \\Sigma \\cdot w\\] where \\(\\Sigma\\) is the covariance matrix. Also, don’t forget to square root the variance when using sd: \\(\\sigma_{\\pi} = \\sqrt{w^T \\cdot \\Sigma \\cdot w}\\)\nTo put this into code, we first create the matrix of returns,then create the covariance matrix. Using both matrices, we create a function that return the portfolio mean and variance using the randomly chosen weights.\n\n# this df to provide vectors of expected returns\ndf_stat &lt;- df |&gt; \n  group_by(ticker) |&gt; \n  summarize(mean_ret = mean(ret1d, na.rm = T) * 252) |&gt; \n  ungroup() |&gt; arrange(ticker) \n\nmu = as.matrix(df_stat$mean_ret, nrow = length(assets))\n\n# this df to provide the covariance matrix\n# note how we have also multiplied it by 252\nsigma &lt;- df |&gt; arrange(ticker) |&gt; \n  pivot_wider(names_from = ticker, values_from = ret1d) |&gt; \n  drop_na() |&gt; \n  select(-date) |&gt; cov() * 252\n\nsigma &lt;- as.matrix(sigma, nrow = length(assets))\n\n# this function to create one simulation using random weights\ncreate_one_portfolio_simulation &lt;- function(n) { \n  # pick random weights\n  weights_rand = runif(length(assets))\n  weights = matrix(weights_rand / sum(weights_rand), nrow = length(assets))\n  \n  #these are textbook formula for return and volat of a portfolio\n  return_pi = as.numeric(t(weights) %*% mu)\n  volat_pi = sqrt(as.numeric((t(weights) %*% sigma) %*% weights))\n  sharpe_ratio = return_pi / volat_pi\n  \n  # wrap everything into a df for later checks / analysis\n  df &lt;- tibble(portf_ret = round(return_pi * 100, 4), portf_volat = round(volat_pi * 100, 4),  \n               weights = round(t(weights) * 100, 4), sharpe_ratio = sharpe_ratio)\n\n  return(df)\n}\n\n#this is really the only inputs to get \nnum_portfolio = 7000\n\nmc_simu = tibble(id = 1:num_portfolio) |&gt; \n  mutate(simul = map(id, create_one_portfolio_simulation)) |&gt; \n  unnest(cols=c(simul)) |&gt; \n  arrange(desc(sharpe_ratio))\n\nhead(mc_simu)\n\n# A tibble: 6 × 5\n     id portf_ret portf_volat weights[,1]  [,2]  [,3]  [,4]   [,5] sharpe_ratio\n  &lt;int&gt;     &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n1  5710      37.8        29.4       0.168  31.1 0.757  61.6  6.43          1.29\n2  6227      34.7        27.6       0.937  25.3 0.648  55.8 17.2           1.26\n3   483      34.8        27.9       0.917  26.1 5.59   58.2  9.18          1.25\n4   184      37.6        30.3       0.546  35.6 6.66   53.1  4.14          1.24\n5  3169      36.0        29.1       1.90   32.0 4.07   50.2 11.9           1.24\n6  1388      34.1        27.5       3.26   18.0 4.97   73.4  0.415         1.24\n\n\n\nVizualisation of mean-variances points\nWe have highlithed the portfolio with best mean-variance returns with a red square around its dot.\n\nlibrary(ggplot2)\n\nggplot(mc_simu, aes(x = portf_volat, y = portf_ret)) + \n  geom_point(aes(colour = sharpe_ratio)) + \n  scale_color_distiller(palette=\"Set1\") + \n  geom_point(data = mc_simu[1,], aes(x = portf_volat, y = portf_ret), \n             color = 'red', size = 6, shape=5) + \n  xlab('Portfolio Volatility') + \n  ylab('Portfolio Returns') + \n  labs(title = 'MC simulation for 5 stocks', color = 'Sharpe \\n Ratio')"
  },
  {
    "objectID": "posts/proba-quant/jensen-inequality/index.html",
    "href": "posts/proba-quant/jensen-inequality/index.html",
    "title": "Jensen’s Inequality",
    "section": "",
    "text": "Jensen’s inequality from Johan Jensen, Danish Mathematician, has popped up several times through my quantitative finance journey. As I never explicitly dealt with it, I thought I’ll make a post about it.\nThe question Jensen’s inequality address is how does the mean of a function relate the function of the mean. First, we’ll check how in the case of linear functions, there are no difference there. Then we’ll go on more complex functions where their concavity (aka second derivative) matters."
  },
  {
    "objectID": "posts/proba-quant/jensen-inequality/index.html#convex-transformations-of-random-variables",
    "href": "posts/proba-quant/jensen-inequality/index.html#convex-transformations-of-random-variables",
    "title": "Jensen’s Inequality",
    "section": "Convex transformations of Random Variables",
    "text": "Convex transformations of Random Variables\nThe Jensen’s inequality is in regards to convex functions (aka its second derivative is positive on an interval) and states that the mean of the transformation is always greater or equal to the transformation of the mean.\nUsing the language of probability, we can write that for any convex functions \\[\\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X]) \\tag{1}\\]\nAgain let’s take an easy example with the convex function \\(f(x) = X^2\\). We will use the same inputs (the roll of a die) as the above example.\nThe intuition is that the mean of squared numbers is greater than the square of the mean numbers. This is because bigger number squared will add extreme value (on the high side) to the mean.\nUsing a small Python script.\n\noutcomes = [1, 2, 3, 4, 5, 6]\n\n# defining our transform functions. \ndef payoff(x): \n  return x**2\n\npayoffs = [payoff(outcome) for outcome in outcomes]\npayoffs\n\n[1, 4, 9, 16, 25, 36]\n\n\nLet’s now calculate both \\(\\mathbb{E}[f(X)]\\) and \\(f(\\mathbb{E}[X])\\)\n\nprint(f\"The mean of the transformed outcomes is {np.mean(payoffs)}\")\nprint(f\"The transformed of the mean is {payoff(np.mean(outcomes))}\")\n\nThe mean of the transformed outcomes is 15.166666666666666\nThe transformed of the mean is 12.25"
  },
  {
    "objectID": "posts/time-series/02-statistical-moments/index.html",
    "href": "posts/time-series/02-statistical-moments/index.html",
    "title": "02 - Statistical Moments",
    "section": "",
    "text": "This post is about summarizing the various statistical moments when doing quantitative finance. The focus is on the asset returns. From a previous post, we already know that financial asset returns do not follow a normal distribution (too peaked at the mean and fat tails).\nWe’ll show these parameters using both R and python.\nWe’ll use the SPY as a low-ish vol asset and AMD as an equity with higher vol. We only use the last 5 years of data (from 2018 and beyond)\nLet’s first load our libraries and the 2 data frame worth of prices.\nlibrary(readr)    # read_csv()\nlibrary(dplyr)    # mutate(), filter()\nlibrary(lubridate)\nlibrary(ggplot2)\n\ndf_spy &lt;- read_csv('../../../raw_data/SPY.csv') |&gt; \n  select(date, adjClose) |&gt; \n  arrange(date) |&gt; \n  mutate(return = log(adjClose / lag(adjClose))) |&gt; \n  filter(date &gt; '2018-01-01')\n\ndf_amd &lt;- read_csv('../../../raw_data/AMD.csv') |&gt; \n  select(date, adjClose) |&gt; \n  arrange(date) |&gt; \n  mutate(return = log(adjClose / lag(adjClose))) |&gt; \n  filter(date &gt; '2018-01-01')"
  },
  {
    "objectID": "posts/time-series/03-autocorrelation/index.html",
    "href": "posts/time-series/03-autocorrelation/index.html",
    "title": "03 - AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "",
    "text": "This post is to set up the basic concepts of time-series analysis."
  },
  {
    "objectID": "posts/time-series/03-autocorrelation/index.html#autocorrelation-plots---correlogram",
    "href": "posts/time-series/03-autocorrelation/index.html#autocorrelation-plots---correlogram",
    "title": "03 - AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Autocorrelation plots - Correlogram",
    "text": "Autocorrelation plots - Correlogram\nAs exercise, we can plot the auto-correlation of a non-stationary (aka with significant autocorrelation) time-series. We are using the Monthly Milk production (no idea where the data come from)\nOn the autocorrelation plot, the threshold line are situated at \\(\\pm \\frac{2}{\\sqrt{T}}\\) .\nLet’s maybe first have a visual of the data.\n\nlibrary(readr)\nlibrary(tibble)\nlibrary(ggplot2)\n\nmilk &lt;- read_csv('../../../raw_data/milk.csv')\n\nggplot(milk, aes(x = month, y = milk_prod_per_cow_kg)) + \n  geom_line()\n\n\n\n\n\nUsing R\nIn R the standard function to plot a correlogram is the acf() function\n\nacf(milk$milk_prod_per_cow_kg)\n\n\n\n\nGraph clearly shows some seasonality (at the 12 lags ==&gt; yearly correlation) which indicates that our data are non-stationary (next section). The threshold line is at 0.154 as there are 168 observations in the dataset (number of monthly reported observations)\nIf we are more attached to the auto-correlation values, we can store the results in a dataframe.\n\nyo &lt;- acf(milk$milk_prod_per_cow_kg, plot = F)\nyo\n\n\nAutocorrelations of series 'milk$milk_prod_per_cow_kg', by lag\n\n    0     1     2     3     4     5     6     7     8     9    10    11    12 \n1.000 0.892 0.778 0.620 0.487 0.428 0.376 0.415 0.454 0.562 0.687 0.769 0.845 \n   13    14    15    16    17    18    19    20    21    22 \n0.745 0.638 0.490 0.364 0.306 0.255 0.287 0.321 0.417 0.529 \n\n\nWe could use the ggplot package to create a function to draw acf and get more customization. We will re-use this function later as well.\n\n# slightly fancier version (with more customization)\nggacf &lt;- function(series) {\n  significance_level &lt;- qnorm((1 + 0.95)/2)/sqrt(sum(!is.na(series)))  \n  a &lt;- acf(series, plot=F)\n  a.2 &lt;- with(a, data.frame(lag, acf))\n  g &lt;- ggplot(a.2[-1,], aes(x=lag,y=acf)) + \n    geom_segment(mapping = aes(xend = lag, yend = 0), linewidth = 0.8) + \n    xlab('Lag') + ylab('ACF') + \n    geom_hline(yintercept=c(significance_level,-significance_level), linetype= 'dashed', color = 'dodgerblue4');\n\n  # fix scale for integer lags\n  if (all(a.2$lag%%1 == 0)) {\n    g&lt;- g + scale_x_discrete(limits = factor(seq(1, max(a.2$lag))));\n  }\n  return(g);\n}\n\n\nggacf(milk$milk_prod_per_cow_kg)\n\n\n\n\n\n\nUsing Python\nIn python, we need to use the statsmodel package.\n\nfrom pandas import read_csv\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf\n\ndf = read_csv('../../../raw_data/milk.csv', index_col=0)\nplot_acf(df)\nplt.show()"
  },
  {
    "objectID": "posts/time-series/03-autocorrelation/index.html#statistical-test-to-check-white-noise.",
    "href": "posts/time-series/03-autocorrelation/index.html#statistical-test-to-check-white-noise.",
    "title": "03 - AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Statistical test to check white-noise.",
    "text": "Statistical test to check white-noise.\nIn R we can use the Ljung-Box test (Portmanteau ‘Q’ test).\n\nBox.test(wn, type = 'Ljung-Box', lag = 1)\n\n\n    Box-Ljung test\n\ndata:  wn\nX-squared = 4.8374, df = 1, p-value = 0.02785"
  },
  {
    "objectID": "posts/time-series/03-autocorrelation/index.html#using-r-1",
    "href": "posts/time-series/03-autocorrelation/index.html#using-r-1",
    "title": "03 - AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Using R",
    "text": "Using R\n\nlibrary(dplyr)\nlibrary(lubridate)\n\ndf &lt;- read_csv('../../../raw_data/SBUX.csv') |&gt; arrange(date) |&gt; \n  select(date, adjClose) |&gt; \n  mutate(ret_1d = log(adjClose / lag(adjClose)), \n         ret_5d = log(adjClose / lag(adjClose, n = 5)), \n         y_t = log(adjClose) - log(lag(adjClose)), \n         day_of_week = weekdays(date)) |&gt; \n  filter(date &gt; '2018-01-01' & day_of_week == 'Tuesday')\n\nggacf(df$y_t)"
  },
  {
    "objectID": "posts/time-series/03-autocorrelation/index.html#python-code",
    "href": "posts/time-series/03-autocorrelation/index.html#python-code",
    "title": "03 - AutoCorrelation, Stationarity and Random-Walk - Part 1",
    "section": "Python code",
    "text": "Python code\n\nfrom statsmodels.tsa.stattools import acf\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\npy_df = pd.read_csv('../../../raw_data/SBUX.csv')\npy_df.index = py_df['date']\npy_df = py_df.sort_index()\n\npy_df_ts = pd.Series(py_df['adjClose'])\nlog_ret = np.log(1 + py_df_ts.pct_change())\nlog_ret = log_ret.dropna()\n\nr, q, p = acf(log_ret, nlags = 25, qstat = True)\n\nfig = plt.figure()\nplot_acf(log_ret, lags=25)\nplt.show()\n\n\n\n\n\n\n\n\n# q is for the Ljung-Box test statistics\nq\n\narray([26.3136516 , 26.316042  , 26.4475469 , 27.2734633 , 28.00235211,\n       28.69472715, 28.70545674, 35.25084316, 39.48634821, 39.62923899,\n       40.19059746, 40.26948906, 40.2707996 , 40.27868851, 44.51737182,\n       44.57802557, 45.63422739, 45.70863764, 46.1791967 , 46.4744188 ,\n       47.53325326, 48.52664511, 49.81175008, 50.99884363, 54.25246436])\n\n\n\n# p is for the p-value of the Ljung-Box statistics. \np\n\narray([2.90229916e-07, 1.92994124e-06, 7.68595886e-06, 1.75019647e-05,\n       3.63602517e-05, 6.94858936e-05, 1.63715557e-04, 2.40667022e-05,\n       9.41189824e-06, 1.96904817e-05, 3.31867363e-05, 6.48649190e-05,\n       1.25022094e-04, 2.30799458e-04, 9.12196699e-05, 1.61044765e-04,\n       1.95803242e-04, 3.27130628e-04, 4.67495545e-04, 6.93532977e-04,\n       7.95682642e-04, 9.24023511e-04, 9.75159499e-04, 1.05482880e-03,\n       6.16160760e-04])\n\n\n\nq1, p1 = acorr_ljungbox(log_ret, lags =25, return_df = False, boxpierce = False)\np1\n\n'lb_pvalue'\n\n\n\nfig = plt.figure()\nplot_pacf(log_ret, lags=25)\nplt.show()\n\n\n\n\n\n\n\n\nlog_ret.describe()\n\ncount    5653.000000\nmean        0.000559\nstd         0.019890\nmin        -0.176788\n25%        -0.008773\n50%         0.000345\n75%         0.009802\nmax         0.168728\nName: adjClose, dtype: float64\n\nlog_ret.plot()\nplt.show()\n\n\n\npd.Series.idxmax(log_ret)\n\n'2009-07-22'\n\npd.Series.idxmin(log_ret)\n\n'2020-03-16'"
  },
  {
    "objectID": "posts/time-series/04-ts-decomposition/index.html",
    "href": "posts/time-series/04-ts-decomposition/index.html",
    "title": "04 - Time-series decomposition",
    "section": "",
    "text": "When studying time-series, we usually consider trend and seasonality. With that in mind, we define time-series either in an additive way \\[y_t = T_t + S_t + R_t\\] or in a multiplicative way \\[y_t = T_t \\cdot S_t \\cdot R_t\\]\nwith:\nBecause decomposition used averages (for seasonal) and noving average (for trend), decomposition is not robust to outlier (aka, it is very sensitive to outlier)"
  },
  {
    "objectID": "posts/time-series/04-ts-decomposition/index.html#compose-deterministic-time-series-with-trend-and-seasonality-with-stochastic-component",
    "href": "posts/time-series/04-ts-decomposition/index.html#compose-deterministic-time-series-with-trend-and-seasonality-with-stochastic-component",
    "title": "04 - Time-series decomposition",
    "section": "Compose deterministic time-series (with trend and seasonality) with stochastic component",
    "text": "Compose deterministic time-series (with trend and seasonality) with stochastic component\n\ndf &lt;- tibble(x = 1:252, phi = rnorm(252, mean = 0, sd = 1.5)) |&gt; \n  mutate(y = 0.07 * x + 0.03 + phi)\n\nggplot(df, aes(x, y)) + \n  geom_line() + \n  ggtitle(label = 'Compose a linear trend with a stochastic component')\n\n\n\n\nWe can also create a seasonality with a stochastic component\n\ndf &lt;- tibble(x = 1:252, phi = rnorm(252, mean = 0, sd = 1.5)) |&gt; \n  mutate(y = 1.7 * sin((2 * pi * x / 50) + 0.3 * pi ) + phi)\n         \nggplot(df, aes(x, y)) + \n  geom_line() + \n  ggtitle(label = 'Compose a seasonal trend with a stochastic component')"
  },
  {
    "objectID": "posts/proba-quant/prob-quant01/index.html",
    "href": "posts/proba-quant/prob-quant01/index.html",
    "title": "Probability For Quant 01",
    "section": "",
    "text": "I am storing here a few nuggets of probability I encountered in my quantitative finance and machine learning journey. I have a similar page on Algebra. This first series is about discrete probability problems.\n\nExpectation of the Poisson distribution\nLet’s \\(X\\) be a Discrete Random Variable (DRV) such that \\(X \\sim Po(\\lambda)\\). Then \\(P(X = x) = \\frac{e^{-\\lambda} \\lambda^x}{x!}\\). Let’s calculate the expectation of X.\n\\[E[X]=\\sum_{x=1}^{\\infty} x P(X=x)=\\sum_{x=1}^{\\infty} x \\frac{e^{-\\lambda} \\lambda^x}{x!} = e^{-\\lambda} \\sum_{x=1}^{\\infty} \\frac{\\lambda^x}{(x-1)!}\\]\n\\[E[X]= e^{-\\lambda} \\lambda \\sum_{x=1}^{\\infty} \\frac{\\lambda^{x-1}}{(x-1)!} = e^{-\\lambda} \\lambda \\sum_{x=0}^{\\infty} \\frac{\\lambda^{x}}{(x)!}= e^{-\\lambda} \\lambda e^{\\lambda} = \\lambda\\]\nThe second to last step is just the McLaurin expansion of \\(e^x\\).\n\n\nCovariance & correlation\nAssuming \\(X\\) and \\(Y\\) are random variables with mean \\(\\mu_x\\) and \\(\\mu_y\\), by definition, we have \\[Cov(X,Y) = \\mathbb{E}[(X-\\mu_x)(Y-\\mu_y)] \\tag{1}\\]\nWith some manipulations: \\[Cov(X,Y) = \\mathbb{E}[XY - X \\mu_y - Y \\mu_x + \\mu_x \\mu_y] = \\mathbb{E}[XY] - \\mathbb{E}[X \\mu_y] - \\mathbb{E}[Y \\mu_x] + \\mathbb{E}[\\mu_x \\mu_y]\\]\n\\(\\mu_x\\) and \\(\\mu_y\\) are constant. So we can re-write above equation as\n\\[Cov(X,Y) = \\mathbb{E}[XY] - \\mu_y \\mathbb{E}[X] - \\mu_x\\mathbb{E}[Y] + \\mathbb{E}[\\mu_x \\mu_y]\\]\nAlso, \\(\\mathbb{E}[\\mu_x \\mu_y] = \\mu_x \\mu_y\\) because the expectation of a constant is the constant itself. And \\(\\mathbb{E}[X] = \\mu_x\\) and \\(\\mathbb{E}[Y] = \\mu_y\\). So let’s re-write\n\\[Cov(X,Y) = \\mathbb{E}[XY] - \\mu_y \\mu_x - \\mu_x \\mu_y + \\mu_x \\mu_y\\] \\[Cov(X,Y) = \\mathbb{E}[XY] - \\mu_y \\mu_x \\tag{2}\\]"
  },
  {
    "objectID": "posts/xgboost-time-series/index.html",
    "href": "posts/xgboost-time-series/index.html",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "",
    "text": "This post is about using xgboost on a time-series using both R with the tidymodel framework and python. It is part of a series of articles aiming at translating python timeseries blog articles into their tidymodels equivalent.\nThe raw data is quite simple as it is energy consumption based on an hourly consumption. Original article can be found here. Minimal changes were made to better fit current python practices.\nXgboost is part of the ensemble machine learning algorithms. It can be used for both regression and classification. There are few issues in using Xgboost with time-series. This article is taking a Xgboost post in python and also translating with the new R tidymodel framework."
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-r",
    "href": "posts/xgboost-time-series/index.html#using-r",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using R",
    "text": "Using R\n\n# setting up main R libraries to start \nthe_path &lt;- here::here()\nlibrary(glue)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\ndf0 &lt;- read_csv(glue(the_path, \"/raw_data/AEP_hourly.csv\"))\n# let's have a quick look at what we are dealing with\nglimpse(df0)\n\nRows: 121,273\nColumns: 2\n$ Datetime &lt;dttm&gt; 2004-12-31 01:00:00, 2004-12-31 02:00:00, 2004-12-31 03:00:0…\n$ AEP_MW   &lt;dbl&gt; 13478, 12865, 12577, 12517, 12670, 13038, 13692, 14297, 14719…\n\n\nThere are only 2 variables. The Datetime being the only independ variable. And the energy consumption labelled as AEP_MW being our variable to predict.\n\n# and graphically - \n# just using a couple of years to get an idea \nggplot(df0 |&gt; filter(Datetime &gt; \"2014-01-01\" & Datetime &lt; \"2016-01-01\"), aes(x =Datetime, y=AEP_MW )) + geom_line(color = \"light blue\")\n\n\n\n\nFigure 1: Graphical glimpse of our raw data\n\n\n\n\nAs Datetime is our only input variable, we’ll use the usual tricks of breaking it down into week number, months, etc. I am doing it slightly differently than in the python version here as I will first create the new time related variables then I will split it into training and testing.\n\nlibrary(lubridate)\ndf &lt;- df0 |&gt; \n  mutate(hour = hour(Datetime), \n         day_of_week = wday(Datetime), \n         day_of_year = yday(Datetime), \n         day_of_month = mday(Datetime), \n         week_of_year = isoweek(Datetime), \n         month = month(Datetime), \n         quarter = quarter(Datetime), \n         year = isoyear(Datetime)\n         ) \n# another glimpse now. \nglimpse(df)\n\nRows: 121,273\nColumns: 10\n$ Datetime     &lt;dttm&gt; 2004-12-31 01:00:00, 2004-12-31 02:00:00, 2004-12-31 03:…\n$ AEP_MW       &lt;dbl&gt; 13478, 12865, 12577, 12517, 12670, 13038, 13692, 14297, 1…\n$ hour         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ day_of_week  &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, …\n$ day_of_year  &lt;dbl&gt; 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 366, 36…\n$ day_of_month &lt;int&gt; 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 3…\n$ week_of_year &lt;dbl&gt; 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 5…\n$ month        &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1…\n$ quarter      &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ year         &lt;dbl&gt; 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 2004, 200…\n\n\nAlthough, there are only 2 variables, there are over 120,000 rows of data. That’s non-negligible."
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-python",
    "href": "posts/xgboost-time-series/index.html#using-python",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using python",
    "text": "Using python\nThis is the code from the original post.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npy_df = pd.read_csv(\"../../raw_data/AEP_hourly.csv\", index_col = [0], parse_dates = [0])\npy_df.tail()\n\n                      AEP_MW\nDatetime                    \n2018-01-01 20:00:00  21089.0\n2018-01-01 21:00:00  20999.0\n2018-01-01 22:00:00  20820.0\n2018-01-01 23:00:00  20415.0\n2018-01-02 00:00:00  19993.0\n\n#plt.plot(df0)\nsplit_date = '01-jan-2016'\npy_df_train = py_df.loc[py_df.index &lt;= split_date].copy()\npy_df_test = py_df.loc[py_df.index &gt; split_date].copy()\n\nThe author of the python blog first created a train / test set then created a function to add the variables then applied that function to both sets. This is a very valid way of doing things when steps include normalizing and/or scaling data before applying our ML algorithms as we don’t want any leakage from our training set into our testing set.\n\n# Create features of df\ndef create_features(df, label = None): \n  df['date'] = df.index \n  df['hour'] = df['date'].dt.hour\n  df['day_of_week'] = df['date'].dt.dayofweek\n  df['day_of_year'] = df['date'].dt.dayofyear \n  df['day_of_month'] = df['date'].dt.day \n  df['week_of_year'] = df['date'].dt.isocalendar().week \n  df['month'] = df['date'].dt.month \n  df['quarter'] = df['date'].dt.quarter \n  df['year'] = df['date'].dt.year\n  \n  X = df[['hour', 'day_of_week', 'day_of_year', 'day_of_month', 'week_of_year', 'month', 'quarter', 'year']]\n  \n  if label: \n    y = df[label]\n    return X, y\n  \n  return X\n\nCompare this way of constructing variables to the much easier and more elegant tidyverse’s way of cleaning and creating variables. The dplyr package really makes it painless to wrangle data."
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-r-1",
    "href": "posts/xgboost-time-series/index.html#using-r-1",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using R",
    "text": "Using R\nRsample is the tidymodel package that deals with creating training and testing sets. There are really many methods available to do this, but we stick to the same methods provided in the original blog post. There are out-of-the-box methods to deal with timeseries like in this case.\n\nlibrary(rsample)\nprop_split = 1 - (nrow(df |&gt; filter(Datetime &gt; \"2016-01-01\")) / nrow(df))\ndf_split &lt;- initial_time_split(df |&gt; arrange(Datetime), prop = prop_split)\ndf_train &lt;- training(df_split)\ndf_test &lt;- testing(df_split)"
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-python-1",
    "href": "posts/xgboost-time-series/index.html#using-python-1",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using Python",
    "text": "Using Python\n\npy_x_train, py_y_train = create_features(py_df_train, label = \"AEP_MW\")\npy_x_test, py_y_test =   create_features(py_df_test, label = \"AEP_MW\")\n#When running xgboost, I got an issue with one of the type of the variable.  \n# Let's fix this. \npy_x_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 98594 entries, 2004-12-31 01:00:00 to 2015-01-02 00:00:00\nData columns (total 8 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   hour          98594 non-null  int64 \n 1   day_of_week   98594 non-null  int64 \n 2   day_of_year   98594 non-null  int64 \n 3   day_of_month  98594 non-null  int64 \n 4   week_of_year  98594 non-null  UInt32\n 5   month         98594 non-null  int64 \n 6   quarter       98594 non-null  int64 \n 7   year          98594 non-null  int64 \ndtypes: UInt32(1), int64(7)\nmemory usage: 6.5 MB\n\npy_x_train = py_x_train.astype(np.int64)\npy_x_test = py_x_test.astype(np.int64)\npy_x_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 98594 entries, 2004-12-31 01:00:00 to 2015-01-02 00:00:00\nData columns (total 8 columns):\n #   Column        Non-Null Count  Dtype\n---  ------        --------------  -----\n 0   hour          98594 non-null  int64\n 1   day_of_week   98594 non-null  int64\n 2   day_of_year   98594 non-null  int64\n 3   day_of_month  98594 non-null  int64\n 4   week_of_year  98594 non-null  int64\n 5   month         98594 non-null  int64\n 6   quarter       98594 non-null  int64\n 7   year          98594 non-null  int64\ndtypes: int64(8)\nmemory usage: 6.8 MB"
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-r-2",
    "href": "posts/xgboost-time-series/index.html#using-r-2",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using R",
    "text": "Using R\nAgain this is a very straightforward xgboost application to a dataset. No fine tuning of models, recipe, etc.\n\nlibrary(parsnip)\nmodel_xgboost &lt;- boost_tree(stop_iter = 50L, trees=1000L) |&gt; \n  set_engine(\"xgboost\") |&gt;\n  set_mode(\"regression\")\n  \nfit_xgboost &lt;- model_xgboost |&gt; \n  fit(AEP_MW ~., data = df_train %&gt;% select(-Datetime))\nfit_xgboost\n\nparsnip model object\n\n##### xgb.Booster\nraw: 4.7 Mb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.3, max_depth = 6, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1, \n    subsample = 1), data = x$data, nrounds = 1000L, watchlist = x$watchlist, \n    verbose = 0, early_stopping_rounds = 50L, nthread = 1, objective = \"reg:squarederror\")\nparams (as set within xgb.train):\n  eta = \"0.3\", max_depth = \"6\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", nthread = \"1\", objective = \"reg:squarederror\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  best_iteration, best_msg, best_ntreelimit, best_score, niter\ncallbacks:\n  cb.evaluation.log()\n  cb.early.stop(stopping_rounds = early_stopping_rounds, maximize = maximize, \n    verbose = verbose)\n# of features: 8 \nniter: 1000\nbest_iteration : 1000 \nbest_ntreelimit : 1000 \nbest_score : 242.3155 \nbest_msg : [1000]   training-rmse:242.315489 \nnfeatures : 8 \nevaluation_log:\n    iter training_rmse\n       1    11175.8839\n       2     7906.5875\n---                   \n     999      242.5272\n    1000      242.3155"
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-python-2",
    "href": "posts/xgboost-time-series/index.html#using-python-2",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using Python",
    "text": "Using Python\n\nfrom xgboost.sklearn import XGBRegressor\n\n/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n  warnings.warn(\n\npy_xgboost_mod = XGBRegressor(n_estimator = 1000, early_stopping_rounds = 50)\npy_xgboost_mod.fit(py_x_train, py_y_train, \n                   eval_set = [(py_x_train, py_y_train), (py_x_test, py_y_test)], \n                   verbose = True)\n\nXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n             early_stopping_rounds=50, enable_categorical=False,\n             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n             missing=nan, monotone_constraints='()', n_estimator=1000,\n             n_estimators=100, n_jobs=0, num_parallel_tree=1, predictor='auto',\n             random_state=0, reg_alpha=0, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n             early_stopping_rounds=50, enable_categorical=False,\n             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n             missing=nan, monotone_constraints='()', n_estimator=1000,\n             n_estimators=100, n_jobs=0, num_parallel_tree=1, predictor='auto',\n             random_state=0, reg_alpha=0, ...)"
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-r-3",
    "href": "posts/xgboost-time-series/index.html#using-r-3",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using R",
    "text": "Using R\n2 ways to do this … (actually more than 2 ways, but here are 2 main ways.). First one is a straight table using the xgboost library itself.\n\nlibrary(xgboost)\nxgb.importance(model = fit_xgboost$fit)\n\n[15:01:32] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\n\n        Feature        Gain       Cover    Frequency\n1:  day_of_year 0.361828048 0.455387001 0.2800303942\n2:         hour 0.336852823 0.125331328 0.2374139102\n3:         year 0.120129969 0.129691117 0.2000679018\n4:  day_of_week 0.105250594 0.073258066 0.1489636887\n5: week_of_year 0.047083085 0.097216236 0.0462379151\n6: day_of_month 0.027801118 0.116483820 0.0864293336\n7:        month 0.001054362 0.002632432 0.0008568565\n\n#detach(xgboost)\n\nAnd also a graphic way.\n\nlibrary(vip)\nfit_xgboost %&gt;%\n  vip(geom = \"point\")"
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-python-3",
    "href": "posts/xgboost-time-series/index.html#using-python-3",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using python",
    "text": "Using python\n\nfrom xgboost import plot_importance, plot_tree\n_ = plot_importance(py_xgboost_mod, height=0.9)\n\n\n\n\nI am a bit confused here in the output of the python graph with F-score vs the output of the R graph with importance."
  },
  {
    "objectID": "posts/xgboost-time-series/index.html#using-r-4",
    "href": "posts/xgboost-time-series/index.html#using-r-4",
    "title": "Translating Python Part 1 - Xgboost with Time-Series",
    "section": "Using R",
    "text": "Using R\nGraphing predicted power output vs actual power output could be a first way to see how our model fares in its predictions. So let’s graph our datetime vs power ouput for both actual and predicted.\n\nlibrary(tibble)  # for the add_column \nlibrary(parsnip)\ndf_test1 &lt;- add_column(df_test,  predict(fit_xgboost, new_data = df_test)) \nggplot(df_test1, aes(x= Datetime, y = AEP_MW)) + \n  geom_line(color = \"blue\") + \n  geom_line(aes(y = .pred), color = \"yellow\", alpha = 0.5) + \n  labs(title = \"Energy Consumption in 2016-2018 (in MWh)\", y = \"Hourly consumption\")\n\n\n\n\nFigure 2: Actual Vs Predicted power consumption for 2016-2018\n\n\n\n\nWe can already see that we are not really modeling well the peaks and through.\nWe could get slightly more granular and try to see whats going on.\n\nggplot(df_test1 %&gt;% filter(Datetime &gt; \"2016-01-01\" & Datetime &lt; \"2016-02-28\"), aes(x= Datetime, y = AEP_MW)) + \n  geom_line(color = \"blue\") + \n  geom_line(aes(y = .pred), color = \"yellow3\", alpha = 0.8)\n\n\n\n\nFigure 3: Actual Vs Predicted power consumption\n\n\n\n\nWe are clearly off there on the second half of February.\nNow, we can use the yardstick package to get numerical values to assess our model on the test set.\n\nlibrary(yardstick)\n# calculating the RMSE (root mean square error)\nrmse(df_test1, truth = AEP_MW, estimate = .pred, na_rm = TRUE)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       2067.\n\n# calculating the MAE (mean absolute error)\nmae(df_test1, truth = AEP_MW, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mae     standard       1495.\n\n# calculating the MAPE (mean absolute percent error)\nmape(df_test1, truth = AEP_MW, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 mape    standard        10.0\n\n# actually much easier to use the metric_set() function !\nxgboost_mod_metrics &lt;- metric_set(rmse, mae, mape)\nxgboost_mod_metrics(df_test1, truth = AEP_MW, estimate = .pred) \n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      2067. \n2 mae     standard      1495. \n3 mape    standard        10.0"
  },
  {
    "objectID": "posts/machine-learning-part1/kmeans-intro/index.html",
    "href": "posts/machine-learning-part1/kmeans-intro/index.html",
    "title": "Intro to Kmeans",
    "section": "",
    "text": "The purpose of this article is to get a deeper dive into Kmeans as an unsupervised machine learning algorithms. To see the algorithm at work on some financial data, you can use the post on kmeans and regime change.\nAs mentioned, Kmeans is an unsupervised learning algorithm, aiming to group “observations” based on their distances to a fixed number of “centroids” (aka: the fixed number is K). Each centroids is defined as the mean (in each dimension) of all the points belonging to that centroid. All the points belonging to a centroid makes a cluster: all the observations belonging to the k-centroid makes the k-cluster.\nThe objective is, for a given number of centroids (i.e. k), to minimize the total within-cluster variation or intra-cluster variation (distance from the observation to the centroid).\nThe standard Kmeans algorithm aims to minimize the total sum of the square distances (Euclidean distance) between observations and centroids.\nFirst, we calculate the within-centroid sum of square: \\[W(C_k) = \\sum_{x_i \\in C_k} (x_i - \\mu_k)^2 \\tag{1}\\]\nThe objective is to minimize the total within cluster variation, that is the total sum of the distance of each observations to its centroid and that for each of the k-centroids: \\[tot.withinss = \\sum_{k=1}^k W(C_k) = \\sum_{k=1}^k \\sum_{x_i \\in C_k} (x_i - \\mu_k)^2 \\tag{2}\\]"
  },
  {
    "objectID": "posts/machine-learning-part1/kmeans-intro/index.html#kmean---hartingan-wonk-in-practice.",
    "href": "posts/machine-learning-part1/kmeans-intro/index.html#kmean---hartingan-wonk-in-practice.",
    "title": "Intro to Kmeans",
    "section": "Kmean - Hartingan-Wonk in practice.",
    "text": "Kmean - Hartingan-Wonk in practice.\n\n# setting up the problem \nlibrary(tibble)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(glue)\n\nnum_obs = 50\nset.seed(1234)\ndf &lt;- tibble(x = runif(num_obs, -10, 10), y = runif(num_obs, -10, 10))\n\n# Step 1: choose number of centroids\nk = 3\n\nset.seed(441)\n# step 2: assign randomly each point to a cluster\ndf &lt;- df |&gt; mutate(centroid = sample(1:k, n(), replace = TRUE))\n\nWe are going to define a couple of function we will use more than once.\n\ncalculate_centroid_mean &lt;- function(df) {\n  yo &lt;- df |&gt; group_by(centroid) |&gt; \n    summarize(mean_x = mean(x), mean_y = mean(y)) \n  #print(yo)\n  return(yo)\n}\n\ntotal_within_cluster_distance &lt;- function(df) {\n  yo &lt;- df |&gt; \n    mutate(distance = (x - centroid_loc$mean_x[centroid])^2 + \n                      (y - centroid_loc$mean_y[centroid])^2)\n  return(sum(yo$distance))\n}\n\n# need to initialize an empty vector for the distance of the observation to a centroid\ndist_centroid &lt;- c()\n\n\n# Starting the iteration process \n\n# step3: \n## a: calculate mean of each centroids\ncentroid_loc &lt;- calculate_centroid_mean(df)\n\n## b: calculate sum of distance between each observation and its assigned cluster \nprint(glue('Initial sum of within-cluster distance is: ', \n           round(total_within_cluster_distance(df), 2)))\n\nInitial sum of within-cluster distance is: 3014.94\n\nrd = 1    # to keep track on how many round / loops we are doing\ni = 0     # to keep track on how many observations have not changed their centroids\n\n# we shall be running the loop until we have no more change of centroids  \nwhile (i &lt; num_obs) { \n  \n  i = 0   # we keep going with the process until no more change of centroids for all the observations\n  \n  # Step 4: for each data point \n  for (obs in 1:num_obs) { \n    \n    # for each centroid \n    for (centroid in 1:k) { \n      # find distance from the observation to the centroid \n      dist_centroid[centroid] = sqrt((df$x[obs] - centroid_loc$mean_x[centroid])^2 + \n                                     (df$y[obs] - centroid_loc$mean_y[centroid])^2) \n      # print(glue('   The distance from point ', obs, ' to centroid ', centroid, ' is ', round(dist_centroid[centroid], 2))) \n      } \n    \n    # assign the observation to its new centroid (based on min distance) \n    prev_centroid = df$centroid[obs] \n    post_centroid = which.min(dist_centroid)  \n    df$centroid[obs] = post_centroid    #assign the new centroid  \n    \n    if (prev_centroid != post_centroid) {  \n      # we recaluate the centroid\n      centroid_loc &lt;- calculate_centroid_mean(df)  \n      print(glue('  The initial centroid for point ', obs, ' was ', \n                 prev_centroid, '. It is now ', post_centroid)) \n      # print(centroid_loc) \n    } else {\n      i = i + 1\n      #print('  No change in centroid')\n    }\n  }\n  rd = rd + 1\n  print(glue('Round ', rd, '  The new sum of within-cluster distance is: ', \n             round(total_within_cluster_distance(df), 2)))\n}\n\n  The initial centroid for point 3 was 3. It is now 1\n  The initial centroid for point 5 was 2. It is now 3\n  The initial centroid for point 6 was 3. It is now 1\n  The initial centroid for point 8 was 1. It is now 2\n  The initial centroid for point 10 was 1. It is now 2\n  The initial centroid for point 11 was 1. It is now 2\n  The initial centroid for point 12 was 2. It is now 3\n  The initial centroid for point 13 was 2. It is now 3\n  The initial centroid for point 14 was 1. It is now 3\n  The initial centroid for point 15 was 2. It is now 3\n  The initial centroid for point 16 was 1. It is now 2\n  The initial centroid for point 19 was 1. It is now 3\n  The initial centroid for point 20 was 3. It is now 2\n  The initial centroid for point 21 was 1. It is now 3\n  The initial centroid for point 23 was 1. It is now 3\n  The initial centroid for point 24 was 1. It is now 2\n  The initial centroid for point 25 was 1. It is now 3\n  The initial centroid for point 26 was 2. It is now 1\n  The initial centroid for point 28 was 1. It is now 3\n  The initial centroid for point 30 was 3. It is now 2\n  The initial centroid for point 31 was 1. It is now 2\n  The initial centroid for point 32 was 1. It is now 2\n  The initial centroid for point 33 was 2. It is now 3\n  The initial centroid for point 34 was 1. It is now 2\n  The initial centroid for point 35 was 2. It is now 3\n  The initial centroid for point 37 was 3. It is now 2\n  The initial centroid for point 38 was 2. It is now 3\n  The initial centroid for point 39 was 3. It is now 1\n  The initial centroid for point 44 was 2. It is now 3\n  The initial centroid for point 45 was 2. It is now 3\n  The initial centroid for point 48 was 2. It is now 3\n  The initial centroid for point 49 was 2. It is now 3\nRound 2  The new sum of within-cluster distance is: 1594.37\n  The initial centroid for point 2 was 3. It is now 1\n  The initial centroid for point 3 was 1. It is now 2\n  The initial centroid for point 5 was 3. It is now 1\n  The initial centroid for point 9 was 3. It is now 1\n  The initial centroid for point 14 was 3. It is now 1\n  The initial centroid for point 17 was 1. It is now 3\n  The initial centroid for point 28 was 3. It is now 1\n  The initial centroid for point 37 was 2. It is now 3\n  The initial centroid for point 41 was 3. It is now 1\n  The initial centroid for point 44 was 3. It is now 1\nRound 3  The new sum of within-cluster distance is: 1146.17\n  The initial centroid for point 7 was 2. It is now 3\n  The initial centroid for point 12 was 3. It is now 1\n  The initial centroid for point 32 was 2. It is now 3\nRound 4  The new sum of within-cluster distance is: 1081.9\n  The initial centroid for point 18 was 2. It is now 3\nRound 5  The new sum of within-cluster distance is: 1071.19\nRound 6  The new sum of within-cluster distance is: 1071.19\n\nprint(centroid_loc)\n\n# A tibble: 3 × 3\n  centroid  mean_x mean_y\n     &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1        1  4.41    -4.99\n2        2 -0.0601   5.27\n3        3 -5.04    -5.47\n\nggplot(df, aes(x, y)) + \n  geom_point(aes(color = as.factor(centroid))) + \n  geom_point(data = centroid_loc, aes(x = mean_x, y = mean_y)) + \n  theme(legend.position = 'none')\n\n\n\n\n\nyoo &lt;- kmeans(df[, 1:2], centers = 3, \n              algorithm = 'Hartigan-Wong', nstart = 10)\nyoo$tot.withinss\n\n[1] 1017.417\n\nlibrary(broom)\ndf2 &lt;- df\naugment(yoo, df2) |&gt; \n  ggplot(aes(x, y)) + \n    geom_point(aes(color = .cluster)) + \n    geom_point(data = as_tibble(yoo$centers), aes(x, y)) + \n    theme(legend.position = 'none')"
  },
  {
    "objectID": "posts/machine-learning-part1/kmeans-intro/index.html#number-of-clusters",
    "href": "posts/machine-learning-part1/kmeans-intro/index.html#number-of-clusters",
    "title": "Intro to Kmeans",
    "section": "Number of clusters",
    "text": "Number of clusters\n\nThe elbow method\nThe idea is to identify where does drop in the total within-cluster sum of square start to slowdown. Of course the total within-clusters sum of square decrease as the number of centroids increase. If we have n centroids (that is \\(n = k\\) - as many centroids as observations), the total within-cluster sum of square will be 0. And if we have only one centroid, the total within-one-cluster sum of square will be the sum of square of the mean of each of the variables.\nSo when does adding a centroid does not significantly reduce the total within-cluster sum of square.\n\n\nThe silhouette method"
  },
  {
    "objectID": "posts/machine-learning-part1/kmeans-intro/index.html#number-of-iterations",
    "href": "posts/machine-learning-part1/kmeans-intro/index.html#number-of-iterations",
    "title": "Intro to Kmeans",
    "section": "Number of iterations",
    "text": "Number of iterations"
  },
  {
    "objectID": "posts/machine-learning-part1/kmeans-intro/index.html#number-of-start",
    "href": "posts/machine-learning-part1/kmeans-intro/index.html#number-of-start",
    "title": "Intro to Kmeans",
    "section": "Number of start",
    "text": "Number of start"
  },
  {
    "objectID": "posts/machine-learning-part1/kmeans-intro/index.html#con",
    "href": "posts/machine-learning-part1/kmeans-intro/index.html#con",
    "title": "Intro to Kmeans",
    "section": "con",
    "text": "con\n\nYou don’t always know in advance thee number of centroids. You can use the elbow method or the silhouette method to determine the numbers of centroids you want.\n\nbecause of the random initialization stage, results might not necessarily be reproducible. If results have to be reproduced, then you need to set a seed."
  },
  {
    "objectID": "posts/machine-learning-part1/linear-regression/index.html",
    "href": "posts/machine-learning-part1/linear-regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is probably the most basic example of a machine learning algorithms."
  },
  {
    "objectID": "posts/machine-learning-part1/linear-regression/index.html#finding-the-coefficients-from-scratch",
    "href": "posts/machine-learning-part1/linear-regression/index.html#finding-the-coefficients-from-scratch",
    "title": "Linear Regression",
    "section": "Finding the coefficients from scratch",
    "text": "Finding the coefficients from scratch\nIn the case of simple linear regression, we just have one independent variable and one dependent variable. Let’s say we have \\(n\\) observations \\((x_i, y_i)\\) and we want to find a linear equations that predict y \\(\\hat{y_i}\\) based on a given \\(x_i\\).\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\]\n\n\\(x_i\\) is the independent variable (aka: predictor, explanatory variable)\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters of our model that have to be found.\n\n\\(\\beta_0\\) is the intercept (value of y when x=0)\n\\(\\beta_1\\) is the slope of our linear model\n\n\\(\\epsilon_i\\) is the residual or error term of the \\(i^{th}\\) observations\n\nFrom a probabilistic perspective, \\(\\epsilon\\) can be seen as a random variable with the following properties: \\(E(\\epsilon)=0\\) and \\(Var(\\epsilon)= \\sigma_{\\epsilon}^2 = \\sigma^2\\)\n\n\\(\\hat{y_i}\\) is the estimated or predicted value of y. In that sense \\(\\hat{y_i} = \\beta_0 + \\beta_1 x_i\\). The error term is then the difference between the actual y and the predicted y: \\(\\epsilon_i = y_i - \\hat{y_i}\\)\n\nThe cost function (or loss function) is to minimize the sum of squared error. In that sense, we seek to minimize \\[ \\text{min } SSE = min \\sum_{i=1}^{n} \\epsilon_i^2 =\n        \\underset{\\beta_0, \\beta1}{argmin} \\sum_{i=1}^{n} (y_i-\\beta_0 - \\beta_1 x_i)^2 \\tag{1}\\]\n\n\n\nTrying to minimize the sum of the squared of the vertical bars\n\n\nTo minimize the SSE, we will need to use partial derivatives for both coefficients and solve it for 0.\nLet’s first focus on \\(\\beta_0\\)\n\\[\\frac{\\partial SSE}{\\partial \\beta_0} = -2 \\sum_{i=1}^{n} (y_i-\\beta_0 - \\beta_1 x_i) = 0\\] Breaking down our sum: \\[\\sum_{i=1}^{n} y_i - \\sum_{i=1}^{n} \\beta_0 - \\sum_{i=1}^{n} \\beta_1 x_i = 0\\] \\(\\beta0\\) and \\(\\beta_1\\) are coefficient, hence: \\[\\sum_{i=1}^{n} y_i - n \\beta_0 - \\beta_1 \\sum_{i=1}^{n} x_i = 0\\] and \\[\\beta_0 = \\frac{\\sum_{i=1}^{n} y_i - \\beta_1 \\sum_{i=1}^{n} x_i}{n}\\] \\(\\bar{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}\\) (mean of y values) and \\(\\bar{x}=\\frac{\\sum_{i=1}^{n} x_i}{n}\\) (mean of x values).\nand our previous equation can then be simplified as \\[\\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\tag{2}\\]\nLet’s now address the second partial derivative wrt \\(\\beta_1\\).\n\\[\\frac{\\partial SSE}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i (y_i-\\beta_0 - \\beta_1 x_i) = 0\\].\nDistributing the \\(x_i\\) and substituting in the value of \\(\\beta_0\\) from Equation 2\n\\[\\sum_{i=1}^{n} (x_i y_i - x_i (\\bar{y} - \\beta_1 \\bar{x}) - \\beta_1 x_i^2) = 0\\].\nFactoring \\(\\beta_1\\) and breaking down the sum and being careful to the sign, we get: \\[\\beta_1 = \\frac{\\sum_{i=1}^{n} x_i (y_i - \\bar{y})}{\\sum_{i=1}^{n} x_i (x_i - \\bar{x})} \\tag{3}\\]\nKnowing \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\) and \\(\\bar{y} = \\frac{\\sum_{i=1}^{n} y_i}{n}\\), we can get one step further (have a common denominator with over n). I have also removed the index on the sum for readability.\n\\[\\beta_1 = \\frac{n \\Sigma x_i y_i - \\Sigma x_i \\Sigma y_i}{n \\Sigma x_i^2 - (\\Sigma x_i)^2} \\tag{4}\\]\nWe can now use the values of \\(\\beta_0\\) Equation 2 and \\(\\beta_1\\) Equation 4 into our estimate of y: \\(\\hat{y_i} = \\beta_0 + \\beta_1 x_i\\)"
  },
  {
    "objectID": "posts/machine-learning-part1/linear-regression/index.html#linking-the-slope-and-covariance",
    "href": "posts/machine-learning-part1/linear-regression/index.html#linking-the-slope-and-covariance",
    "title": "Linear Regression",
    "section": "Linking the slope and covariance",
    "text": "Linking the slope and covariance\nWhile going over some textbooks or online resources, we find another formula for the slope of our regression line. That formula involve the covariance and or the Pearson coefficient of correlation.\n\\[\\beta_1 = \\frac{Cov(x, y)}{\\sigma^2 x} \\tag{5}\\]\nNow let’s connect both Equation 3 and Equation 5\nI’ll rewrite Equation 3 in a slightly simpler form just to lighten the notation \\[\\beta_1 = \\frac{\\sum x_i (y_i - \\bar{y})}{\\sum x_i (x_i - \\bar{x})}\\]\nNow, it can be noted that that \\(\\sum (x_i - \\bar{x}) = 0\\) or similarly \\(\\sum (y_i - \\bar{y}) = 0\\). Hence \\(\\bar{x} \\sum (y_i - \\bar{y}) = 0\\)\nConsidering \\(\\bar{x}\\) or \\(\\bar{y}\\) are constant, we could also write \\(\\sum \\bar{x} (y_i - \\bar{y}) = 0\\) and similarly \\(\\sum \\bar{x} (x_i - \\bar{x}) = 0\\).\nWith that in mind, we can now, go back on our Equation 3 \\[\\beta_1 = \\frac{\\sum x_i (y_i - \\bar{y})}{\\sum x_i (x_i - \\bar{x})} =\n\\frac{\\sum x_i (y_i - \\bar{y}) - \\sum \\bar{x} (y_i - \\bar{y})}{\\sum x_i (x_i - \\bar{x}) - \\sum \\bar{x} (x_i - \\bar{x})}\\]\n\\[\\beta1 = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\]\nDefining \\(Cov(x, y) = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{n}\\) and \\(\\sigma_x = \\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n}}\\)\nWe can finally rewrite\n\\[\\beta_1 = \\frac{Cov(x, y)}{\\sigma_x^2}\\]\nFinally, if we want to involve the Pearson coefficient of correlation \\(\\rho = \\frac{Cov(x, y)}{\\sigma_x \\sigma_y}\\), we could also re-write our slope as \\[\\beta_1 = \\rho \\frac{\\sigma_y}{\\sigma_x}\\]"
  },
  {
    "objectID": "posts/machine-learning-part1/linear-regression/index.html#considerations-when-doing-linear-regression",
    "href": "posts/machine-learning-part1/linear-regression/index.html#considerations-when-doing-linear-regression",
    "title": "Linear Regression",
    "section": "Considerations when doing linear regression",
    "text": "Considerations when doing linear regression\n\nStart with a scatter plot to check if data have a linear trend. No points of doing a linear regression on a set of data, if data are not showing a linear trend.\n\nHow well the data fits the regression line (correlation) have NO incidence on causality. Correlation is no indication of causation\nVariables have to be normally distributed. This can be checked using histogram or QQ-plot or some other stat tests - Shapiro-Wilk test, Kolmogorov–Smirnov test. Skewness and kurtosis can also be used for that. In case of violation of this assumption, a Box-Cox transformation could be used.\nHomoscedasticity in the residuals. Variance in the spread of residuals should be constant. \nError terms are normally distributed (visual: histogram, QQ-plot)\nIn the case of multi-variables linear regression, ensure no correlation between independent variables"
  },
  {
    "objectID": "posts/quant-part2/SDE-Part1/index.html",
    "href": "posts/quant-part2/SDE-Part1/index.html",
    "title": "01 - Stochastic Differential Equation - Part I",
    "section": "",
    "text": "Function of a stochastic process \\(X_t\\) - Itô I\nLet \\(F\\) be a function of a stochastic random variable \\(X_t\\). So \\(F = F(X_t)\\).\nUsing a Taylor expansion: \\[F(X + dX) \\approx F(X) + \\frac{dF}{dX} \\space dX + \\frac{1}{2} \\frac{d^2F}{dX^2} dX^2\\] Using \\(F(X+dX) - F(X) = dF\\): \\[dF = \\frac{dF}{dX} \\space dX + \\frac{1}{2} \\frac{d^2F}{dX^2} dX^2\\]\nNow, recall \\(dX\\) is a random variable with mean 0 and standard deviation \\(\\sqrt{t}\\). Also, for small values of \\(t\\), we have \\(\\sqrt{dt} \\gt dt\\), hence it is of higher order than dt. Hence, we re-write the previous equation as\n\\[dF = \\frac{dF}{dX} \\space dX + \\frac{1}{2} \\frac{d^2F}{dX^2} dt\\] Usually, we write the deterministic part of the equation first.\n\\[dF = \\frac{1}{2} \\frac{d^2F}{dX^2} \\space dt + \\frac{dF}{dX} \\space dX \\tag{1}\\]\nAll our Stochastic Differential Equations will have in them these 2 different time steps: \\(dt\\) and \\(\\sqrt{dt}\\). \\[dF = \\underbrace {\\dots \\space dt}_{deterministic-drift} + \\underbrace{\\dots \\space dX}_{random-diffusion}\\]\nThe integral form of this Stochastic Differential Equation is \\[\\int_0^t \\frac{dF}{dX_\\tau} \\space dX_\\tau = \\int_0^t dF - \\frac{1}{2} \\int_0^t \\frac{d^2F}{dX_\\tau^2} \\space d\\tau\\]\n\n\n\n\n\n\nExample 1\n\n\n\n\\(F(X_t) = X^2\\), where \\(X_t\\) is stochastic random variable. \\[\\frac{dF}{dX} = 2X\\] \\[\\frac{d^2F}{dX^2} = 2\\]\nHence, using the previous Equation 1 (aka itô I): \\[dF = \\frac{1}{2} \\cdot 2 \\space dt + 2X \\space dX\\] \\[dF = dt + 2X \\space dX\\] Using integral form\nWe could integrate both side of the previous equation. \\[\\int_0^t dF = \\int_0^t d\\tau + \\int_0^t 2X(\\tau) d{\\tau}\\] \\[F(t) - F(0) = t - 0 + 2 \\int_0^t X(\\tau) d{\\tau}\\] And assuming F(0) = 0: \\[F(t) = t + 2 \\int_0^t X_{\\tau} \\space d{\\tau}\\] \\[X^2(t) = t + 2 \\int_0^t X_{\\tau} \\space d{\\tau}\\]\n\n\n\n\n\n\n\n\nexample 2\n\n\n\n\\(F(W_t) = sin \\space W_t + cos \\space W_t\\), where \\(X_t\\) is stochastic random variable. \\[\\frac{dF}{dW_t} = cos \\space W_t - sin \\space W_t\\] \\[\\frac{d^2F}{dW_t^2} = -(sin \\space W_t + cos \\space W_t)\\] Hence, we can now write\n\\[dF = - \\frac{1}{2}(sin \\space W_t + cos \\space W_t) \\space dt + (cos \\space W_t - sin \\space W_t) \\space dW_t\\]\n\n\n\n\nFunction of a stochastic process \\((t, X(t))\\) - Itô II\nLet \\(F\\) be a function of both time \\(t\\) and a stochastic random variable \\(X_t\\). So \\(F = F(t, X_t)\\).\nUsing a Taylor expansion: \\[F(t + dt, X + dX) \\approx F(t, X) + \\frac{\\partial F}{\\partial t} \\space dt + \\frac{\\partial F}{\\partial X} \\space dX + \\frac{1}{2} \\frac{d^2F}{dX^2} dX^2\\] Using \\(F(t + dt, X+dX) - F(t, X) = dF\\) and factoring the \\(dt\\) and recall \\(dX^2=dt\\):\n\\[dF = \\left( \\frac{\\partial F}{\\partial t} + \\frac{1}{2} \\frac{\\partial ^2F}{\\partial X^2} \\right) dt + \\frac{\\partial F}{\\partial X} dX \\tag{2}\\]\nWe can transform this Stochastic Differential Equation (SDE) into an itô integral.\n\\[\\int_0^t  \\frac{\\partial F}{\\partial X_\\tau} dX_\\tau = \\int_0^t dF - \\int_0^t \\left( \\frac{\\partial F}{\\partial \\tau} + \\frac{1}{2} \\frac{\\partial ^2F}{\\partial X_\\tau^2} \\right) d\\tau\\] \\[\\int_0^t  \\frac{\\partial F}{\\partial X_\\tau} dX_\\tau = F(t, X_t) - F(0, X_0) - \\int_0^t \\left( \\frac{\\partial F}{\\partial \\tau} + \\frac{1}{2} \\frac{\\partial ^2F}{\\partial X_\\tau^2} \\right) d\\tau \\tag{3}\\]\n\n\n\n\n\n\nExample 3\n\n\n\nExpress \\(\\int_0^t \\left( \\tau + W_\\tau \\right) dW_\\tau\\)\nUsing Equation 3, we establish that what is being integrated is \\(\\frac{\\partial F}{\\partial W_\\tau}\\), hence \\[\\frac{\\partial F}{\\partial W_\\tau} = \\tau + W_\\tau \\tag{4}\\]\nThis allows us to find \\(F\\) by integrating both side in regards to \\(W_\\tau\\). \\[F = \\tau W_\\tau + \\frac{1}{2} W_\\tau^2 \\tag{5}\\] and derving Equation 5 for \\(\\tau\\), \\[\\frac{\\partial F}{\\partial \\tau} = W_\\tau \\tag{6}\\] and deriving the first derivative Equation 4 one more time for \\(dW_\\tau\\), we get \\[\\frac{\\partial ^2F}{\\partial W_\\tau^2} = 1\\] Hence, we can re-write \\[\\int_0^t \\left( \\tau + W_\\tau \\right) dW_\\tau = \\left( \\tau W_\\tau + \\frac{1}{2} W_\\tau^2 \\right) - \\int_0^t W_\\tau + \\frac{1}{2} \\space d_\\tau\\]"
  },
  {
    "objectID": "posts/quant-part2/SDE-part2/index.html",
    "href": "posts/quant-part2/SDE-part2/index.html",
    "title": "02 - Stochastic Differential Equation - Part II",
    "section": "",
    "text": "Let \\(V\\) be a function of \\(S\\) where \\(S\\) satisfies the stochastic differential equation \\(dS = \\mu S \\space dt + \\sigma S \\space dX(t)\\) Note how in this case \\(\\mu\\) and \\(\\sigma\\) are constants. In more elaborate models, both can be time-dependent variables and be stochastic themselves.\nUsing a one-dimension Taylor Series expansion, we can write \\[V(S + dS) \\approx V(S) + \\frac{dV}{dS} \\space dS + \\frac{1}{2} \\frac{d^2V}{dS^2} \\space dS^2\\]\nTo express \\(dS^2\\)? \\[dS^2 = (dS)^2 = \\mu^2 S^2 \\space dt^2 + 2 \\mu \\sigma S^2  \\space dt dX(t) + \\sigma^2 S^2 \\space dX(t)^2\\]\n\\[dS^2 = \\sigma^2 S^2 \\space dX(t)^2 = \\sigma^2 S^2 \\space dt \\tag{1}\\]\n\n\n\n\n\n\nTip\n\n\n\nWe could generalize this a bit further Let’s have a function \\[dG_t = A(t, X_t) \\space dt + B(t, X_t) \\space dW_t\\]\nThen \\[\\mathbb{E}[dG_t] = \\mathbb{E}[A \\space dt] + \\mathbb{E}[B \\space dW_t] = A \\space \\mathbb{E}[dt] + B \\space \\mathbb{E}[dW_t]\\]\nRecall that \\(\\mathbb{E}[dW_t] = 0\\), hence \\[\\mathbb{E}[dG_t] = A \\space \\mathbb{E}[dt]\\] Considering the variance, we can write \\[\\mathbb{V}ar[dG_t] = \\mathbb{V}ar[A \\space dt] + \\mathbb{V}ar[B \\space dW_t] = A^2 \\space \\mathbb{V}ar[dt] + B^2 \\space \\mathbb{V}ar[dW_t] = B^2 \\space dt\\] Recall that \\(\\mathbb{V}ar[dt] = 0\\), hence \\[\\mathbb{V}ar[dG_t] = B^2 \\space \\mathbb{V}ar[dW_t]\\]\n\n\nGoing back to our expansion and considering \\(dV = V(S+dS) - V(S)\\): \\[dV = \\frac{dV}{dS} \\space (\\mu S \\space dt + \\sigma S \\space dX(t)) + \\frac{1}{2} \\frac{d^2V}{dS^2} \\space (\\sigma^2 S^2 \\space dt)\\] \\[dV = \\left( \\mu S \\frac{dV}{dS} + \\frac{1}{2} \\sigma^2 S^2 \\frac{d^2V}{dS^2} \\right) \\cdot dt + \\left( \\sigma S \\frac{dV}{dS} \\right) \\cdot dX(t) \\tag{2}\\]\n\n\n\n\n\n\nExample\n\n\n\nLet \\(V(S) = log(S)\\) with S satisfies the usual SDE: \\(dS = \\mu S dt + \\sigma S dX_t\\). We can then use the above SDE form.\n\\[\\frac{dV}{dS} = \\frac{1}{S}\\] \\[\\frac{d^2V}{dS^2} = - \\frac{1}{S^2}\\]\nUsing above Equation 2: \\[dV = \\left( \\mu S \\frac{1}{S} + \\frac{1}{2} \\sigma^2 S^2 \\frac{-1}{S^2} \\right) \\cdot dt + \\left( \\sigma S \\frac{1}{S} \\right) \\cdot dX(t)\\] \\[dV = \\left( \\mu - \\frac{1}{2} \\sigma^2  \\right) \\cdot dt + \\sigma  \\cdot dX(t)\\] Using the integral form:\n\\[\\int_0^t d(log \\space S) = \\int_0^t \\mu - \\frac{1}{2} \\sigma^2 \\space d\\tau + \\int_0^t \\sigma \\space dX(\\tau)\\] \\[log(S_t) - log(S_0) = \\mu t - \\frac{1}{2} \\sigma^2 t + \\sigma (X_t - X_0)\\] \\[log \\left( \\frac{S_t}{S_0} \\right) = \\mu t - \\frac{1}{2} \\sigma^2 t + \\sigma (X_t - X_0)\\] \\[S_t = S_0 \\cdot e^{\\mu t - \\frac{1}{2} \\sigma^2 t + \\sigma (X_t - X_0)}\\]\nUsing \\(X_0 = 0\\) and \\(X_t = \\phi \\sqrt{t}\\): \\[S_t = S_0 \\cdot e^{\\mu t - \\frac{1}{2} \\sigma^2 t + \\sigma \\phi \\sqrt{t}}\\]\n\n\nAnother example with interest rate\n\n\n\n\n\n\nVasicek model\n\n\n\nThis model developed in 1978 by Vasicek is about interest rate.\nThe basic SDE takes this form \\[dr = (\\eta - \\gamma r) dt + \\sigma dX \\tag{3}\\] In this model, \\(\\eta\\), \\(\\gamma\\) and \\(\\sigma\\) are all constant.\n\n\\(\\gamma\\) is the speed of reversion to the (long term) mean rate. It’s the rate of reversion.\nwe demote \\(\\bar{r}\\) the mean interest rate such that \\(\\bar{r} = \\frac{\\eta}{\\gamma}\\)\n\n\\[dr = \\gamma (\\bar{r} - r) \\space dt + \\sigma \\space dX \\tag{4}\\]\nIf we let \\(u = r - \\bar{r}\\), then \\(du = dr\\) because we consider \\(\\bar{r}\\) as a constant. Hence \\[du = - \\gamma u \\space dt + \\sigma dX\\] \\[du + \\gamma u dt = \\sigma dX\\]\n\\[e^{\\gamma t} du + \\gamma u e^{\\gamma t} dt = e^{\\gamma t} \\sigma dX\\] \\[d(u e^{\\gamma t}) = \\sigma e^{\\gamma t} dX\\] \\[\\int_0^t d(u_s e^{\\gamma s}) = \\sigma \\int_0^t e^{\\gamma s} dX_s\\] \\[u(t) e^{\\gamma t} - u(0) = \\sigma \\int_0^t e^{\\gamma s} dX_s\\] \\[u(t) = u(0) e^{-\\gamma t} +  \\sigma \\int_0^t e^{\\gamma (s - t)} dX_s\\]"
  },
  {
    "objectID": "posts/quant-part2/SDE-part2/index.html#steady-state",
    "href": "posts/quant-part2/SDE-part2/index.html#steady-state",
    "title": "02 - Stochastic Differential Equation - Part II",
    "section": "Steady-state",
    "text": "Steady-state\nIn some case, there are situation (random-walk) with a long term mean reversal - we say that they have a steady state distribution. This means that in the long run, the \\(p(y, t; y', t')\\) doesn’t depend of the starting point \\(y, t\\); the probability becomes time independent. Think of situations such as interest rate and volatility.\nIn the case of a steady state situation, \\(\\frac{\\partial P}{\\partial t'} = 0\\) since the process becomes time independent in the long run. And the probability (now written $p_{} (y’) $) satisfies the ordinary differential equation: \\[\\frac{1}{2} \\frac{d \\left(B(y')^2 p_{\\infty} \\right)}{d y'^2} - \\frac{d \\left( A(y') p_{\\infty} \\right)}{d y'} = 0 \\tag{8}\\] It isn’t anymore a partial differential equation as the time component vanishes (aka steady-state, long-term reversal)\n\n\n\n\n\n\nVasicek revisited\n\n\n\nRecall from above the Vasicek model \\[dr = \\gamma (\\bar{r} - r) \\space dt + \\sigma \\space dW_t\\] Using Equation 8, we can write the steady state distribution \\(p_{\\infty} r'\\) following ordinary differential equation. \\[\\frac{1}{2} \\sigma^2 \\frac{d^2  p_{\\infty} }{d r'^2} - \\gamma \\frac{d( \\left(\\bar{r} - r) p_{\\infty} \\right)}{d r'} = 0\\] Integrating both sides, we get: \\[\\frac{1}{2} \\sigma^2 \\frac{d  p_{\\infty} }{d r'} + \\gamma (r - \\bar{r}) p_{\\infty} = K\\] K being a constant. \\[\\frac{1}{2} \\sigma^2 \\frac{d  p_{\\infty} }{d r'} = - \\gamma (r - \\bar{r}) p_{\\infty} + K\\] Letting the constant be 0 (need explanation here) Letting the \\({\\infty}\\) just for convenience purposes.\n\\[\\frac{1}{p} dp = \\frac{-2 \\gamma}{\\sigma^2} (r - \\bar{r}) \\space dr'\\] Integrating both sides, \\[\\int \\frac{1}{p} dp = \\frac{-2 \\gamma}{\\sigma^2} \\int (r - \\bar{r}) \\space dr'\\] \\[log(p) = \\frac{-2 \\gamma}{\\sigma^2} \\frac{1}{2} (r - \\bar{r})^2 + K\\] Using a normalizing constant, as we inverse the log \\[p(r) = A \\cdot e^{- \\frac{\\gamma}{\\sigma^2} (r - \\bar{r})^2} \\tag{9}\\]\nWe know that \\(\\int_{\\mathbb{R}} p(r) dr= 1\\), hence \\[A \\int_{-\\infty}^{\\infty} e^{- \\frac{\\gamma}{\\sigma^2} (r - \\bar{r})^2} dr = 1\\] We can integrate this using substitution \\(u = \\frac{\\sqrt{\\gamma} \\space (r - \\bar{r})}{\\sigma}\\) with \\(\\frac{du}{dr} = \\frac{\\sqrt{\\gamma}}{\\sigma}\\) \\[A \\int_{-\\infty}^{\\infty} e^{-u^2} \\frac{\\sigma}{\\sqrt{\\gamma}} du = 1\\] \\[A \\frac{\\sigma}{\\sqrt{\\gamma}} \\int_{-\\infty}^{\\infty} e^{-u^2} du = 1\\] \\[A \\frac{\\sigma}{\\sqrt{\\gamma}} \\sqrt{\\pi}  = 1\\] \\[A = \\frac{\\sqrt{\\gamma}}{\\sigma \\sqrt{\\pi}} = \\frac{1}{\\sigma} \\sqrt{\\frac{\\gamma}{\\pi}}\\]\nPutting it all back together in Equation 9: \\[p_{\\infty}(r) = A \\cdot e^{- \\frac{\\gamma}{\\sigma^2} (r - \\bar{r})^2} = \\frac{1}{\\sigma} \\sqrt{\\frac{\\gamma}{\\pi}} e^{- \\frac{\\gamma}{\\sigma^2} (r - \\bar{r})^2}\\] This means: in our case of a steady state stochastic process, the variable \\(r\\) follows a normal distribution with mean \\(\\bar{r}}\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt{2 \\cdot \\gamma}}\\)"
  },
  {
    "objectID": "posts/quant-part3/monte-carlo-sim/index.html",
    "href": "posts/quant-part3/monte-carlo-sim/index.html",
    "title": "Modeling Option prices using Monte-Carlo simulations",
    "section": "",
    "text": "Recall Monte-Carlo method exploits the relationship between options prices and expectation under a risk-neutral measure. It is the present value of the expectation (under a risk-neutral measure) of the payoff. In this sense \\[V(S, t) = \\text{PV} \\space \\space \\mathbb{E}^\\mathbb{Q} (Payoff)\\]\nWe start with the usual SDE (except we use \\(r\\) instead of \\(\\mu\\) as we are under the risk-neutral framework). \\[dS_t = r S_t dt + \\sigma S_t dW_t\\] Using the Euler discretization \\[S_{t + \\delta t} = S_t \\cdot (1 + r \\delta t + \\sigma \\sqrt{\\delta t} \\phi)\\]"
  },
  {
    "objectID": "posts/quant-part3/monte-carlo-sim/index.html#using-python",
    "href": "posts/quant-part3/monte-carlo-sim/index.html#using-python",
    "title": "Modeling Option prices using Monte-Carlo simulations",
    "section": "Using Python",
    "text": "Using Python\n\nimport pandas as pd\nimport numpy as np\n\ndef simulate_path(s0, mu, sigma, Time, num_timestep, n_sim): \n  \n  np.random.seed(20230902)\n  \n  S0 = s0\n  r = mu\n  T = Time\n  t = num_timestep \n  n = n_sim \n  \n  #defining dt\n  dt = T/t\n  \n  S = np.zeros((t, n))\n  S[0] = S0\n  \n  for i in range(0, t-1): \n    w = np.random.standard_normal(n)\n    S[i+1] = S[i] * (1 + r * dt + sigma * np.sqrt(dt) * w)\n  \n  return S\n\nLet’s create a simulation for a quarter of a year (3 months or 63 trading days).\n\nsimulate_path(s0=100, mu=0.045, sigma=0.17, Time=0.25, num_timestep=63, n_sim=100)\n\narray([[100.        , 100.        , 100.        , ..., 100.        ,\n        100.        , 100.        ],\n       [ 99.39825714, 100.88405395, 100.17361119, ..., 100.79029332,\n         98.89439673,  99.86236711],\n       [ 99.50936214, 100.97945468,  99.7824842 , ...,  98.66331487,\n         98.67131431, 100.50278255],\n       ...,\n       [100.32398459, 110.16941406,  95.79494772, ..., 101.76681189,\n         91.43131552,  98.94795092],\n       [100.93630069, 111.0365789 ,  94.89177952, ..., 101.32109813,\n         93.37392012,  98.42725475],\n       [101.17836924, 110.76099538,  95.51591487, ..., 101.28364139,\n         92.50938162,  96.80815562]])\n\n\nLet’s put that into a data frame for further plotting and manipulation\nNote each column of the data frame is a simulation. The number of rows is the number of time steps.\n\nsimulated_paths = pd.DataFrame(simulate_path(s0=100, mu=0.045, sigma=0.17, Time=0.25, num_timestep=63, n_sim=100))\n\n\nsimulated_paths.iloc[-1].hist(bins = 100)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.plot(simulated_paths)  #plot the first 100 paths\nplt.xlabel('time steps')\nplt.xlim(0, 64)\nplt.ylim(75, 135)\nplt.ylabel('Pries')\nplt.title('Monte-Carlo Simulation of an Asset Price')\nplt.show()\n\n\n\n\nUnder the risk-neutral measure, the value of the option is the discounted value of the expected payoff. \\[C = e^{rT} \\cdot \\mathbb{E}[max(S_T - K, 0)]\\]\n\n\\(K\\) is the strike price\n\nFor this simulation, we let \\(K=100\\) as well!\n\nK = 100\nr = 0.045\nT = 0.25 \n\nS = simulate_path(s0=100, mu=0.045, sigma=0.17, Time=0.25, num_timestep=63, n_sim=10000)\n\n## calculate payoff for call options \nCo = np.exp(-r*T) * np.mean(np.maximum(0, S[-1]-K))\n## calculate payoff for put options \nPo = np.exp(-r*T) * np.mean(np.maximum(0, K - S[-1]))\n\nprint(f\"European Call Option value is {Co: 0.4f}\")\nprint(f\"European Put Option value is {Po: 0.4f}\")\n\nEuropean Call Option value is  3.8587\nEuropean Put Option value is  2.7757\n\n\n\nimport matplotlib.pyplot as plt\n\nsT= np.linspace(50,150,100)\n\nfigure, axes = plt.subplots(1, 2, figsize=(20, 6), sharey = True)\ntitle = ['Call payoff', 'Put payoff']\npayoff = [np.maximum(0, sT-K), np.maximum(0, K-sT)] \ncolor = ['green', 'red'] \nlabel = ['Call', 'Put']\n\nfor i in range(2): \n  axes[i].plot(sT, payoff[i], color = color[i], label = label[i])\n  axes[i].set_title(title[i])\n  axes[i].legend()\n\nfigure.suptitle('Option Payoff at Maturity')\n\nplt.show()"
  },
  {
    "objectID": "posts/algebra-quant/index.html",
    "href": "posts/algebra-quant/index.html",
    "title": "Algebra For Quant",
    "section": "",
    "text": "I am storing here a few nuggets of algebra, I need for quantitative finance and machine learning.\n\n\\(e^x\\) as an infinite serie\nUsing the McLaurin series expansion, we can define \\(e^x\\) as an infinite sum.\nHere is how it goes: \\[e^x \\approx f(0) + f'(0) \\frac{x}{1} + f''(0) \\frac{x^2}{2!} + f'''(0) \\frac{x^3}{3!} + \\cdots + f^n(0) \\frac{x^n}{n!}\\]\nAs \\(f(0) = f'(0) = f''(0) = f^n(0) = e^0 = 1\\), we can rewrite our previous expression as\n\\[e^x \\approx 1 + \\frac{x}{1} + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots + \\frac{x^n}{n!}\\]\nHence: \\[e^x \\approx \\sum_{n=1}^\\infty \\frac{x^n}{n!}\\]"
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html",
    "href": "posts/kmeans-regime-change/index.html",
    "title": "Kmeans with regime changes",
    "section": "",
    "text": "This post is about how to use Kmeans to classify various market regimes or to use Kmeans to classify financial observations.\nWith K-means we are trying to establish groups of data that are homegenous and distinctly different from other groups. The K- stands for the number of clusters we will create.\nThe concept of distance comes in when deciding if a data point belongs to a cluster. The most common way to measure distance is the Euclidean Distance.\nWith multivariate data set, it is important to normalize the data.\nA usual rule of thumb is to set the number of clusters as the square root of the number of observation."
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html#load-up-packages-and-read-data",
    "href": "posts/kmeans-regime-change/index.html#load-up-packages-and-read-data",
    "title": "Kmeans with regime changes",
    "section": "Load up packages and read data",
    "text": "Load up packages and read data\n\nlibrary(readr)        # load and read .csv file\nlibrary(glue)         # concatenate strings together\nlibrary(dplyr)        # the tidy plyr tool for data wrangling\nlibrary(tidyr)        # to use the drop_na function\nthe_path &lt;- here::here()\ndf &lt;- read_csv(glue(the_path, \"/raw_data/AMD.csv\")) |&gt; \n  rename(adj_close = 'adjClose') |&gt; \n  select(date, high, low, close, adj_close)\nglimpse(df)\n\nRows: 5,611\nColumns: 5\n$ date      &lt;date&gt; 2023-04-21, 2023-04-20, 2023-04-19, 2023-04-18, 2023-04-17,…\n$ high      &lt;dbl&gt; 89.8000, 91.5795, 90.5400, 92.1600, 90.6900, 92.9700, 93.160…\n$ low       &lt;dbl&gt; 88.0550, 88.7300, 88.2200, 89.3300, 88.3000, 90.5000, 91.830…\n$ close     &lt;dbl&gt; 88.43, 90.11, 89.94, 89.78, 89.87, 91.75, 92.09, 92.33, 94.0…\n$ adj_close &lt;dbl&gt; 88.43, 90.11, 89.94, 89.78, 89.87, 91.75, 92.09, 92.33, 94.0…"
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html#feature-engineering",
    "href": "posts/kmeans-regime-change/index.html#feature-engineering",
    "title": "Kmeans with regime changes",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nlibrary(TTR)      # The technical analysis package\nyo &lt;- aroon(df[, c('high', 'low')], n = 23)\ndf$aroon &lt;- yo[, 3]\nyo &lt;- CCI(df[, c('high', 'low', 'close')], n = 17)\ndf$cci &lt;- yo\nyo &lt;- chaikinVolatility(df[, c('high', 'low')], n = 13)\ndf$chaikinVol &lt;- yo\ndf1 &lt;- df |&gt; \n  select(date, aroon, cci, chaikinVol, adj_close) |&gt; \n  mutate(across(c(aroon, cci, chaikinVol), ~ as.numeric(scale(.)))) |&gt;\n  drop_na()\nskimr::skim(df1 %&gt;% select(-date))\n\n\nData summary\n\n\nName\ndf1 %&gt;% select(-date)\n\n\nNumber of rows\n5586\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\naroon\n0\n1\n0.00\n1.00\n-1.49\n-0.94\n-0.19\n0.90\n1.65\n▇▆▂▆▆\n\n\ncci\n0\n1\n0.00\n1.00\n-4.66\n-0.78\n-0.08\n0.80\n4.06\n▁▂▇▅▁\n\n\nchaikinVol\n0\n1\n0.00\n1.00\n-2.49\n-0.70\n-0.10\n0.59\n4.37\n▂▇▅▁▁\n\n\nadj_close\n0\n1\n22.27\n28.78\n1.62\n5.45\n11.48\n23.12\n161.91\n▇▁▁▁▁\n\n\n\n\n# also good to check for correlation between variables. \nlibrary(corrr)\ndf1 |&gt; select(-date, -adj_close) |&gt; \n  correlate() |&gt; \n  rearrange() |&gt; \n  shave()\n\n# A tibble: 3 × 4\n  term          cci  aroon chaikinVol\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 cci        NA     NA             NA\n2 aroon       0.564 NA             NA\n3 chaikinVol  0.212  0.223         NA\n\n\nThese 3 variables seem to complete each other well as little to-no correlation."
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html#create-clusters",
    "href": "posts/kmeans-regime-change/index.html#create-clusters",
    "title": "Kmeans with regime changes",
    "section": "Create clusters",
    "text": "Create clusters\n\nlibrary(purrr)     #use the map function\nlibrary(broom)     #use the glance function on kmeans \ndf1sc &lt;- df1 %&gt;% select(-date, -adj_close)\nkclusts &lt;- tibble(k = 1:9) |&gt; \n  mutate(kclust = map(k, ~kmeans(df1sc, centers = .x, nstart = 30, iter.max = 50L)), \n         glanced = map(kclust, glance), \n         augmented = map(kclust, augment, df1))\nkclusts |&gt; unnest(cols = c('glanced'))\n\n# A tibble: 9 × 7\n      k kclust    totss tot.withinss betweenss  iter augmented           \n  &lt;int&gt; &lt;list&gt;    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;list&gt;              \n1     1 &lt;kmeans&gt; 16755.       16755. -3.27e-10     1 &lt;tibble [5,586 × 6]&gt;\n2     2 &lt;kmeans&gt; 16755.        9842.  6.91e+ 3     1 &lt;tibble [5,586 × 6]&gt;\n3     3 &lt;kmeans&gt; 16755.        7916.  8.84e+ 3     3 &lt;tibble [5,586 × 6]&gt;\n4     4 &lt;kmeans&gt; 16755.        6425.  1.03e+ 4     4 &lt;tibble [5,586 × 6]&gt;\n5     5 &lt;kmeans&gt; 16755.        5492.  1.13e+ 4     5 &lt;tibble [5,586 × 6]&gt;\n6     6 &lt;kmeans&gt; 16755.        4626.  1.21e+ 4     5 &lt;tibble [5,586 × 6]&gt;\n7     7 &lt;kmeans&gt; 16755.        4199.  1.26e+ 4     5 &lt;tibble [5,586 × 6]&gt;\n8     8 &lt;kmeans&gt; 16755.        3791.  1.30e+ 4     6 &lt;tibble [5,586 × 6]&gt;\n9     9 &lt;kmeans&gt; 16755.        3477.  1.33e+ 4     5 &lt;tibble [5,586 × 6]&gt;\n\n\nThere are several ways to choose the ideal number of clusters. One of them is the elbow method, another one is the Silhouette Method.\nThe tot.withinss is the total within-cluster sum of square. This is the value used for the eblow method.\nFor the Silhouette Method, we can use the cluster package.\n\navg_sil &lt;- function(k) { \n  kmeans_object &lt;- kmeans(df1sc, centers = k, iter.max = 50L)\n  silh = cluster::silhouette(kmeans_object$cluster, dist(df1sc))\n  mean(silh[, 3])\n  }\n# Compute and plot wss for k = 2 to k = 15\nyo &lt;- tibble(k_values =  2:9) |&gt; \n  mutate(avg_sil_values = map_dbl(k_values, avg_sil))\nyo\n\n# A tibble: 8 × 2\n  k_values avg_sil_values\n     &lt;int&gt;          &lt;dbl&gt;\n1        2          0.378\n2        3          0.345\n3        4          0.286\n4        5          0.312\n5        6          0.296\n6        7          0.284\n7        8          0.295\n8        9          0.279\n\n\nA more elegant way to do that, using this post from SO\n\nyo &lt;- kclusts |&gt; \n  mutate(silhouetted = map(augmented, ~ cluster::silhouette(as.numeric(levels(.x$.cluster))[.x$.cluster], dist(df1sc)))) |&gt; \n  select(k, silhouetted) |&gt; unnest(cols=c('silhouetted')) |&gt; \n  group_by(k) %&gt;% \n  summarise(avg_sil_values = mean(silhouetted[,3]))\nyo\n\n# A tibble: 9 × 2\n      k avg_sil_values\n  &lt;int&gt;          &lt;dbl&gt;\n1     1         NA    \n2     2          0.378\n3     3          0.345\n4     4          0.293\n5     5          0.313\n6     6          0.320\n7     7          0.305\n8     8          0.298\n9     9          0.273"
  },
  {
    "objectID": "posts/kmeans-regime-change/index.html#some-visualizations",
    "href": "posts/kmeans-regime-change/index.html#some-visualizations",
    "title": "Kmeans with regime changes",
    "section": "Some visualizations",
    "text": "Some visualizations\n\nElbow method\n\nlibrary(ggplot2)\nkclusts |&gt; \n  unnest(cols = c('glanced')) |&gt; \n  ggplot(aes(k, tot.withinss)) + \n  geom_line(alpha = 0.5, size = 1.2, color = 'midnightblue') + \n  geom_point(size = 2, color = 'midnightblue')\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nTotal within-cluster sum of square for k-cluster\n\n\n\n\nBased on the elbow method, I would be tempted to choose to 5 clusters (2 seems another obvious one).\n\n\nSilhouette Method\n\nyo |&gt; ggplot(aes(k, avg_sil_values)) + \n  geom_line(alpha = 0.5, size = 1.2, color = 'midnightblue') + \n  geom_point(size = 2, color = 'midnightblue')\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nSilhouette score for k-clusters\n\n\n\n\n2 is the winner ;-)\n\n\nPlotting the stocks with clustered observations\n\nlibrary(lubridate)\nyo &lt;- kmeans(df1 |&gt; select(-date, -adj_close), centers = 2)\naugment(yo, df1) |&gt; filter(date &gt;= today() - 500) |&gt; \n  ggplot(aes(x = date, y = adj_close)) + \n    geom_line(alpha = 0.5, color = 'midnightblue') + \n    geom_point(aes(color = .cluster)) + \n    theme(legend.position = 'none')\n\n\n\n\nPlotting adjusted close price with only 2 clusters\n\n\n\n\n\nyo &lt;- kmeans(df1 |&gt; select(-date, -adj_close), centers = 3)\naugment(yo, df1) |&gt; filter(date &gt;= today() - 500) |&gt; \n  ggplot(aes(x = date, y = adj_close)) + \n    geom_line(alpha = 0.5, color = 'midnightblue') + \n    geom_point(aes(color = .cluster)) + \n    theme(legend.position = 'none')\n\n\n\n\nPlotting adjusted close price with only 3 clusters\n\n\n\n\n\nyo &lt;- kmeans(df1 |&gt; select(-date, -adj_close), centers = 6)\naugment(yo, df1) |&gt; filter(date &gt;= today() - 500) |&gt; \n  ggplot(aes(x = date, y = adj_close)) + \n    geom_line(alpha = 0.5, color = 'midnightblue') + \n    geom_point(aes(color = .cluster)) + \n    theme(legend.position = 'none')\n\n\n\n\nPlotting adjusted close price with only 6 clusters"
  },
  {
    "objectID": "posts/quant-part1/trinomial-model/index.html",
    "href": "posts/quant-part1/trinomial-model/index.html",
    "title": "05 - Trinomials models for Quantitative Finance",
    "section": "",
    "text": "library(tibble)     # tibble()\nlibrary(dplyr)      # mutate()\nlibrary(ggplot2)    # ggplot()\n\n\n\nA discrete random variable (DRV) \\(y\\) can either go up with a probability of \\(\\alpha\\) or down with a probability of \\(\\alpha\\) or stay the same with a probability of \\(1 - 2\\alpha\\).\n\n\n\n\nflowchart RL\n  y' -- alpha --&gt; y'+delta_y\n  y' -- 1 - 2*alpha --&gt; y\n  y' -- alpha --&gt; y'-delta_y\n\n\n\n\n\nWe call that a trinomial walk.\n\n\n\n\nalpha &lt;- 0.3     # probability to go up or down \n                 # Hence, prob to stay the same is 0.4\n\n#let's do a 252 steps trinomial walk (aka a year of daily movement). \n\nnum_steps &lt;- 252\nprob &lt;- runif(num_steps)\n\ndf &lt;- tibble(step = 1:num_steps, prob = prob) |&gt; \n  mutate(direction = if_else(prob &lt; alpha, -1, if_else(prob &gt; (1 - alpha), 1, 0)), \n         cum_walk = cumsum(direction))\n\nggplot(df, aes(x = step, y = cum_walk)) + \n  geom_line() + \n  ggtitle(label = 'Instance of a trinomial walk', subtitle = 'with 252 steps and symetric move') + \n  ylab(label = 'Cumulative distance')\n\n\n\n\nThis is just one instance of a trinomial walk. In reality, we are interested in getting to know the probabilistic properties of the \\(y\\) variable."
  },
  {
    "objectID": "posts/quant-part1/trinomial-model/index.html#setting-the-stage",
    "href": "posts/quant-part1/trinomial-model/index.html#setting-the-stage",
    "title": "05 - Trinomials models for Quantitative Finance",
    "section": "",
    "text": "A discrete random variable (DRV) \\(y\\) can either go up with a probability of \\(\\alpha\\) or down with a probability of \\(\\alpha\\) or stay the same with a probability of \\(1 - 2\\alpha\\).\n\n\n\n\nflowchart RL\n  y' -- alpha --&gt; y'+delta_y\n  y' -- 1 - 2*alpha --&gt; y\n  y' -- alpha --&gt; y'-delta_y\n\n\n\n\n\nWe call that a trinomial walk."
  },
  {
    "objectID": "posts/quant-part1/trinomial-model/index.html#generating-an-instance-of-a-trinomial-walk",
    "href": "posts/quant-part1/trinomial-model/index.html#generating-an-instance-of-a-trinomial-walk",
    "title": "05 - Trinomials models for Quantitative Finance",
    "section": "",
    "text": "alpha &lt;- 0.3     # probability to go up or down \n                 # Hence, prob to stay the same is 0.4\n\n#let's do a 252 steps trinomial walk (aka a year of daily movement). \n\nnum_steps &lt;- 252\nprob &lt;- runif(num_steps)\n\ndf &lt;- tibble(step = 1:num_steps, prob = prob) |&gt; \n  mutate(direction = if_else(prob &lt; alpha, -1, if_else(prob &gt; (1 - alpha), 1, 0)), \n         cum_walk = cumsum(direction))\n\nggplot(df, aes(x = step, y = cum_walk)) + \n  geom_line() + \n  ggtitle(label = 'Instance of a trinomial walk', subtitle = 'with 252 steps and symetric move') + \n  ylab(label = 'Cumulative distance')\n\n\n\n\nThis is just one instance of a trinomial walk. In reality, we are interested in getting to know the probabilistic properties of the \\(y\\) variable."
  },
  {
    "objectID": "posts/quant-part1/random-behavior-assets/index.html",
    "href": "posts/quant-part1/random-behavior-assets/index.html",
    "title": "01 - Random Behavior of Financial Assets",
    "section": "",
    "text": "One of the main pillar of quantitative finance is the assumption that assets’ returns behave in a random manner. Assets returns are normally distributed. It is a poor assumption as asset’s return are usually not normally distributed (fat tails, skewness, etc.), but it is one that is considered when approaching finance with a quantitative finance. Check this post on the normality of assets returns for a deeper dive into how random (or not) are assets returns."
  },
  {
    "objectID": "posts/quant-part1/random-behavior-assets/index.html#ignoring-randomness",
    "href": "posts/quant-part1/random-behavior-assets/index.html#ignoring-randomness",
    "title": "01 - Random Behavior of Financial Assets",
    "section": "Ignoring randomness",
    "text": "Ignoring randomness\n\\[R_i = \\frac{S_{i+1} - S_i}{S_i} = mean  = \\mu \\delta t\\] \\[S_{i+1} - S_i= S_i \\mu \\delta t\\] \\[S_{i+1} = S_i \\cdot (1 +  \\mu \\delta t) \\tag{3}\\]\nWe could also rewrite Equation 3 so it depends of the initial (starting) price, instead of the previous price.\n\\[S_n = S_0 (1+\\mu \\delta t)^n\\]\nUsing natural log:\n\\(S_n = S_0 e^{log (1+\\mu \\delta t)^n} = S_0 e^{n \\cdot log{(1+\\mu \\delta t)}}\\)\nWe could argue that \\(log(1+\\mu \\delta t) \\approx \\mu \\delta t\\) as \\(log(1+x) \\approx x\\) for small values of x.\n\\[S_n \\approx S_0 \\cdot e^{n \\mu \\delta t} \\tag{4}\\]\nNow, \\(n \\cdot \\delta t\\) is the same as \\(t\\). Hence,\n\n\n\n\n\n\n\\[S(t) \\approx S_0 \\cdot e^{\\mu t}\\]"
  },
  {
    "objectID": "posts/quant-part1/random-behavior-assets/index.html#considering-randomness",
    "href": "posts/quant-part1/random-behavior-assets/index.html#considering-randomness",
    "title": "01 - Random Behavior of Financial Assets",
    "section": "Considering randomness",
    "text": "Considering randomness\nLet’s restart with Equation 2\n\\[R_i = \\frac{S_{i+1} - S_i}{S_i} = \\bar{R} + std \\cdot \\phi = \\mu \\delta t + \\sigma \\phi \\delta t^{1/2}\\] \\[S_{i+1} - S_i= S_i \\mu \\delta t + S_i \\sigma \\phi \\delta t^{1/2} \\tag{5}\\]\n\n\n\n\n\n\n\\[S_{i+1} = S_i \\cdot (1 +  \\mu \\delta t +  \\sigma \\phi \\sqrt{\\delta t}) \\tag{6}\\]\nThis last Equation 6 is the basis for Monte-Carlo simulation.\n\nNotice the standard deviation of return: \\(\\sigma \\sqrt{\\delta t}\\)\nunit of \\(\\mu = \\frac{1}{t}\\)\nunit of \\(\\sigma = \\frac{1}{\\sqrt{t}}\\)\nthis is because we can only add variance together (no sd). For independent variable X and Y: \\(Var(X+Y) = Var(X) + Var(Y)\\)\nthe standard deviation of returns scale up with the square root of the time step."
  },
  {
    "objectID": "posts/quant-part1/random-behavior-assets/index.html#going-to-continuous-time",
    "href": "posts/quant-part1/random-behavior-assets/index.html#going-to-continuous-time",
    "title": "01 - Random Behavior of Financial Assets",
    "section": "Going to continuous time",
    "text": "Going to continuous time\nRestarting from Equation 5 : \\[S_{i+1} - S_i= S_i \\mu \\delta t + S_i \\sigma \\sqrt{\\delta t} \\phi\\]\n\n\\(S_{i+1} - S_i = dS\\)\n\\(S_i = S(t)\\)\n\\(\\delta t = dt\\)\n\\(\\phi \\sqrt{\\delta t} = dX\\) where \\(dX\\) is a random variable with mean = 0 and variance = dt. Hence \\(E[dX] = 0\\) and \\(E[(dX)^2] = dt\\)\n\n\n\n\n\n\n\n\\[dS = S \\mu dt + S \\sigma dX\\] This stochastic differential equation on the change of prices assume:\n\nreturns are treated as random\nreturns are assumed to be normally distributed (again not totally exact)\nprices (S) are modelled as a log-normal walk (SDE)\n\\(\\mu\\) is the drift rate or growth rate\nbecause of the different scaling of time (\\(t\\) and \\(\\sqrt{t}\\)), on a short time frame, drift is negligible and volatility matters."
  },
  {
    "objectID": "posts/quant-part1/random-behavior-assets/index.html#the-euler-maruyana-method-to-compute-the-sde",
    "href": "posts/quant-part1/random-behavior-assets/index.html#the-euler-maruyana-method-to-compute-the-sde",
    "title": "01 - Random Behavior of Financial Assets",
    "section": "The Euler-Maruyana Method to compute the SDE",
    "text": "The Euler-Maruyana Method to compute the SDE\nWe start with Equation 6 : \\(S_{i+1} = S_i \\cdot (1 + \\mu \\delta t + \\sigma \\phi \\sqrt{\\delta t})\\)\n\n# create one simulation for price \nndays &lt;- 252 \nprice &lt;- c()\nprice[1] &lt;- last(df$adjClose)\n\nphi = rnorm(ndays, mean = 0, sd = 1)\n\nfor (i in 2:ndays){ \n  price[i] = price[i-1] * (1 + mu * delta_t + sigma * phi[i] * sqrt(delta_t))\n}\n\nyo &lt;- tibble(x = 1:ndays, price = price)\nggplot(yo, aes(x, price)) + \n  geom_line()\n\n\n\n\nWe can now create 100’s such simulations re-using previous code in a function.\n\ncreate_price_simul &lt;- function(x) {\n  price &lt;- c() \n  price[1] &lt;- last(df$adjClose) \n  phi = rnorm(ndays, mean = 0, sd = 1) \n  for (i in 2:ndays){ \n    price[i] = price[i-1] * (1 + mu * delta_t + sigma * phi[i] * sqrt(delta_t)) \n    } \n  yo &lt;- tibble(x = 1:ndays, price = price)\n  return(yo)\n}\n\nlibrary(purrr)      # map()\nlibrary(RColorBrewer)\n\nnum_of_simul &lt;- 100\ndf1 &lt;- tibble(simul_num = 1:num_of_simul) |&gt; \n  mutate(prices = map(simul_num, create_price_simul))\n\nyo &lt;- df1 |&gt; unnest(cols = c(prices))\n\ngetPalette = colorRampPalette(brewer.pal(9, \"Set1\"))\ncolourCount = num_of_simul\nggplot(yo, aes(x, price, group = simul_num)) + \n  scale_fill_manual(values = colorRampPalette(brewer.pal(9, \"Accent\"))(colourCount)) +\n  geom_line(aes(color = simul_num)) + \n  theme(legend.position = 'none')\n\nWarning in brewer.pal(9, \"Accent\"): n too large, allowed maximum for palette Accent is 8\nReturning the palette you asked for with that many colors"
  },
  {
    "objectID": "posts/quant-part1/brownian-motion/index.html",
    "href": "posts/quant-part1/brownian-motion/index.html",
    "title": "03 - Random-walks & Brownian Motions",
    "section": "",
    "text": "This post is a collection of notes about Random Walk and Brownian Motions as well as their properties.\nThis post on the random behavior of financial assets might be of interest before to go through this."
  },
  {
    "objectID": "posts/quant-part1/brownian-motion/index.html#expected-returns",
    "href": "posts/quant-part1/brownian-motion/index.html#expected-returns",
    "title": "03 - Random-walks & Brownian Motions",
    "section": "Expected Returns",
    "text": "Expected Returns\nSo let’s define a Discrete Random Variable \\(R_i\\) as the i-th toss of a coin. And \\(R\\) takes the value \\(1\\) if it’s a tail and \\(-1\\) if it’s a head.\nThe expectation of \\(R_i\\) is then: \\(\\mathbb{E}[R_i] = \\frac{1}{2} \\cdot 1 + \\frac{1}{2} \\cdot -1 = 0\\)\nThe variance of \\(R_i\\) is then: \\(\\mathbb{V}ar[R_i] = \\mathbb{E}[R_i^2] - \\mathbb{E}[R_i]^2 = \\left( \\frac{1}{2} \\cdot (1)^2 + \\frac{1}{2} \\cdot (-1)^2 \\right) - 0^2 = 1\\)"
  },
  {
    "objectID": "posts/quant-part1/brownian-motion/index.html#expected-sum-of-returns",
    "href": "posts/quant-part1/brownian-motion/index.html#expected-sum-of-returns",
    "title": "03 - Random-walks & Brownian Motions",
    "section": "Expected Sum of Returns",
    "text": "Expected Sum of Returns\nLet’s now define another Discrete Random Variable \\(S_i\\) which is the sum of the returns after n toss. Hence, \\(S_i = \\sum_{i=1}^{n} R_i\\) with an initial condition that \\(S_0=0\\). This initial condition not only makes sense (we did not win any money before to start the game) but it will also constitutes an important initial condition in the definition of Brownian Motion (or Wiener Process)\nThe expectation of \\(S_i\\) is then: \\[\\mathbb{E}[S_i] = \\mathbb{E} \\left[ \\sum_{i=0}^{n} R_i \\right] = \\mathbb{E} \\left[ R_0 + R_1 + \\dots + R_n \\right] = \\mathbb{E}[R_0] + \\mathbb{E}[R_1] + \\dots + \\mathbb{E}[R_n] = 0 + 0 + \\dots + 0 = 0 \\]\nAnd the variance of \\(S_i\\) is then: \\[\\mathbb{V}ar[S_i] = \\mathbb{E}[S_i^2] - \\mathbb{E}[S_i]^2\\]\nNote how the second term \\(\\mathbb{E}[S_i]^2 = 0\\)\n\\[\\mathbb{V}ar[S_i] = \\mathbb{E} \\left[ \\left( \\sum_{i=0}^{n} R_i \\right)^2 \\right] = \\mathbb{E} \\left[ R_0^2 + R_1^2 + \\dots + R_n^2 + 2R_1R_2 + 2R_1R_3 + \\cdots \\right] \\tag{1}\\]\nBecause the events are independents, \\(\\mathbb{E}[R_i R_j] = 0\\). Hence all the double terms in Equation 1 above are equals to 0 and \\[\\mathbb{V}ar[S_i] = \\mathbb{E}[R_0^2] + \\mathbb{E}[R_1^2] + \\dots + \\mathbb{E}[R_n^2] = 1 + 1 + \\dots + 1 = n \\cdot 1 = n \\tag{2}\\]"
  },
  {
    "objectID": "posts/quant-part1/brownian-motion/index.html#the-markov-property",
    "href": "posts/quant-part1/brownian-motion/index.html#the-markov-property",
    "title": "03 - Random-walks & Brownian Motions",
    "section": "The Markov Property",
    "text": "The Markov Property\nImagine, we already threw the coin a few times, let’s say 10 times. Now, we have some additional information as we know \\(S_{10}\\). We could ask ourselves the question what is the expectation for the next toss considering that additional information. It would make sense to say that we will get no more no less that what we already have since \\(\\mathbb{E}[R_i] = 0\\). Mathematically, we can write this: \\[\\mathbb{E}\\left[ S_{i+1} | R_1, R_2, \\dots, R_i \\right] = S_i\\]\nThe expected value of \\(S_i\\) is only dependent of the previous value \\(s_{i-1}\\) and not of any previous values (no memory beyond the one prior value)\nThe idea of the Markov Property is to say that the Random Walk Stochastic Process has no memory beyond the point where it (the walker) is now."
  },
  {
    "objectID": "posts/quant-part1/brownian-motion/index.html#the-martingale-property",
    "href": "posts/quant-part1/brownian-motion/index.html#the-martingale-property",
    "title": "03 - Random-walks & Brownian Motions",
    "section": "The Martingale Property",
    "text": "The Martingale Property\nThe conditional probability of any point in the future is what you already have.\nAnd actually, we can generalize this further and state that: \\[\\mathbb{E}\\left[ S_i | S_j \\space \\forall i \\gt j \\right] = S_j\\]\nIn a ‘fair game’, knowledge of the past will be of no value in predicting future events."
  },
  {
    "objectID": "posts/quant-part1/brownian-motion/index.html#the-quadratic-variation",
    "href": "posts/quant-part1/brownian-motion/index.html#the-quadratic-variation",
    "title": "03 - Random-walks & Brownian Motions",
    "section": "The Quadratic Variation",
    "text": "The Quadratic Variation\nThe quadratic variation of a random walk is defined as \\[\\sum_{i=1}^n (S_i - S_{i-1})^2\\]\nThe quadratic variation will be an extremely important concept as we extend the random-walk from a discrete time model to a continuous time model (aka to a Brownian Motion or Wiener process).\nIntuitively, it makes sense to say that \\[(S_i - S_{i-1}) = \\pm 1\\] Hence \\[(S_i - S_{i-1})^2 = 1\\] Hence \\[\\sum_{i=1}^n (S_i - S_{i-1})^2 = n\\]"
  },
  {
    "objectID": "posts/quant-part1/brownian-motion/index.html#properties-of-brownian-motions",
    "href": "posts/quant-part1/brownian-motion/index.html#properties-of-brownian-motions",
    "title": "03 - Random-walks & Brownian Motions",
    "section": "Properties of Brownian Motions",
    "text": "Properties of Brownian Motions\n\nThey are finite - thanks to the increment (y-axis) scale with the square root of the time-steps\nthey are continuous - limit when time-steps are infinity small and the random-walk is becoming continuous\nfollow Markov property\nfollow Martingale property\nthe quadratic variation from 0 to t is \\(t\\) itself - see Equation 3\nnormality. \\(W(t_i) - W(t_{i-1})\\) is normally distributed with mean 0 and variance \\(t_i - t_{i-1}\\)"
  },
  {
    "objectID": "posts/quant-part1/brownian-motion/index.html#expectation-of-w_t",
    "href": "posts/quant-part1/brownian-motion/index.html#expectation-of-w_t",
    "title": "03 - Random-walks & Brownian Motions",
    "section": "Expectation of \\(W_t\\)",
    "text": "Expectation of \\(W_t\\)\nExpectation is the same as in the random-walk except this time we are using the limit when \\(n \\to 0\\). \\[\\mathbb{E}[W_t] = \\mathbb{E} \\left[\\lim_{n \\to \\infty} \\sum_{i=1}^n R_i\\right] = \\lim_{n \\to \\infty} \\sum_{i=1}^n \\mathbb{E}[R_i] = n \\cdot 0 = 0 \\tag{4}\\]\nIn this sense, the mean of \\(W_t = 0\\)."
  },
  {
    "objectID": "posts/quant-part1/brownian-motion/index.html#variance-of-w_t",
    "href": "posts/quant-part1/brownian-motion/index.html#variance-of-w_t",
    "title": "03 - Random-walks & Brownian Motions",
    "section": "Variance of \\(W_t\\)",
    "text": "Variance of \\(W_t\\)\n\\[\\mathbb{V}ar[W_t] = \\mathbb{E[W_t^2]} - \\mathbb{E}[W_t]^2\\]\nFrom Equation 4, we already know that \\(\\mathbb{E}[W_t] = 0\\); hence \\(\\left( \\mathbb{E}[W_t] \\right)^2\\) is also \\(0\\)\n\\[\\mathbb{E}[W_t^2] = \\mathbb{E}\\left[\\lim_{n \\to \\infty} \\sum_{i=1}^n R_i^2 \\right] = \\lim_{n \\to \\infty} \\sum_{i=1}^n \\mathbb{E}[R_i^2] = \\lim_{n \\to \\infty} n \\cdot \\left( \\sqrt{\\frac{t}{n}} \\right)^2 = t\\]"
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "Series: Probability",
    "section": "",
    "text": "Discrete Probability Simulations in R\n\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\n1 min\n\n\n\n\n\n\nJensen's Inequality\n\n\n\n\n\n\n`Oct 21, 2023`{=html}\n 6 min \n\n\n\n\n\n\nProbability For Quant 01\n\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2022\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "machine-learning-part1.html",
    "href": "machine-learning-part1.html",
    "title": "Series: Machine Learning - Part 1",
    "section": "",
    "text": "KNN\n\n\n\n\n\nUsing KNN in both python and R\n\n\n\n\n\n\nNov 14, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nNaive-Bayes - Part 1\n\n\n\n\n\nMaking Naive-Bayes work in R\n\n\n\n\n\n\nMay 16, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n\n\nA dive into the math behind the linear regression algorithm.\n\n\n\n\n\n\nApr 14, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\nIntro to Kmeans\n\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "quant-part2.html",
    "href": "quant-part2.html",
    "title": "Series: Quant - Part 2",
    "section": "",
    "text": "01 - Stochastic Differential Equation - Part I\n\n\n\n\n\nIntroducing itô integrals.\n\n\n\n\n\n\nJul 22, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\n02 - Stochastic Differential Equation - Part II\n\n\n\n\n\nSome more examples of ito integrals.\n\n\n\n\n\n\nJul 22, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n03 - Stochastic Calculus - Part III\n\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\n04 -Martingales\n\n\n\n\n\nDigging into Martingales. Making connections between martingales and itô integrals.\n\n\n\n\n\n\nJul 20, 2023\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "time-series.html",
    "href": "time-series.html",
    "title": "Series: Time-series",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n02 - Statistical Moments\n\n\n\n\n\nIntroducing the first 4 moments of statistical analysis: mean, standard deviation, skewness and kurtosis. Showing how to use R and Python on these concepts. We then provide 2 methods to transform data in order to bring it closer to a normal distribution.\n\n\n\n\n\n\nNov 2, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\n03 - AutoCorrelation, Stationarity and Random-Walk - Part 1\n\n\n\n\n\nA dive into the concepts of autocorrelation and stationarity of time-series. We also get into how to plot correlogram using R and Python, random-walk, white-noise.\n\n\n\n\n\n\nSep 29, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n04 - Time-series decomposition\n\n\n\n\n\nIntroducing time-series decomposition. We first show how to compose time-series using linear trend, seasonality and then white nosie.\n\n\n\n\n\n\nOct 21, 2022\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/proba-quant/jensen-inequality/index.html#simulation",
    "href": "posts/proba-quant/jensen-inequality/index.html#simulation",
    "title": "Jensen’s Inequality",
    "section": "Simulation",
    "text": "Simulation\nTo make our experiment a bit more realistic, we could simulate 50 roll of dice and check that the Jensen’s inequality still hold. Our convex function is stil \\(f(x)=X^2\\).\n\noutcomes = np.random.randint(1, 7, 50)\n\ndef payoff(x): \n  return x**2\n\npayoffs = [payoff(outcome) for outcome in outcomes]\n\nAnd let’s again compare the mean of the transformed vs the transformed of the mean.\n\nprint(f\"The mean of the transformed outcomes is {np.mean(payoffs)}\")\nprint(f\"The transformed of the mean is {payoff(np.mean(outcomes))}\")\n\nThe mean of the transformed outcomes is 14.2\nThe transformed of the mean is 11.0224"
  },
  {
    "objectID": "posts/proba-quant/jensen-inequality/index.html#arithmetic-mean-vs-geometric-mean",
    "href": "posts/proba-quant/jensen-inequality/index.html#arithmetic-mean-vs-geometric-mean",
    "title": "Jensen’s Inequality",
    "section": "Arithmetic mean vs Geometric mean",
    "text": "Arithmetic mean vs Geometric mean\nOne way, the Jensen’s inequality is used in finance is when it comes to returns. We can indeed compute the average returns as a arithmetic average or as a geometric average.\nLet’s say, we have a data set with n observations. Then we define, the arithmetic mean as \\[AM = \\frac{1}{n} \\sum_{i=1}^n i\\] and the geometric mean as \\[GM = \\left( \\prod_{i=1}^n i \\right)^{\\frac{1}{n}}\\]\nUsing logarithms, and starting with the arithmetic mean, we have: \\[log(AM) = log \\left( \\frac{1}{n} \\sum_{i=1}^n i \\right) \\tag{2}\\]\nContinuing with the geometric mean, we have: \\[log (GM) = log \\left( \\prod_{i=1}^n i \\right)^{\\frac{1}{n}} = \\frac{1}{n} log \\left( \\prod_{i=1}^n i \\right) = \\frac{1}{n} \\sum_{i=1}^n log(i) \\tag{3}\\]\nWe could transform Equation 3 saying that the geometric mean is the exponential of the arithmetic mean. \\[GM = exp \\left( \\frac{1}{n} \\sum_{i=1}^n i \\right) \\tag{4}\\]\nBack to Jensens’s inequality.\nThe log function is a concave function. We just re-write Equation 1 changing the inequality sign: the mean of the transformation is less or equal to the transformation of the mean. On probabilistic terms, \\[\\mathbb{E}[f(X)] \\leq f(\\mathbb{E}[X])\\]\n\\[log(GM) = \\text{mean of the transformation (log)}\\] \\[log(AM) = \\text{transformation (log) of the mean}\\]\nIf we go back to our dice throwing example, we should see that the \\(log(GM) &lt;= log(AM)\\). Let’s model that\n\noutcomes = [1, 2, 3, 4, 5, 6]\nmean_outcomes = np.mean(outcomes)\n\ndef payoff(x): \n  return np.log(x)\n\npayoffs = [payoff(outcome) for outcome in outcomes]\n\n\nprint(f\"Log(GM) - The mean of the transformed (log) is {np.mean(payoffs)}\")\nprint(f\"Log(AM) - The transformed (log) of the mean is {np.log(np.mean(outcomes))}\")\n\nLog(GM) - The mean of the transformed (log) is 1.0965418686683501\nLog(AM) - The transformed (log) of the mean is 1.252762968495368\n\n\nThis match our initial statement. mean of the transformed being smaller than the transformed of the mean as the transformation is concave!\nAnd we can undo that log using an exponent (which will preserve the inequality sign). Hence GM &lt; AM. In finance, especially portfolio management, GM means are preferred as it is taking into account the compounding effect of the returns."
  },
  {
    "objectID": "posts/proba-quant/jensen-inequality/index.html#options-convexity",
    "href": "posts/proba-quant/jensen-inequality/index.html#options-convexity",
    "title": "Jensen’s Inequality",
    "section": "Options convexity",
    "text": "Options convexity\nWe are talking about the convexity of options to draw the attention on the non-linear relationship (in this case convex relationship … duh!) between the option’s price and the price of its underlying asset.\nThis comes from the second order derivative of the option’s price in regards to the price of the underlying. In the Black-Schole Merton equation, this can be seen as: \\[\\frac{\\partial{V}}{\\partial{t}} + \\color{blue}{\\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2{V}}{\\partial{S}^2}} + r S \\frac{\\partial{V}}{\\partial{S}} - rV = 0\\]\nRecall the value of a vanilla European call option at expiration is \\(C(S) = max(S-K, 0)\\) where:\n\n\\(K\\) is the strike price\n\\(S\\) is the price of the underlying\n\\(C(S)\\) is the value of the option price at expiry\n\\(S-K\\) is a linear relationship\n\nBut the rate of change of the option price in regards to its underlying is not linear.\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nS0 = 100     # Initial stock price\nK = 100      # Strike price\nr = 0.05     # Risk-free rate\nT = 1.0      # Time to maturity\nsigma = 0.2  # Volatility\n\nS = np.linspace(80, 120, 100)\n\n# Black-Scholes call option price formula\nd1 = (np.log(S / K) + (r + (sigma**2) / 2) * T) / (sigma * np.sqrt(T))\nd2 = d1 - sigma * np.sqrt(T)\n\nC = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n\ndelta = np.gradient(C, S)\ngamma = np.gradient(delta, S)\n\nplt.figure(figsize = (12, 6))\n\nplt.subplot(131)\nplt.plot(S, C, label = 'Option Price')\nplt.xlabel('Asset Price')\nplt.ylabel('Option Value')\n\nplt.subplot(132)\nplt.plot(S, delta, label = 'Delta')\nplt.xlabel('Asset Price')\nplt.ylabel('Delta')\n\nplt.subplot(133)\nplt.plot(S, gamma, label = 'Gamma')\nplt.xlabel('Asset Price')\nplt.ylabel('Gamma')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/proba-quant/discrete-simulations/index.html",
    "href": "posts/proba-quant/discrete-simulations/index.html",
    "title": "Discrete Probability Simulations in R",
    "section": "",
    "text": "The idea behind this post is to collect various numerical methods to simulate discrete probability problems.\n\nExpectation of a uniform variable.\nQuestion: what is the expectation if one square a number that is picked at random out of a hat (with replacement) that contains the numbers 1 to 100.\n\nexpec &lt;- mean(sample(1:100, size = 1000000, replace = TRUE)^2)\nprint(expec)\n\n[1] 3376.422\n\n\nThe calculated expectation should be: \\[\\sum_{x=1}^{100} x^2 P(X=x) = \\sum_{x=1}^{100} x^2 \\frac{1}{n} = \\frac{101 \\cdot 201}{6} = 3383.5\\]\nWe could connect this to the Jensen’s inequality (as we are dealing with a convex function) and show that indeed the expectation of the function is greater than the function of the expectation.\n\nexp_square &lt;- mean(sample(1:100, 1000000, replace = TRUE)^2)\nsquare_exp &lt;- (mean(sample(1:100, 1000000, replace = TRUE)))^2\n\nprint(exp_square)\n\n[1] 3389.124\n\nprint(square_exp)\n\n[1] 2550.72"
  },
  {
    "objectID": "posts/machine-learning-part1/knn/index.html",
    "href": "posts/machine-learning-part1/knn/index.html",
    "title": "KNN",
    "section": "",
    "text": "Some very basic ML using KNN and python and the tidymodel framework."
  },
  {
    "objectID": "posts/machine-learning-part1/knn/index.html#scaling",
    "href": "posts/machine-learning-part1/knn/index.html#scaling",
    "title": "KNN",
    "section": "Scaling",
    "text": "Scaling\nBecause KNN use distance, it is important to scale the data as a pre-processing steps. Otherwise, features with big scale (let’s say price) will skew the distance against features with lower scale (let’s say percentage)."
  },
  {
    "objectID": "posts/machine-learning-part1/knn/index.html#pros-cons-of-knn",
    "href": "posts/machine-learning-part1/knn/index.html#pros-cons-of-knn",
    "title": "KNN",
    "section": "Pros-Cons of KNN",
    "text": "Pros-Cons of KNN\n\nPros\n\nEasy to understand intuition, mathematics (Euclidean Distance)\nKNN is non-parametric. It’s not making any assumptions on the the type of distribution of the data\nonly one parameter to tune\n\n\n\nCons\n\nnon-efficient in terms of memory\nnon-efficient on speed of execution with new data\nnot suitable for high dimensional data\nnot suitable for big data sets"
  },
  {
    "objectID": "posts/time-series/02-statistical-moments/index.html#log-transformations",
    "href": "posts/time-series/02-statistical-moments/index.html#log-transformations",
    "title": "02 - Statistical Moments",
    "section": "Log-transformations",
    "text": "Log-transformations\nLog transformations tends to reduce the spread of the data. They are appropriate for right-skewed data that is the tail is on the right side of the data. Majority of the data are on the left side with few extreme data on the right side.\nIn the case of left-skewed data, a log transformation will accentuate that skewness even more.\nAlso you cannot log negative values (duh!)\nThe effect of a log transformation on dummy data looks like \nThe points that were all close to each other are now more spread out. For low values, the log curve is pretty steep in increase the spread. And for the more extreme values (on the right side) log bring them closer to each others (curve at higher values is less steep, lower gradient).\n\nlibrary(readr)\nlibrary(dplyr)\n\nw_oil &lt;- read_csv('../../../raw_data/WCOILWTICO.csv') |&gt; \n  mutate(w_log_return = log(WCOILWTICO / lag(WCOILWTICO)), \n         w_return = WCOILWTICO / lag(WCOILWTICO))\n\nlibrary(moments)\nskewness(w_oil$w_return, na.rm = T)\n\n[1] 27.9621\n\nkurtosis(w_oil$w_return, na.rm = T)\n\n[1] 1081.682\n\nlibrary(ggplot2)\n\nggplot(w_oil, aes(x = w_return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density()\n\n\n\n\nApplying now the same process (histogram, skewness and kurtosis) on our log transformed returns and checking the results.\n\nskewness(w_oil$w_log_return, na.rm = T)\n\n[1] -3.073457\n\nkurtosis(w_oil$w_log_return, na.rm = T)\n\n[1] 336.4981\n\nggplot(w_oil, aes(x = w_log_return)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density()\n\n\n\n\nIndeed we have a slightly more normal distribution of data (although, we still have both skewness and kurtosis)"
  },
  {
    "objectID": "posts/time-series/02-statistical-moments/index.html#box-cox-transformation",
    "href": "posts/time-series/02-statistical-moments/index.html#box-cox-transformation",
    "title": "02 - Statistical Moments",
    "section": "Box-Cox transformation",
    "text": "Box-Cox transformation\nThe Box-Cox transformation is a statistical technique used to transform non-normal data into a more normal distribution. It is named after George Box and John Tukey, who introduced the method in 1964.\nThe Box-Cox transformation is based on the idea that many non-normal distributions can be approximated by a power transformation of a normal distribution. The power transformation is given by:\n\\[\\begin{equation}\nY_i^{(\\lambda)} =\n\\left\\{ \\begin{aligned}\n\\frac{Y_i^{\\lambda} -1}{\\lambda} \\; \\space (\\lambda \\neq 0) \\\\\nlog(Y_i) \\; \\space (\\lambda = 0)\n\\end{aligned}\\right.\n\\end{equation}\\]\nIf \\(\\lambda\\) equal …\n\n2: square transformation\n1: no transformation needed; produces results identical to original data\n0.50: square root transformation\n0.33: cube root transformation\n0.00: natural log transformation\n-0.50: reciprocal square root transformation\n-1.00: reciprocal (inverse) transformation\n\nOnce data have been transformed, we will need to apply the inverse transformed back at some point. The inverse transformation is using the log function (inverse of exponent!)\n\\[\\begin{equation}\nY_i =\n\\left\\{ \\begin{aligned}\nexp \\left(\\frac{ log(1 + \\lambda Y_i^{(\\lambda)})}{\\lambda} \\right) \\; \\space (\\lambda \\neq 0) \\\\\nexp(Y_i^{\\lambda}) \\; \\space (\\lambda = 0)\n\\end{aligned}\\right.\n\\end{equation}\\]\nHow is lambda chosen? Using maximum likelihood.\nLet’s apply the transformation to our oil data using the tidymodel approach with the recipes library.\n\nlibrary(recipes)\n\ndat &lt;- w_oil |&gt; na.omit() |&gt; select(w_return)\n\nrecipe_bc &lt;- recipe(~ w_return, data = dat) |&gt; \n  step_BoxCox(w_return) |&gt; \n  # the estimating of lambda step\n  prep(training = dat, retain = TRUE)\n\n# to check the recipe steps\ntidy(recipe_bc)\n\n# A tibble: 1 × 6\n  number operation type   trained skip  id          \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;       \n1      1 step      BoxCox TRUE    FALSE BoxCox_SJuL5\n\n# to check the estimation of lambda\ntidy(recipe_bc, number = 1)\n\n# A tibble: 1 × 3\n  terms    value id          \n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 w_return 0.115 BoxCox_SJuL5\n\n# transforming the data using the estimated lambda\nw_oil_bc &lt;- juice(recipe_bc)\n\nw_oil$w_return_bc &lt;- c(NA, w_oil_bc$w_return)\n\nskewness(w_oil$w_return_bc, na.rm = T)\n\n[1] 0.8642865\n\nkurtosis(w_oil$w_return_bc, na.rm = T)\n\n[1] 324.378\n\nlibrary(ggplot2)\n\nggplot(w_oil, aes(x = w_return_bc)) + \n  geom_histogram(aes(y = after_stat(density)), alpha = 0.3, fill = 'blue') + \n  geom_density()"
  },
  {
    "objectID": "posts/machine-learning-part1/knn/index.html#starting-example",
    "href": "posts/machine-learning-part1/knn/index.html#starting-example",
    "title": "KNN",
    "section": "Starting example",
    "text": "Starting example\n\nx = [4, 5, 10, 4, 3, 11, 14 , 8, 10, 12]\ny = [21, 19, 24, 17, 16, 25, 24, 22, 21, 21]\nclasses = [0, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(x, y, c = classes)\n\n&lt;matplotlib.collections.PathCollection at 0x15b8618b0&gt;\n\n\n\n\n\nNow let’s create a KNN object and a new point\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(list(zip(x, y)), classes)\n\nnew_x = 3\nnew_y = 20\nnew_point = [(new_x, new_y)]\nprediction = knn.predict(new_point)\n\nplt.scatter(x + [new_x], y + [new_y], c = classes + [prediction[0]])\nplt.text(x = new_x-1, y = new_y-1, s = f\"new point, class:{prediction[0]}\")\n\nText(2, 19, 'new point, class:0')"
  },
  {
    "objectID": "posts/machine-learning-part1/knn/index.html#example-with-synthetic-data",
    "href": "posts/machine-learning-part1/knn/index.html#example-with-synthetic-data",
    "title": "KNN",
    "section": "Example with synthetic data",
    "text": "Example with synthetic data\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\n# create our synthetic data\nX, y = make_blobs(n_samples = 1000, n_features = 2, \n                  centers = 4, cluster_std = 1.9, \n                  random_state = 4)\n\n\n# visualizing the dataset \nplt.scatter(X[:,0], X[:,1], c = y, s = 20)\n\n&lt;matplotlib.collections.PathCollection at 0x28065c9a0&gt;\n\n\n\n\n\nSplitting our dataset into training & testing + running KNN on the data\n\n# spliting our data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\nknn5 = KNeighborsClassifier(n_neighbors = 5)\nknn19 = KNeighborsClassifier(n_neighbors = 19)\n\n# fit our 'model' with either '5' or '19' Nearest Neighbors\nknn5.fit(X_train, y_train)\nknn19.fit(X_train, y_train)\n\n# apply prediction on our test set\ny_pred_5 = knn5.predict(X_test)\ny_pred_19 = knn19.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\nprint('Accuracy with K = 5 is', round(accuracy_score(y_test, y_pred_5)*100, 2), '%')\nprint('Accuracy with k = 19 is', round(accuracy_score(y_test, y_pred_19)*100, 2), '%')\n\nAccuracy with K = 5 is 87.6 %\nAccuracy with k = 19 is 89.6 %\n\n\nLet’s visualize both ‘models’ and the impact of the choice of K.\n\n#using subplots to compare\nplt.figure(figsize = (9, 5))\n\n# first subplot\nplt.subplot(1, 2, 1)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred_5, s=20)\nplt.title('Predictions with K=5')\n\n# second subplot\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred_19, s=20)\nplt.title('Prediction with K=19')\n\nText(0.5, 1.0, 'Prediction with K=19')\n\n\n\n\n\nBecause the data are already pretty well separated, the only changes we see easily are the ones in the junction between the blue and purple dots."
  },
  {
    "objectID": "posts/quant-part1/normality-returns/index.html#jarque-bera-test",
    "href": "posts/quant-part1/normality-returns/index.html#jarque-bera-test",
    "title": "02 - Normality of asset returns",
    "section": "Jarque-Bera test",
    "text": "Jarque-Bera test\nThe Jarque-Bera test is a statistical test used to assess whether a sample of data follows a normal distribution. It is a goodness-of-fit test that compares the skewness and kurtosis of the sample data to the skewness and kurtosis of a normal distribution. The test statistic follows a chi-squared distribution with 2 degrees of freedom under the null hypothesis that the data is normally distributed. So low p-values indicates that the data do not follow a normal distribution.\nAs oppose to the Shapiro-Wilk test, the Jarque Bera test can be apply to big data set.\nJarque Bera test is defined as \\[JB = \\frac{n}{6} \\cdot \\left( S^2 + \\frac{(K-3)^2}{4} \\right)\\]\n\nS is the sample skewness\nK is the sample kurtosis\nn is the sample size\n\n\nmoments::jarque.test(df_spy$return)\n\n\n    Jarque-Bera Normality Test\n\ndata:  df_spy$return\nJB = 3373.7, p-value &lt; 2.2e-16\nalternative hypothesis: greater\n\n\nAs expected, we have a tiny p-value, hence we reject the \\(H_0\\) that the data are normally distributed.\nAnd now for our dummy normal returns.\n\nmoments::jarque.test(df_dummy$return)\n\n\n    Jarque-Bera Normality Test\n\ndata:  df_dummy$return\nJB = 0.72149, p-value = 0.6972\nalternative hypothesis: greater\n\n\np-value is above the 0.05 threshold, we do not reject the \\(H_0\\) that data are normally distributed."
  },
  {
    "objectID": "posts/time-series/04-ts-decomposition/index.html#decomposing-a-time-series.",
    "href": "posts/time-series/04-ts-decomposition/index.html#decomposing-a-time-series.",
    "title": "04 - Time-series decomposition",
    "section": "Decomposing a time-series.",
    "text": "Decomposing a time-series.\n\nIn R using standard library\nLet’s go back to the milk example which looks like this.\n\nlibrary(readr)\n\nmilk &lt;- read_csv('../../../raw_data/milk.csv')\n\nhead(milk)\n\n# A tibble: 6 × 2\n  month      milk_prod_per_cow_kg\n  &lt;date&gt;                    &lt;dbl&gt;\n1 1962-01-01                 265.\n2 1962-02-01                 252.\n3 1962-03-01                 288 \n4 1962-04-01                 295.\n5 1962-05-01                 327.\n6 1962-06-01                 314.\n\nggplot(milk, aes(x = month, y = milk_prod_per_cow_kg)) + \n  geom_line()\n\n\n\n\nIf we transform our tibble into a time-series df, then we can use the decompose() function.\n\nts_milk &lt;- ts(milk$milk_prod_per_cow_kg, start = c(1962, 1, 1), frequency = 12)\nmilk_dec &lt;- decompose(ts_milk, type = 'additive', filter = NULL)\n\nplot(milk_dec)\n\n\n\n\nAnd now, we can use our decomposed time-series to detrend or remove seasonality\n\nmilk_adj &lt;- ts_milk - milk_dec$seasonal\nplot(milk_adj)\n\n\n\n\nThe timtk package can achieve the same output in a more direct way as timetk fit the tidyverse framework.\n\nlibrary(timetk)\n\nmilk |&gt; \n  plot_stl_diagnostics(month, milk_prod_per_cow_kg, .frequency = 12)\n\nfrequency = 12 observations\n\n\ntrend = 60 observations per 5 years\n\n\n\n\n\n\nIt should be noted that the timetk package use a Seasonal-Trend-Loess decomposition (STL). If we want to get the values, we use the tk_stl_diagnostics(date, value) function.\n(From FPP 3rd ed. )While loess is a method for estimating nonlinear relationships. The STL method was developed by R. B. Cleveland et al. (1990).\nSTL has several advantages over classical decomposition:\n\nSTL will handle any type of seasonality, not only monthly and quarterly data.\nThe seasonal component is allowed to change over time, and the rate of change can be controlled by the user.\nThe smoothness of the trend-cycle can also be controlled by the user.\nIt can be robust to outliers (i.e., the user can specify a robust decomposition), so that occasional unusual observations will not affect the estimates of the trend-cycle and seasonal components. They will, however, affect the remainder component.\n\n\nhead(milk |&gt; tk_stl_diagnostics(month, milk_prod_per_cow_kg))\n\nfrequency = 12 observations per 1 year\n\n\ntrend = 60 observations per 5 years\n\n\n# A tibble: 6 × 6\n  month      observed season trend remainder seasadj\n  &lt;date&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 1962-01-01     265.  -8.44  272.     1.81     273.\n2 1962-02-01     252. -27.2   272.     7.15     280.\n3 1962-03-01     288   15.5   273.    -0.726    273.\n4 1962-04-01     295.  22.4   274.    -1.18     273.\n5 1962-05-01     327.  50.0   275.     2.42     277.\n6 1962-06-01     314.  37.3   276.     0.815    276."
  },
  {
    "objectID": "posts/machine-learning-part1/knn/index.html#example-with-the-adult-dataset-from-kaggle",
    "href": "posts/machine-learning-part1/knn/index.html#example-with-the-adult-dataset-from-kaggle",
    "title": "KNN",
    "section": "Example with the adult dataset (from Kaggle)",
    "text": "Example with the adult dataset (from Kaggle)\n\nimport numpy as np\nimport pandas as pd\n\ndata = pd.read_csv('../../../raw_data/adult.csv')\ndata.head()\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducational-num\nmarital-status\noccupation\nrelationship\nrace\ngender\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nincome\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n&lt;=50K\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n&gt;50K\n\n\n4\n18\n?\n103497\nSome-college\n10\nNever-married\n?\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\n&lt;=50K\n\n\n\n\n\n\n\nDoing some cleaning of data. All the cleaning ideas come from the Kaggle notebooks (I have not deeply analysed the data)\n\n# in a few columns, many cells have '?', let's replace those with Nan\ndata['workclass'] = data['workclass'].replace('?', np.nan)\ndata['occupation'] = data['occupation'].replace('?', np.nan)\ndata['native-country'] = data['native-country'] .replace('?', np.nan)\n\ndf = data.copy()\n\n# dropping row with NA\ndf.dropna(how = 'any', inplace = True)\n\n# dropping dupplicates rows\ndf = df.drop_duplicates()\n\n# drop columns"
  },
  {
    "objectID": "posts/quant-part1/binomials_models/index.html#put-call-parity",
    "href": "posts/quant-part1/binomials_models/index.html#put-call-parity",
    "title": "04 - Binomials models for Quantitative Finance",
    "section": "Put-Call parity",
    "text": "Put-Call parity\n\nPut-call parity shows the relationship that has to exist between European put and call options that have the same underlying asset, expiration, and strike prices.\nPut-call parity states that simultaneously holding a short European put and long European call of the same class will deliver the same return as holding one forward contract on the same underlying asset, with the same expiration, and a forward price equal to the option’s strike price.\nAn arbitrage opportunity exists when the P-C parity is violated.\nThe P-C parity is defined by \\[C + PV(x) = P + S\\]\n\nC = price of the European Call option\nP = price of the European Put Option\nS = Spot price of underlying asset (current market value of asset)\nPV(x) = Present-Value of the strike price (discounted strike price from expiration)\n\nAnother way to see the relationship: \\[P-C = PV(x) - S\\]"
  },
  {
    "objectID": "posts/quant-part1/brownian-motion/index.html#properties-of-weiner-processes",
    "href": "posts/quant-part1/brownian-motion/index.html#properties-of-weiner-processes",
    "title": "03 - Random-walks & Brownian Motions",
    "section": "Properties of Weiner processes",
    "text": "Properties of Weiner processes\n\nThey are finite - thanks to the increment (y-axis) scale with the square root of the time-steps\nthey are continuous - limit when time-steps are infinitely small and the random-walk is becoming continuous\nfollow Markov property\nfollow Martingale property\nthe quadratic variation from 0 to t is \\(t\\) itself - see Equation 3\nnormality. \\(W(t_i) - W(t_{i-1})\\) is normally distributed with mean 0 and variance \\(t_i - t_{i-1}\\)"
  },
  {
    "objectID": "posts/quant-part2/02-SDE-part2/index.html",
    "href": "posts/quant-part2/02-SDE-part2/index.html",
    "title": "02 - Stochastic Differential Equation - Part II",
    "section": "",
    "text": "Let \\(V\\) be a function of \\(S\\) where \\(S\\) satisfies the stochastic differential equation \\(dS = \\mu S \\space dt + \\sigma S \\space dX(t)\\) Note how in this case \\(\\mu\\) and \\(\\sigma\\) are constants. In more elaborate models, both can be time-dependent variables and be stochastic themselves.\nUsing a one-dimension Taylor Series expansion, we can write \\[V(S + dS) \\approx V(S) + \\frac{dV}{dS} \\space dS + \\frac{1}{2} \\frac{d^2V}{dS^2} \\space dS^2\\]\nTo express \\(dS^2\\)? \\[dS^2 = (dS)^2 = \\mu^2 S^2 \\space dt^2 + 2 \\mu \\sigma S^2  \\space dt dX(t) + \\sigma^2 S^2 \\space dX(t)^2\\]\n\\[dS^2 = \\sigma^2 S^2 \\space dX(t)^2 = \\sigma^2 S^2 \\space dt \\tag{1}\\]\n\n\n\n\n\n\nTip\n\n\n\nWe could generalize this a bit further Let’s have a function \\[dG_t = A(t, X_t) \\space dt + B(t, X_t) \\space dW_t\\]\nThen \\[\\mathbb{E}[dG_t] = \\mathbb{E}[A \\space dt] + \\mathbb{E}[B \\space dW_t] = A \\space \\mathbb{E}[dt] + B \\space \\mathbb{E}[dW_t]\\]\nRecall that \\(\\mathbb{E}[dW_t] = 0\\), hence \\[\\mathbb{E}[dG_t] = A \\space \\mathbb{E}[dt]\\] Considering the variance, we can write \\[\\mathbb{V}ar[dG_t] = \\mathbb{V}ar[A \\space dt] + \\mathbb{V}ar[B \\space dW_t] = A^2 \\space \\mathbb{V}ar[dt] + B^2 \\space \\mathbb{V}ar[dW_t] = B^2 \\space dt\\] Recall that \\(\\mathbb{V}ar[dt] = 0\\), hence \\[\\mathbb{V}ar[dG_t] = B^2 \\space \\mathbb{V}ar[dW_t]\\]\n\n\nGoing back to our expansion and considering \\(dV = V(S+dS) - V(S)\\): \\[dV = \\frac{dV}{dS} \\space (\\mu S \\space dt + \\sigma S \\space dX(t)) + \\frac{1}{2} \\frac{d^2V}{dS^2} \\space (\\sigma^2 S^2 \\space dt)\\] \\[dV = \\left( \\mu S \\frac{dV}{dS} + \\frac{1}{2} \\sigma^2 S^2 \\frac{d^2V}{dS^2} \\right) \\cdot dt + \\left( \\sigma S \\frac{dV}{dS} \\right) \\cdot dX(t) \\tag{2}\\]\n\n\n\n\n\n\nExample\n\n\n\nLet \\(V(S) = log(S)\\) with S satisfies the usual SDE: \\(dS = \\mu S dt + \\sigma S dX_t\\). We can then use the above SDE form.\n\\[\\frac{dV}{dS} = \\frac{1}{S}\\] \\[\\frac{d^2V}{dS^2} = - \\frac{1}{S^2}\\]\nUsing above Equation 2: \\[dV = \\left( \\mu S \\frac{1}{S} + \\frac{1}{2} \\sigma^2 S^2 \\frac{-1}{S^2} \\right) \\cdot dt + \\left( \\sigma S \\frac{1}{S} \\right) \\cdot dX(t)\\] \\[dV = \\left( \\mu - \\frac{1}{2} \\sigma^2  \\right) \\cdot dt + \\sigma  \\cdot dX(t)\\] Using the integral form:\n\\[\\int_0^t d(log \\space S) = \\int_0^t \\mu - \\frac{1}{2} \\sigma^2 \\space d\\tau + \\int_0^t \\sigma \\space dX(\\tau)\\] \\[log(S_t) - log(S_0) = \\mu t - \\frac{1}{2} \\sigma^2 t + \\sigma (X_t - X_0)\\] \\[log \\left( \\frac{S_t}{S_0} \\right) = \\mu t - \\frac{1}{2} \\sigma^2 t + \\sigma (X_t - X_0)\\] \\[S_t = S_0 \\cdot e^{\\mu t - \\frac{1}{2} \\sigma^2 t + \\sigma (X_t - X_0)}\\]\nUsing \\(X_0 = 0\\) and \\(X_t = \\phi \\sqrt{t}\\): \\[S_t = S_0 \\cdot e^{\\mu t - \\frac{1}{2} \\sigma^2 t + \\sigma \\phi \\sqrt{t}}\\]\n\n\nAnother example with interest rate\n\n\n\n\n\n\nVasicek model\n\n\n\nThis model developed in 1978 by Vasicek is about interest rate.\nThe basic SDE takes this form \\[dr = (\\eta - \\gamma r) dt + \\sigma dX \\tag{3}\\] In this model, \\(\\eta\\), \\(\\gamma\\) and \\(\\sigma\\) are all constant.\n\n\\(\\gamma\\) is the speed of reversion to the (long term) mean rate. It’s the rate of reversion.\nwe demote \\(\\bar{r}\\) the mean interest rate such that \\(\\bar{r} = \\frac{\\eta}{\\gamma}\\)\n\n\\[dr = \\gamma (\\bar{r} - r) \\space dt + \\sigma \\space dX \\tag{4}\\]\nIf we let \\(u = r - \\bar{r}\\), then \\(du = dr\\) because we consider \\(\\bar{r}\\) as a constant. Hence \\[du = - \\gamma u \\space dt + \\sigma dX\\] \\[du + \\gamma u dt = \\sigma dX\\]\n\\[e^{\\gamma t} du + \\gamma u e^{\\gamma t} dt = e^{\\gamma t} \\sigma dX\\] \\[d(u e^{\\gamma t}) = \\sigma e^{\\gamma t} dX\\] \\[\\int_0^t d(u_s e^{\\gamma s}) = \\sigma \\int_0^t e^{\\gamma s} dX_s\\] \\[u(t) e^{\\gamma t} - u(0) = \\sigma \\int_0^t e^{\\gamma s} dX_s\\] \\[u(t) = u(0) e^{-\\gamma t} +  \\sigma \\int_0^t e^{\\gamma (s - t)} dX_s\\]"
  },
  {
    "objectID": "posts/quant-part2/02-SDE-part2/index.html#steady-state",
    "href": "posts/quant-part2/02-SDE-part2/index.html#steady-state",
    "title": "02 - Stochastic Differential Equation - Part II",
    "section": "Steady-state",
    "text": "Steady-state\nIn some case, there are situation (random-walk) with a long term mean reversal - we say that they have a steady state distribution. This means that in the long run, the \\(p(y, t; y', t')\\) doesn’t depend of the starting point \\(y, t\\); the probability becomes time independent. Think of situations such as interest rate and volatility.\nIn the case of a steady state situation, \\(\\frac{\\partial P}{\\partial t'} = 0\\) since the process becomes time independent in the long run. And the probability (now written $p_{} (y’) $) satisfies the ordinary differential equation: \\[\\frac{1}{2} \\frac{d \\left(B(y')^2 p_{\\infty} \\right)}{d y'^2} - \\frac{d \\left( A(y') p_{\\infty} \\right)}{d y'} = 0 \\tag{8}\\] It isn’t anymore a partial differential equation as the time component vanishes (aka steady-state, long-term reversal)\n\n\n\n\n\n\nVasicek revisited\n\n\n\nRecall from above the Vasicek model \\[dr = \\gamma (\\bar{r} - r) \\space dt + \\sigma \\space dW_t\\] Using Equation 8, we can write the steady state distribution \\(p_{\\infty} r'\\) following ordinary differential equation. \\[\\frac{1}{2} \\sigma^2 \\frac{d^2  p_{\\infty} }{d r'^2} - \\gamma \\frac{d( \\left(\\bar{r} - r) p_{\\infty} \\right)}{d r'} = 0\\] Integrating both sides, we get: \\[\\frac{1}{2} \\sigma^2 \\frac{d  p_{\\infty} }{d r'} + \\gamma (r - \\bar{r}) p_{\\infty} = K\\] K being a constant. \\[\\frac{1}{2} \\sigma^2 \\frac{d  p_{\\infty} }{d r'} = - \\gamma (r - \\bar{r}) p_{\\infty} + K\\] Letting the constant be 0 (need explanation here) Letting the \\({\\infty}\\) just for convenience purposes.\n\\[\\frac{1}{p} dp = \\frac{-2 \\gamma}{\\sigma^2} (r - \\bar{r}) \\space dr'\\] Integrating both sides, \\[\\int \\frac{1}{p} dp = \\frac{-2 \\gamma}{\\sigma^2} \\int (r - \\bar{r}) \\space dr'\\] \\[log(p) = \\frac{-2 \\gamma}{\\sigma^2} \\frac{1}{2} (r - \\bar{r})^2 + K\\] Using a normalizing constant, as we inverse the log \\[p(r) = A \\cdot e^{- \\frac{\\gamma}{\\sigma^2} (r - \\bar{r})^2} \\tag{9}\\]\nWe know that \\(\\int_{\\mathbb{R}} p(r) dr= 1\\), hence \\[A \\int_{-\\infty}^{\\infty} e^{- \\frac{\\gamma}{\\sigma^2} (r - \\bar{r})^2} dr = 1\\] We can integrate this using substitution \\(u = \\frac{\\sqrt{\\gamma} \\space (r - \\bar{r})}{\\sigma}\\) with \\(\\frac{du}{dr} = \\frac{\\sqrt{\\gamma}}{\\sigma}\\) \\[A \\int_{-\\infty}^{\\infty} e^{-u^2} \\frac{\\sigma}{\\sqrt{\\gamma}} du = 1\\] \\[A \\frac{\\sigma}{\\sqrt{\\gamma}} \\int_{-\\infty}^{\\infty} e^{-u^2} du = 1\\] \\[A \\frac{\\sigma}{\\sqrt{\\gamma}} \\sqrt{\\pi}  = 1\\] \\[A = \\frac{\\sqrt{\\gamma}}{\\sigma \\sqrt{\\pi}} = \\frac{1}{\\sigma} \\sqrt{\\frac{\\gamma}{\\pi}}\\]\nPutting it all back together in Equation 9: \\[p_{\\infty}(r) = A \\cdot e^{- \\frac{\\gamma}{\\sigma^2} (r - \\bar{r})^2} = \\frac{1}{\\sigma} \\sqrt{\\frac{\\gamma}{\\pi}} e^{- \\frac{\\gamma}{\\sigma^2} (r - \\bar{r})^2}\\] This means: in our case of a steady state stochastic process, the variable \\(r\\) follows a normal distribution with mean \\(\\bar{r}}\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt{2 \\cdot \\gamma}}\\)"
  },
  {
    "objectID": "posts/quant-part2/03-SDE-part3/index.html",
    "href": "posts/quant-part2/03-SDE-part3/index.html",
    "title": "03 - Stochastic Calculus - Part III",
    "section": "",
    "text": "Recall\n\n\n\n\n\n\n\nFunction\nItô Lemma\n\n\n\n\n\\(F(X_t)\\)\n\\(dF = \\frac{1}{2} \\frac{d^2F}{dX^2} \\space dt + \\frac{dF}{dX} \\space dx\\)\n\n\n\\(F(t, X_t)\\)\n\\(dF = \\left( \\frac{\\partial F}{\\partial t} + \\frac{1}{2} \\frac{\\partial^2 F}{\\partial X^2} \\right) dt + \\frac{\\partial F}{\\partial X} dX\\)\n\n\n\\(V(S)\\) when \\(dS = \\mu S dt + \\sigma S dX\\)\n\\(dV = \\left( \\mu S \\frac{dV}{dS} + \\frac{1}{2} \\sigma^2 S^2 \\frac{d^2V}{dS^2} \\right)dt + \\left( \\sigma S \\frac{dV}{dS}\\right) dX\\)\n\n\n\n\n\nItô Integrals as non-anticipatory\nLet’s consider the stochastic integral of the form \\[\\int_0^T f(t, X(t)) dX(t)\\] where \\(X_t\\) is a Brownian motion. We’ll shorten this form to \\(\\int_0^T f(t, X) dX\\)\nWe define this integral as \\[\\int_0^T f(t, X) dX = \\lim_{N \\to \\infty} \\sum_{i=0}^{N-1} f(t_i, X_i) \\cdot \\underbrace{ (X_{i+1} - X_i) }_{dX}\\]\nIt’s important to define it this way in order for the itô integral to stay non-anticipatory. We know everything up to time \\(t_i\\) and so the only uncertainties left is \\(X_{i+1} - X_i\\) which is \\(dX\\)\n\n\nProduct rule within Stochastic Calculus\nWhen dealing with Stochastic Differential Equations, we can adapt some of the rules of classical calculus such as the product rule: \\(d(xy) = xdy + ydx\\)\nLet’s say we have 2 stochastic processes: \\[d(X(t)) = \\mu_1 X(t) dt + \\sigma_1 X(t)  dW_t\\] \\[d(Y(t)) = \\mu_2 Y(t) dt + \\sigma_2 Y(t) dW_t\\]\nAnd we define a function \\(F\\) which is a product of these 2 stochastic processes such that \\(F = F(X,Y) = XY\\).\nUsing a Taylor Series Expansion, we can write: \\[F(X + dX, Y + dY) \\approx F(X,Y) + \\frac{\\partial F}{\\partial X} dX + \\frac{\\partial F}{\\partial Y} dY + \\frac{1}{2} \\frac{\\partial^2F}{\\partial X^2} dX^2 + \\frac{1}{2} \\frac{\\partial^2F}{\\partial Y^2} dY^2 + \\frac{\\partial^2F}{\\partial X \\partial Y} dXdY + \\dots\\]\nHence, \\[dF = \\frac{\\partial F}{\\partial X} dX + \\frac{\\partial F}{\\partial Y} dY + \\frac{1}{2} \\frac{\\partial^2F}{\\partial X^2} dX^2 + \\frac{1}{2} \\frac{\\partial^2F}{\\partial Y^2} dY^2 + \\frac{\\partial^2F}{\\partial X \\partial Y} dXdY + \\dots \\tag{1}\\]\nNow, we can calculate all these partial derivatives and plugged them back in the above equation. \\(\\frac{\\partial F}{\\partial X} = Y\\) and \\(\\frac{\\partial^2 F}{\\partial X^2} = 0\\).\nSimilarly \\(\\frac{\\partial F}{\\partial Y} = X\\) and \\(\\frac{\\partial^2 F}{\\partial Y^2} = 0\\).\nFinally: \\(\\frac{\\partial^2F}{\\partial X \\partial Y} = 1\\)\nPlugging it all back in Equation 1: \\[dF = Y dX + X dY + dXdY \\tag{2}\\]\n\n\nIntegral by parts\nIn classical calculus, we re-use the product rule to come up with the integration by part: \\(d(xy) = xdy + ydx\\). That is \\(xdy = d(xy) - ydx\\) which we can integrate for and get: \\(\\int xdy = \\int d(xy) - \\int y dx\\) which is the same as \\(\\int x dy = xy - \\int y dx\\).\nLet’s bring this to stochastic calculus. Again \\(F\\) is a function of the product of 2 stochastic processes: \\(F = F(X,Y) = XY\\) Using the same logic and our previous result Equation 2, we write \\[d(XY) = Y dX + X dY + dXdY\\] \\[X dY = d(XY) - Y dX - dXdY \\] \\[\\int_0^t X_s dY_s = \\int_0^t d(X_sY_s) - \\int_0^t Y_s dX_s - \\int_0^t dX_sdY_s\\] \\[\\int_0^t X_s dY_s = X_tY_t - X_0Y_0 - \\int_0^t Y_s dX_s - \\int_o^t dX_sdY_s\\]\n\n\nQuotient Rule within Stochastic Calculus\nWe will re-use the Taylor Series Expansion (Equation 1) except this time the function \\(F\\) is a function of the quotient of 2 stochastic processes: \\(F = F(X, Y) = \\frac{X}{Y}\\). Calculating all the partial derivatives: \\(\\frac{\\partial F}{\\partial X} = \\frac{1}{Y}\\) and \\(\\frac{\\partial^2 F}{\\partial X^2} = 0\\).\nSimilarly \\(\\frac{\\partial F}{\\partial Y} = \\frac{-X}{Y^2}\\) and \\(\\frac{\\partial^2 F}{\\partial Y^2} = \\frac{2X}{Y^3}\\).\nFinally: \\(\\frac{\\partial^2F}{\\partial X \\partial Y} = \\frac{-1}{Y^2}\\)\nPutting it all back together: \\[dF = \\frac{1}{Y} dX + \\frac{-X}{Y^2} dY + \\frac{1}{2} \\frac{2X}{Y^3} dY^2+ \\frac{-1}{Y^2} dXdY\\] Which we can re-write as: \\[dF = d \\left( \\frac{X}{Y} \\right) = \\frac{X}{Y} \\cdot \\left( \\frac{1}{X} dX - \\frac{1}{Y} dY - \\frac{1}{XY} dXdY + \\frac{1}{Y^2} dY^2\\right) \\tag{3}\\]\n\n\n\n\n\n\nIn the quant world.\n\n\n\nwe can word these results in the following way - taken from here:\n\nItô product rule: we buy correlation when we have a product\nItô quotient rule: we sell correlation when we have a ratio, and we are long vol of the denominator."
  },
  {
    "objectID": "posts/quant-part2/01-SDE-Part1/index.html",
    "href": "posts/quant-part2/01-SDE-Part1/index.html",
    "title": "01 - Stochastic Differential Equation - Part I",
    "section": "",
    "text": "Function of a stochastic process \\(X_t\\) - Itô I\nLet \\(F\\) be a function of a stochastic random variable \\(X_t\\). So \\(F = F(X_t)\\).\nUsing a Taylor expansion: \\[F(X + dX) \\approx F(X) + \\frac{dF}{dX} \\space dX + \\frac{1}{2} \\frac{d^2F}{dX^2} dX^2\\] Using \\(F(X+dX) - F(X) = dF\\): \\[dF = \\frac{dF}{dX} \\space dX + \\frac{1}{2} \\frac{d^2F}{dX^2} dX^2\\]\nNow, recall \\(dX\\) is a random variable with mean 0 and standard deviation \\(\\sqrt{t}\\). Also, for small values of \\(t\\), we have \\(\\sqrt{dt} \\gt dt\\), hence it is of higher order than dt. Hence, we re-write the previous equation as\n\\[dF = \\frac{dF}{dX} \\space dX + \\frac{1}{2} \\frac{d^2F}{dX^2} dt\\] Usually, we write the deterministic part of the equation first.\n\\[dF = \\frac{1}{2} \\frac{d^2F}{dX^2} \\space dt + \\frac{dF}{dX} \\space dX \\tag{1}\\]\nAll our Stochastic Differential Equations will have in them these 2 different time steps: \\(dt\\) and \\(\\sqrt{dt}\\). \\[dF = \\underbrace {\\dots \\space dt}_{deterministic-drift} + \\underbrace{\\dots \\space dX}_{random-diffusion}\\]\nThe integral form of this Stochastic Differential Equation is \\[\\int_0^t \\frac{dF}{dX_\\tau} \\space dX_\\tau = \\int_0^t dF - \\frac{1}{2} \\int_0^t \\frac{d^2F}{dX_\\tau^2} \\space d\\tau\\]\n\n\n\n\n\n\nExample 1\n\n\n\n\\(F(X_t) = X^2\\), where \\(X_t\\) is stochastic random variable. \\[\\frac{dF}{dX} = 2X\\] \\[\\frac{d^2F}{dX^2} = 2\\]\nHence, using the previous Equation 1 (aka itô I): \\[dF = \\frac{1}{2} \\cdot 2 \\space dt + 2X \\space dX\\] \\[dF = dt + 2X \\space dX\\] Using integral form\nWe could integrate both side of the previous equation. \\[\\int_0^t dF = \\int_0^t d\\tau + \\int_0^t 2X(\\tau) d{\\tau}\\] \\[F(t) - F(0) = t - 0 + 2 \\int_0^t X(\\tau) d{\\tau}\\] And assuming F(0) = 0: \\[F(t) = t + 2 \\int_0^t X_{\\tau} \\space d{\\tau}\\] \\[X^2(t) = t + 2 \\int_0^t X_{\\tau} \\space d{\\tau}\\]\n\n\n\n\n\n\n\n\nexample 2\n\n\n\n\\(F(W_t) = sin \\space W_t + cos \\space W_t\\), where \\(X_t\\) is stochastic random variable. \\[\\frac{dF}{dW_t} = cos \\space W_t - sin \\space W_t\\] \\[\\frac{d^2F}{dW_t^2} = -(sin \\space W_t + cos \\space W_t)\\] Hence, we can now write\n\\[dF = - \\frac{1}{2}(sin \\space W_t + cos \\space W_t) \\space dt + (cos \\space W_t - sin \\space W_t) \\space dW_t\\]\n\n\n\n\nFunction of a stochastic process \\((t, X(t))\\) - Itô II\nLet \\(F\\) be a function of both time \\(t\\) and a stochastic random variable \\(X_t\\). So \\(F = F(t, X_t)\\).\nUsing a Taylor expansion: \\[F(t + dt, X + dX) \\approx F(t, X) + \\frac{\\partial F}{\\partial t} \\space dt + \\frac{\\partial F}{\\partial X} \\space dX + \\frac{1}{2} \\frac{d^2F}{dX^2} dX^2\\] Using \\(F(t + dt, X+dX) - F(t, X) = dF\\) and factoring the \\(dt\\) and recall \\(dX^2=dt\\):\n\\[dF = \\left( \\frac{\\partial F}{\\partial t} + \\frac{1}{2} \\frac{\\partial ^2F}{\\partial X^2} \\right) dt + \\frac{\\partial F}{\\partial X} dX \\tag{2}\\]\nWe can transform this Stochastic Differential Equation (SDE) into an itô integral.\n\\[\\int_0^t  \\frac{\\partial F}{\\partial X_\\tau} dX_\\tau = \\int_0^t dF - \\int_0^t \\left( \\frac{\\partial F}{\\partial \\tau} + \\frac{1}{2} \\frac{\\partial ^2F}{\\partial X_\\tau^2} \\right) d\\tau\\] \\[\\int_0^t  \\frac{\\partial F}{\\partial X_\\tau} dX_\\tau = F(t, X_t) - F(0, X_0) - \\int_0^t \\left( \\frac{\\partial F}{\\partial \\tau} + \\frac{1}{2} \\frac{\\partial ^2F}{\\partial X_\\tau^2} \\right) d\\tau \\tag{3}\\]\n\n\n\n\n\n\nExample 3\n\n\n\nExpress \\(\\int_0^t \\left( \\tau + W_\\tau \\right) dW_\\tau\\)\nUsing Equation 3, we establish that what is being integrated is \\(\\frac{\\partial F}{\\partial W_\\tau}\\), hence \\[\\frac{\\partial F}{\\partial W_\\tau} = \\tau + W_\\tau \\tag{4}\\]\nThis allows us to find \\(F\\) by integrating both side in regards to \\(W_\\tau\\). \\[F = \\tau W_\\tau + \\frac{1}{2} W_\\tau^2 \\tag{5}\\] and derving Equation 5 for \\(\\tau\\), \\[\\frac{\\partial F}{\\partial \\tau} = W_\\tau \\tag{6}\\] and deriving the first derivative Equation 4 one more time for \\(dW_\\tau\\), we get \\[\\frac{\\partial ^2F}{\\partial W_\\tau^2} = 1\\] Hence, we can re-write \\[\\int_0^t \\left( \\tau + W_\\tau \\right) dW_\\tau = \\left( \\tau W_\\tau + \\frac{1}{2} W_\\tau^2 \\right) - \\int_0^t W_\\tau + \\frac{1}{2} \\space d_\\tau\\]"
  },
  {
    "objectID": "posts/quant-part2/04 - martingales/index.html",
    "href": "posts/quant-part2/04 - martingales/index.html",
    "title": "04 -Martingales",
    "section": "",
    "text": "This post is a collection of notes about Martingales.\nA Martingales is a stochastic process that is driftless (aka it is pure randomness or just volatility). We also say that martingales are constant mean stochastic process.\n\nContinuous time martingales\nA continuous time stochastic process \\(\\{ M_t: t \\in \\mathbb{R}^+ \\}\\) such that \\(M_t\\) is adapted to \\(\\mathcal{F}_t\\) (or is \\(\\mathcal{F}_t\\) measurable) is a martingale if:\n\nintegrability condition: \\(\\mathbb{E}[M_t] \\lt \\infty\\)\nconditional expectation condition: \\(\\mathbb{E}_s[M_{t}|\\mathcal{F}_s] = M_s, \\space 0 \\leq s \\leq t\\)\n\n\n\nLink between itô integrales and martingales\n\n\n\n\n\n\nIntuitive & Motivating example\n\n\n\nLet \\(X\\) be a stochastic process and \\(F = X^2(t)\\). Recall then \\[F(t) = t + 2 \\int_0^t X_{\\tau} \\space d{\\tau}\\] \\[X^2(t) = t + 2 \\int_0^t X_{\\tau} \\space d{\\tau}\\] Taking expectation on both side \\[\\mathbb{E} \\left[ X^2(t) \\right] = t + \\mathbb{E} \\left[ 2 \\int_0^t X_{\\tau} \\space d{\\tau} \\right]\\] With the quadratic variation We already know that \\[\\mathbb{E} \\left[ X^2(t) \\right] = t\\] Which means that \\[\\mathbb{E} \\left[ 2 \\int_0^t X_{\\tau} \\space d{\\tau} \\right]\\] should be equal to 0. Which means that the itô integral \\[\\mathbb{E} \\left[ 2 \\int_0^t X_{\\tau} \\space d{\\tau} \\right]\\] is a martingale.\n\n\nItô integrals are martingales.\nLet \\(g(t, X_t)\\) be a function of a stochastic process, then \\(\\mathbb{E} \\left[ \\int_0^t g(\\tau, x_\\tau) \\space dX_\\tau \\right] = 0\\)\n\n\nA continuous time stochastic process is a martingale\n\\(Yt)\\) is a stochastic process that satisfies the following Stochastic Differential Equation \\[dY(t) = f(Y_t, t) \\space dt + g(Y_t, t) \\space dX(t) \\tag{1}\\] with initial condition \\(Y(0) = 0\\).\nHow to tell if \\(Y(t)\\) is martingale? We will use the fact, from above, that Itô integrals are martingales. For this, by definition, we need \\[\\mathbb{E}_s[Y_t | \\mathcal{F}_s] = Y_s \\space, \\space 0 \\leq s \\leq t\\]\nIntegrating Equation 1 both side, we can get an exact form for \\(Y(t)\\) \\[Y(t) = Y(s) + \\int_s^t f(Y_u, u) \\space du + \\int_s^t g(Y_u, u) \\space dX(u)\\]\nTaking the expectation on both side: \\[\\mathbb{E}(Y_t | \\mathcal{F}_s)= \\mathbb{E} \\left[Y(s) + \\int_s^t f(Y_u, u) \\space du + \\int_s^t g(Y_u, u) \\space dX(u) \\space | \\mathcal{F} \\right]\\]\n\\[\\mathbb{E}(Y_t | \\mathcal{F}_s) = Y(s) + \\mathbb{E} \\left[\\int_s^t f(Y_u, u) \\space du \\space | \\mathcal{F_s} \\right]\\]\nThis is because, see above, ito integrals are martingales and \\(\\int_s^t g(Y_u, u) \\space dX(u)\\) is an ito integral. Hence, its expectation is 0 ==&gt; \\(\\mathbb{E} \\left[ \\int_s^t g(Y_u, u) \\space dX(u) \\right] = 0\\)\nIn order for \\(\\mathbb{E}(Y_t | \\mathcal{F}_s) = Y(s)\\), we now need \\(\\mathbb{E} \\left[\\int_s^t f(Y_u, u) \\space du \\space | \\mathcal{F}_s \\right] = 0\\). This means that \\(f(Y_t, t) = 0, \\space \\forall t\\).\nGoing back to the SDE, we can say that \\(dY(t)\\) is a martingale iff \\[dY(t) = g(Y_t, t) \\space dX(t)\\]\n\n\nExponential martingales\nLEt’s consider a stochastic process \\(Y(t)\\) that satisfies the following Stochastic Differential Equation: \\[dY(t) = f(t) \\space dt + g(t) \\space dX(t)\\] with initial condition \\(Y(0) = 0\\). \\(X(t)\\) is a Brownian Motion, \\(f(t)\\) and \\(g(t)\\) are time-dependent functions.\nWe can now define a new process such that \\[Z(t) = e^{Y(t)}\\]\nHow can we ensure \\(Z(t)\\) is a martingale? How should we choose \\(f(t)\\) such that \\(Z(t)\\) is a martingale?"
  },
  {
    "objectID": "posts/time-series/04-ts-decomposition/index.html#moving-averages",
    "href": "posts/time-series/04-ts-decomposition/index.html#moving-averages",
    "title": "04 - Time-series decomposition",
    "section": "Moving averages",
    "text": "Moving averages\nThere are so many packet in R to calculate moving averages. We’ll keep using the timetk package\n\nyo &lt;- milk |&gt; \n  mutate(ma_3m = slidify_vec(.x = milk_prod_per_cow_kg, .f = mean, .period = 3, .align = 'right'))"
  },
  {
    "objectID": "posts/time-series/04-ts-decomposition/index.html#compose-deterministic-time-series-with-a-trend-and-a-stochastic-component",
    "href": "posts/time-series/04-ts-decomposition/index.html#compose-deterministic-time-series-with-a-trend-and-a-stochastic-component",
    "title": "04 - Time-series decomposition",
    "section": "Compose deterministic time-series (with a trend and a stochastic component)",
    "text": "Compose deterministic time-series (with a trend and a stochastic component)\n\ndf &lt;- tibble(x = 1:252, phi = rnorm(252, mean = 0, sd = 1.5)) |&gt; \n  mutate(y = 0.07 * x + 0.03 + phi)\n\nggplot(df, aes(x, y)) + \n  geom_line() + \n  ggtitle(label = 'Compose a linear trend with a stochastic component')\n\n\n\n\nWe could also create a seasonal time-series with a stochastic component.\n\ndf &lt;- tibble(x = 1:252, phi = rnorm(252, mean = 0, sd = 1.5)) |&gt; \n  mutate(y = 1.7 * sin((2 * pi * x / 50) + 0.3 * pi ) + phi)\n         \nggplot(df, aes(x, y)) + \n  geom_line() + \n  ggtitle(label = 'Compose a seasonal trend with a stochastic component')"
  }
]