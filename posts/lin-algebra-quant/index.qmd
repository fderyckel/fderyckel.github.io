---
title: "Linear Algebra for Quantitative Finance"
author: "Francois de Ryckel"
date: "2023-03-11"
categories: [Linear-Algebra, Matrix, Covariance]
editor: source
date-modified: "2023-03-17"
---

On this post, I am just sharing some linear algebra tools and tricks useful in quantitative finance. This is mainly a post for myself to have a place where I can remember them for when I need them.

# Finding the inverse of a 3x3 matrix 

* Find the transpose of the cofactor matrix.  
    + for each element of the matrix, find its minor (cross the row $i$ and column $j$ for element $ij$, and find the determinant of the square matrix left) 
    + alternate the signs (diagonals are positive) $\begin{pmatrix} + & - & + \\  -  & + & - \\ + & - & + \\  \end{pmatrix}$ 
* Find the determinant of the 3x3 matrix 
    + $a_{1,1} \cdot (\mbox{ cofactor of } a_{1,1}) - a_{1,2} \cdot (\mbox{ cofactor of } a_{1,2}) + a_{1,3} \cdot (\mbox{ cofactor of } a_{1,3})$

# Finding the covariance between 2 vectors

Covariance measures how much two random variables change together, or their joint variability

The sign of the covariance indicates the direction of the relationship between the variables: 

* Positive: The variables move in the same direction. For example, if the covariance of two companies' stocks is positive, it means the stocks move together. 
* Negative: The variables move in opposite directions. For example, if the covariance between rainfall and time spent outside is negative, it means people tend to spend less time outside when it rains more. 
* Zero: There is no link between the values of the two variables. 

The magnitude of the covariance doesn't indicate how strong the relationship is. 

The covariance between 2 vectors is defined as 
$$cov(x, y) = \frac{\sum_{i=1}^n \left( (x_i - \mu_{x}) \cdot (y_i - \mu_{y} \right))}{(n-1)}$$

# The covariance matrix 

The covariance matrix is a square matrix of *n x n* dimensions.  The diagonals values of the matrix shows the variance of the variables; while the off-diagonals are showing the covariance between 2 variables.

$$
S = \begin{pmatrix}
\text{Var}(X_1) & \text{Cov}(X_1,X_2) & \text{Cov}(X_1,X_3) & \cdots & \text{Cov}(X_1,X_n) \\
\text{Cov}(X_2,X_1) & \text{Var}(X_2) & \text{Cov}(X_2,X_3) & \cdots & \text{Cov}(X_2,X_n) \\
\text{Cov}(X_3,X_1) & \text{Cov}(X_3,X_2) & \text{Var}(X_3) & \cdots & \text{Cov}(X_3,X_n) \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\text{Cov}(X_n,X_1) & \text{Cov}(X_n,X_2) & \text{Cov}(X_n,X_3) & \cdots & \text{Var}(X_n)
\end{pmatrix}
$$

where $X_n$ are column vectors. 

Some notable properties: 

* The covariance matrix is always a square matrix. 
* The covariance matrix is symmetric.  That is: $$X^T = X$$  That is because $cov(x, y) = cov(y, x)$
* The covariance matrix has all real eigenvalues (because it is symmetric)
* The covariance matrix is positive semi-definite; which is to say that $x^T \cdot S \cdot x \ge 0$


# Finding the correlation between 2 vectors

Correlation between 2 vectors is defined as 
$$cor(x, y) = \frac{cov(x, y)}{\sigma_x \cdot \sigma_y}$$

# Going from Correlation to Covariance matrix

How to go from the correlation matrix and standard deviation vector to the covariance matrix? 

The standard deviation vector is defined as $\sigma = \pmatrix{\sigma_1 \\ \sigma_2 \\ \vdots \\ \sigma_n}$ 

The correlation matrix is defined as 
$$ R = \begin{pmatrix} 1 & \rho_{12} & \cdots & \rho_{1n} \\ 
                       \rho_{21} & 1 & \cdots & \rho_{2n} \\
                        \vdots & \vdots & \ddots \\
                        \rho_{n1} & \rho_{n2} & \cdots & 1
                                    \end{pmatrix} $$

where $\rho_{ij}$ is the correlation between returns of asset $i$ and asset $j$

we create a diagonal matrix from the standard deviation vector. 

$$ S = D(\sigma) = \begin{pmatrix} \sigma_1 & 0 & \cdots & 0 \\ 
                                    0 & \sigma_2 & \cdots & 0 \\
                                    \vdots & \vdots & \ddots \\
                                    0 & 0 & \cdots & \sigma_n
                                    \end{pmatrix} $$ (aka all other entries being 0)

In R, we use the function **diag(x)** with x being a vector!  Note that S is symmetric, and so $S = S^T$

To get the covariance matrix $\Sigma$, we'll pre & post-multiply the correlation matrix by the diagonal of standard deviation.  Hence: 
$$ S \cdot R \cdot S = \Sigma = \begin{pmatrix} 
\sigma_1^2 & \rho_{12} \sigma_1 \sigma2 & \cdots & \rho_{1n} \sigma_1 \sigma_n  \\ 
\rho_{21} \sigma_2 \sigma1 & \sigma2^2 & \cdots & \rho_{2n} \sigma_2 \sigma_n \\
\vdots & \vdots & \ddots \\
\rho_{n1} \sigma_n \sigma_1 & \rho_{n2} \sigma_n \sigma_2 & \cdots & \sigma_n^2
                                    \end{pmatrix} $$