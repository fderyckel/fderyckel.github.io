---
title: "03 - AutoCorrelation, Stationarity and Random-Walk - Part 1"
author: "Francois de Ryckel"
date: '2022-09-29'
description: 'A dive into the concepts of autocorrelation and stationarity of time-series.  We also get into how to plot correlogram using R and Python, random-walk, white-noise.'
categories: [Time-Series, Autocorrelation, Stationarity, Random Walk]
date-modified: '2023-11-23'
---

This post is to set up the basic concepts of time-series analysis.  

# Autocovariance & autocorrelation 

Autocorrelation as the name indicates is the correlation of the time-series with itself, more specifically with a lag version of itself. We can think of it as many different coefficient of correlations.  One for each lag.  $r_2$ measure the correlation between the time series at time $t$ and time $t - 2$.  Similarly, $r_k$ measure the correlation between the time series at time $t$ and time $t - k$  

$$r_k = \frac{\sum_{t=k+1}^T \left( y_t - \bar{y}\right) \left(y_{t-k} - \bar{y} \right)}{\sum_{t=1}^T \left(y_t - \bar{y} \right)^2}$$

More genrally, let's consider $\{X_t\}$ a time series.  

*   Then the mean function of $\{X_t\}$ (the first moment) is defined as $\mu_t = \textbf{E}(X_t)$.  In other words, $\mu_t$ is the expected value of the time series at point t.  
*   The Variance of the time series is defined as $\sigma_t ^2 = Var(X_t) = \textbf{E}[(X_t - \mu_t)^2]$. 
*   In general, $\mu_t$ and $\sigma_t ^2$ are different at different point in time. 

Now, we define the **autocovariance** function of the time series as 
$$\gamma(s, t) = Cov(X_s, X_t) = \textbf{E}[(X_s - \mu_s)(X_t - \mu_t)]$$  
In the same vein, we define the **autocorrelation** function of the time series as 
$$\rho(s,t) = Corr(X_s, X_t) = \frac {\gamma (s, t)}{\sigma_s \sigma_t} = \frac{Cov(X_s, X_t)}{\sqrt{Var(X_s) Var(X_t)}}$$

Autocovariance and autocorrelation measure the linear correlation between between two points $X_s$ and $X_t$ on the same time-series. 

Few properties of autocovariance and autocorrelation of time-series 

* $\gamma(t, t) = \sigma_t^2$ 
* $\gamma(s, t) = \gamma(t, s)$ 
* $|\gamma(s, t)| \le \sigma_s \sigma_t$ 
* $\rho(t, t) \equiv 1$

## Autocorrelation plots - Correlogram 

As exercise, we can plot the auto-correlation of a non-stationary (aka with significant autocorrelation) time-series.  We are using the [Monthly Milk production](https://www.kaggle.com/datasets/bhaveshsonagra/monthly-milk-production) (no idea where the data come from)

On the autocorrelation plot, the threshold line are situated at $\pm \frac{2}{\sqrt{T}}$ . 

Let's maybe first have a visual of the data. 

```{r}
#| warning: false
#| message: false
#| label: milk-production

library(readr)
library(tibble)
library(ggplot2)

milk <- read_csv('../../../raw_data/milk.csv')

ggplot(milk, aes(x = month, y = milk_prod_per_cow_kg)) + 
  geom_line()
```



### Using R 

In R the standard function to plot a correlogram is the *acf()* function

```{r}
#| warning: false
#| message: false
#| label: acf-milk

acf(milk$milk_prod_per_cow_kg)
```

Graph clearly shows some seasonality (at the 12 lags ==> yearly correlation) which indicates that our data are non-stationary (next section). The threshold line is at `r round(2 / sqrt(nrow(milk)), 3)` as there are `r nrow(milk)` observations in the dataset (number of monthly reported observations)

If we are more attached to the auto-correlation values, we can store the results in a dataframe. 

```{r}
yo <- acf(milk$milk_prod_per_cow_kg, plot = F)
yo
```


We could use the *ggplot* package to create a function to draw acf and get more customization.  We will re-use this function later as well. 

```{r}
# slightly fancier version (with more customization)
ggacf <- function(series, num_lags) {
  significance_level <- qnorm((1 + 0.95)/2)/sqrt(sum(!is.na(series)))  
  a <- acf(series, lag.max = num_lags, plot=F)
  a.2 <- with(a, data.frame(lag, acf))
  g <- ggplot(a.2[-1,], aes(x=lag,y=acf)) + 
    geom_segment(mapping = aes(xend = lag, yend = 0), linewidth = 0.8) + 
    xlab('Lag') + ylab('ACF') + 
    geom_hline(yintercept=c(significance_level,-significance_level), linetype= 'dashed', color = 'dodgerblue4');

  # fix scale for integer lags
  if (all(a.2$lag%%1 == 0)) {
    g<- g + scale_x_discrete(limits = factor(seq(1, max(a.2$lag))));
  }
  return(g);
}
```


```{r}
#| label: acf-milk-ggplot

ggacf(milk$milk_prod_per_cow_kg, num_lags = 37)
```

### Using Python 

In python, we need to use the *statsmodel* package. 

```{python}
from pandas import read_csv
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf

df = read_csv('../../../raw_data/milk.csv', index_col=0)
plot_acf(df)
plt.show()
```


# Stationarity 

A time-series $\{X_t\}$ is (weakly) stationary if: 

* $E[X_t] = \mu$ is a constant 
* $E[X_t^2] < \infty$ 
* $Cov(X_t, X_{t+k}) = \gamma(k)$ is independent of t for each integer k. $\gamma(k)$ is called the lag $k$ autocovariance of function of $\{X_t\}$

# Random Walk & White Noise 

White noise is a special type of time-series and a special case of stationarity. The concept emerge when studying **Random Walk**.  

$\{X_t\}$ is a random-walk if it satisfies the equation: 

$$X_t = X_{t-1} + W_t$$ {#eq-rw1}
where $\{W_t\}$ is a white-noise.  In other words, $W_t \sim iid N(0, \sigma_w^2)$



$\{W_t\}$ is a white-noise if 

* $E[W_t] = \mu$ is a constant for all t
* $Var[W_t] = \sigma_w^2$ is a constant 
* $Cov(W_s, W_t) = 0$ for any s and t with $s<t$.  In other words, any 2 subset of W are uncorrelated. 

If $\{W_t\}$ is iid (independent and identically distributed) and $\rho(k) = 1$ (when k=0) and $\rho(k) = 0$ (otherwise, aka for any other values of k), then $\{W_t\}$ is a white noise. 

Therefore, as $n \rightarrow \infty$, we can say that $\frac{1}{n} (X_1 + \dots + X_n) = E[X_t] = \mu$

A random walk is mean stationary: $E(X_t) = E(X_0)$.  However, the random walk is NOT variance stationary: $Var(X_t) = Var(X_{t-1}) + \sigma_w^2 \gt Var(X_{t-1})$.  

We can generate white-noise in R using the arima.sim() function.  

```{r}
set.seed(1234)

# Generate a white noise in R
wn <- stats::arima.sim(model = list(order = c(0, 0, 0)), n = 250)
df <- tibble(x = 1:250, y = as.vector(wn))

ggplot(df, aes(x, y)) + 
  geom_line(color = 'dodgerblue3') + 
  xlab('Time') + ylab('White Noise') + 
  labs(title = 'Generated White Noise')
```

And let's check the autocorrelation plot to visually confirm that. 

```{r}
# using the standard R function. 
acf(wn, lag.max = 20)
```

We could also use a ggplot function to plot the auto-correlation of our time-series. 

```{r}
ggacf(wn)
```

## Statistical test to check white-noise. 

In R we can use the Ljung-Box test (Portmanteau 'Q' test).  

```{r}
Box.test(wn, type = 'Ljung-Box', lag = 1)
```


# PACF 

The Partial Autocorrelation measures the correlation between $\{X_{t-k} \}$ and $\{ X_t \}$. 

```{r}
pacf(milk$milk_prod_per_cow_kg)
```




# Application to a financial asset 

We know that most financial assets prices are not stationary.  Let's take *SBUX* for instance. 
That being said, the log difference of their prices is stationary.  Note how $log(P_t) - log(P{t-1}) = log( \frac{P_t}{P_{t-1} )$

## Using R

```{r}
#| warning: false
#| message: false

library(dplyr)
library(lubridate)

df <- read_csv('../../../raw_data/SBUX.csv') |> arrange(date) |> 
  select(date, adjClose) |> 
  mutate(ret_1d = log(adjClose / lag(adjClose)), 
         ret_5d = log(adjClose / lag(adjClose, n = 5)), 
         y_t = log(adjClose) - log(lag(adjClose)), 
         day_of_week = weekdays(date)) |> 
  filter(date > '2018-01-01' & day_of_week == 'Tuesday')

ggacf(df$y_t)
```


## Python code 

```{python}
#| label: python_pacf_plot

from statsmodels.tsa.stattools import acf
from statsmodels.stats.diagnostic import acorr_ljungbox
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

py_df = pd.read_csv('../../../raw_data/SBUX.csv')
py_df.index = py_df['date']
py_df = py_df.sort_index()

py_df_ts = pd.Series(py_df['adjClose'])
log_ret = np.log(1 + py_df_ts.pct_change())
log_ret = log_ret.dropna()

r, q, p = acf(log_ret, nlags = 25, qstat = True)

fig = plt.figure()
plot_acf(log_ret, lags=25)
plt.show()
```

```{python}

# q is for the Ljung-Box test statistics
q
```

```{python}

# p is for the p-value of the Ljung-Box statistics. 
p
```

```{python}
q1, p1 = acorr_ljungbox(log_ret, lags =25, return_df = False, boxpierce = False)
p1
```


```{python}
fig = plt.figure()
plot_pacf(log_ret, lags=25)
plt.show()
```

```{python}
log_ret.describe()

log_ret.plot()
plt.show()

pd.Series.idxmax(log_ret)
pd.Series.idxmin(log_ret)
```



