---
title: "05 - AR, MA and ARIMA models"
author: "Fran√ßois de Ryckel"
date: "2024-01-9"
categories: [Time-Series, ARIMA, Decomposition]
description: 'Introducing Arima - Autoregressive Integrated Moving Average.'
editor: source
date-modified: "2024-06-10"
---

# Introduction 

This post is about introducing ARIMA using the CPI data and various R framework for time series. Autoregressive because it is based on past value and moving average to smooth the time series data. 
Our [previous post](../03-autocorrelation/index.qmd) on autocorrelation and partial autocorelation could be considered as prior material.

## Autoregressive models  

Autoregression is a class of linear model where the outcome variable is regressed on its previous values (lagged observations). 

$$Y_t = \delta + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \cdots + \phi_p Y_{t-p} + \epsilon_t$$
This AR model used $p$ lags, hence we say it is of order $p$ or $AR(p)$.

* $\delta$ is an intercept like term
* $Y_{t-i}$ are the regressors (time series own lagged observations) with parameters $\phi_{t-i}$
* $\epsilon$ is the error term

AR(1) is then define as $$Y_t = \delta + \phi_1 Y_{t-1} + \epsilon_t$$

Few characteristics of AR models

* for stationary time-series, $-1 < \phi < 1$
* negative $\phi$ indicates mean-reversion series
* positive $\phi$ indicates momentum series
* the auto-correlation ACF of the AR time-series decay at the rate of $\phi$.  So small $\phi$ will lead to steeper decay in the auto-correlation.  For instance, if $\phi = -0.5$; the first lag autocorrelation will be $-0.5$, the second lag will be $0.25$, the third lag will be $-0.125$, etc. 

We can simulate an AR(1) timeseries in both R and Python. 

::: {.panel-tabset}

## Python

In python, we use the *arima_process* module from the *statsmodels* library.  
Because these models have been developed with ARIMA in mind, we will set up the MA parameter to 1.  Also, we set up the intercept to $1$.  

```{python}
#| label: py_arma01

import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima_process import ArmaProcess

ar1 = np.array([1, -0.9])    # the first term: the delta, the second is the phi_1
ma1 = np.array([1])          # as ma is mandatory for ArmaProcess, we set it up to 1
ar_obj1 = ArmaProcess(ar1, ma1)
sim_data1 = ar_obj1.generate_sample(nsample = 500)
plt.clf()
plt.plot(sim_data1)
plt.show()
```

Doing it similar with a positive $\phi$

```{python}
#| label: py_arma02

ar2 = np.array([1, +0.9])
ma2 = np.array([1])
ar_obj2 = ArmaProcess(ar2, ma2)
sim_data2 = ar_obj2.generate_sample(nsample = 500)
plt.clf()
plt.plot(sim_data2)
plt.show()
```

### Auto-correlation 

Looking at the auto-correlation decay. 

```{python}
#| label: py_arma03

from statsmodels.graphics.tsaplots import plot_acf

ar3 = np.array([1, -0.5])
ma3 = np.array([1])
ar_obj3 = ArmaProcess(ar3, ma3)
sim_data3 = ar_obj3.generate_sample(nsample = 500) # to show fast decay with negative phi

plot_acf(sim_data1, alpha = 1, lags = 20)
plt.show()

plt.clf()
plot_acf(sim_data2, alpha = 1, lags = 20)
plt.show()

plt.clf()
plot_acf(sim_data3, alpha = 1, lags = 20)
plt.show()
```


### Estimating the parameters of AR model 

```{python}
#| label: py_arma04

from statsmodels.tsa.arima.model import ARIMA

model_ar = ARIMA(sim_data1, order = (1, 0, 0)) # the order ensure we are dealing with just AR model
res = model_ar.fit()

print(res.summary())
```

```{python}
print(res.param_names)
print(res.params)
```

Our simulated data had $0.9$ as the parameter of the autoregressive term.  The estimated parameters is quite close indeed. 

### Forecasting with an AR model 

We can also use the AR to make prediction. 

```{python}

from statsmodels.graphics.tsaplots import plot_predict

res.predict(start = 490, end = 510)
#res.plot_predict(start = 400, end = 510)
res.plot_diagnostics()
plt.show()
```



## R

:::

## Moving Average Models

Moving Average (MA) is another class of linear model where the outcome variable is regressed using its own previous error terms. 
$$Y_t = \mu +  \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q} + \epsilon_t$$ 
This MA model used $q$ lags, hence we say it is of order $q$.

Putting it all together, the outcome variable of an ARIMA model can be predicted: 
$$Y_t = \{\delta + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \cdots + \phi_p Y_{t-p} + \epsilon_t \} + \{\mu +  \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \cdots + \theta_q \epsilon_{t-q} + \epsilon_t \}$$
This can be simplify into: 
$$Y_t = \delta + \sum_{i=1}^p \phi_i Y_{t-i} + \sum_{j=1}^q \theta_j \epsilon_{t-j} + \epsilon_t$$

The parameters of an ARIMA model are (p, d, q) : 

* $p$ - Autoregressive.  The number of lagged observations in the model.  Use the previous $n$ observations as predictors. 
* $d$ - Integrated.  The number of times the data is differenced to make the data stationary  
* $q$ - the size of the moving average. Use previous errors to predict $Y_t$

To apply an ARIMA model to a set of data, we will use the US CPI Energy component that we downloaded on the [FED St-Louis website](https://fred.stlouisfed.org/series/CPIENGSL). 

# Application using R 

```{r}
#| label: loading_data
#| message: false
#| warning: false

library(readr)
library(dplyr)
library(ggplot2)
library(modeltime)
library(timetk)

df <- read_csv('../../../raw_data/CPI_energy.csv') |> 
  select(date = DATE, cpi_energy = CPIENGNS)
glimpse(df)
```

```{r}
#| label: own_functions
#| message: false
#| warning: false
#| echo: false

ggacf <- function(series, num_lags) {
  significance_level <- qnorm((1 + 0.95)/2)/sqrt(sum(!is.na(series)))  
  a <- acf(series, lag.max = num_lags, plot=F)
  a.2 <- with(a, data.frame(lag, acf))
  g <- ggplot(a.2[-1,], aes(x=lag,y=acf)) + 
    geom_segment(mapping = aes(xend = lag, yend = 0), linewidth = 0.8) + 
    xlab('Lag') + ylab('ACF') + 
    geom_hline(yintercept=c(significance_level,-significance_level), linetype= 'dashed', color = 'dodgerblue4');

  # fix scale for integer lags
  if (all(a.2$lag%%1 == 0)) {
    g<- g + scale_x_discrete(limits = factor(seq(1, max(a.2$lag))));
  }
  return(g);
}
```

## ARIMA using the base R framework 

We need first to convert our df into a ts object. 

```{r}
df_ts <- ts(df$cpi_energy, start = c(1957,01), frequency = 12, end = c(2023, 11))
str(df_ts)
ts.plot(df_ts, xlab = 'Date', ylab = 'Energy CPI')
```

It is usually a wise idea to check for outliers? An easy way to do this is using a boxplot. 

```{r}
boxplot(df_ts)
```

We use the ACF to determine the MA parameter (q) of the ARIMA model. 

* Significant spikes at specific lags in the ACF plot suggest potential MA terms at those lags 
* An ACF plot that cuts off sharply after a few lags often indicates a suitable MA model 

```{r}
acf(df_ts, lag.max = 100)
```

```{r}
ggacf(df_ts, num_lags = 100)
```

Data is clearly not stationary.  We will need to use differentiation to make it stationary. 

The PCAF helps determine the AR (Autoregressive) order (p) in an ARIMA model.

* Significant spikes at specific lags in the PACF plot suggest potential AR terms at those lags 
* A PACF plot that cuts off sharply after a few lags often indicates a suitable AR model 

```{r}
pacf(df_ts, lag.max = 100)
```

This can be confirm with Augmented Dickey Fuller test

```{r}
tseries::adf.test(df_ts)
```
adf test confirm non-stationarity of our ts. We can differentiate our ts to make it stationary. 

```{r}
diff_ts <- diff(x = df_ts, lag = 1)
ts.plot(diff_ts)
tseries::adf.test(diff_ts)
```

We can create our training and testing set. 

```{r}
train_ts <- df_ts[1:793]
```

We can now use ARIMA model using 1 and 1 for parameter (all we could see from acf)

```{r}
result <- arima(train_ts, order = c(0, 1, 2))
result
tsdiag(result)
```

```{r}
predict(result, 3)
 
```

```{r}
result_df <- arima(df_ts, order = c(0, 1, 2))
result_df
predict(result_df, 2)
```

```{r}
result_df <- arima(df_ts, order = c(4, 1, 1))
result_df
predict(result_df, 2)
```

```{r}
forecast::auto.arima(df_ts, trace = T, stepwise = F, approximation = F)
```


# Using modeltime framework 

```{r}

df |> plot_time_series(date, cpi_energy, .interactive = FALSE)
```
with model time, it is pretty easy to do get the value for arima. 

```{r}
cv_splits <- df |> 
  time_series_split(date_var = date, assess = '13 months', cumulative = TRUE)

cv_splits |> 
  tk_time_series_cv_plan() |>
  plot_time_series_cv_plan(.date_var = date, .value = cpi_energy, .interactive = FALSE)
```

```{r}
library(tidymodels)
library(parsnip)
library(modeltime)

model_fit_arima <- arima_reg() |> 
  set_engine('auto_arima') |>
  fit(cpi_energy ~ date, training(cv_splits))

model_fit_arima
```

