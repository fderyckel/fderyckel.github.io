---
title: "Defining Success"
author: "Francois de Ryckel"
date: "2024-04-16"
categories: [sklearn, tidymodel]
editor: source
date-modified: '2024-04-20'
execute:
  cache: true
---

When evaluating models for a given ML algorithm, we need to define in advance what would be our metric to measure success.  How would we decide if this models is better than this model? Or even which are the hyper-parameters that fine-tuned a model better? 

This post is about defining what is *'best'* or *'better'* when comparing different **supervised models**.  we'll have 2 main parts: measure of success for regression models and measure of success for classification models. 

# Regression models 

When modeling for regression, we somehow **measure the distance between our prediction and the actual observed value**.  When comparing models, we usually want to keep the model which give the smallest sum of distance.  

## RMSE

This is probably the most well-known measure when comparing regression models. Because we are squaring the distance between the predicted and the observed, this penalizes predicted values that are far off the real values.  Hence this measures is used when we want to avoid 'outlier' predictions (prediction that are far off.)

$$RMSE = \sqrt \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{n}$$

## MAE

With **Mean Absolute Error**, we just take the average of the errors.  Useful when we don't really care if predictions is far off from the observed data. 

$$MAE = \frac {\sum_{i=1}^{n}  \lvert y_i - \hat{y}_i \rvert}{n}$$

## Huber Loss

Huber loss is a mixture of RMSE and MAE.  Kind of the best of both world basically. 

$$$$


# Classfication models 

## Accuracy 

Shortcomings: 

* for imbalanced dataset, we can have good accuracy by just predicting most observation with the most frequent class.  For instance in the case of a rare disease or big financial meltdown, we can just predict 

##  Precision 

If you call it true, is it indeed true? In other words, the proportion of predicted positive that are actually positive. 

## Recall 

If there is a positive, did the model predict a positive.  


## F1 score 

It is the **harmonic mean** of both precision and recall. The harmonic mean penalizes model that have very low precision or recall.  Which wouldn't be the case with arithmetic mean. 

$$\frac{2 \cdot Precision \cdot Recall}{Precision + Recall}$$

## AUC & ROC Curve

need to get the prediction as a probability 
```{r}
library(yardstick)

```

